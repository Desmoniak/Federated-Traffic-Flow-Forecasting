{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "flow_file= \"./data/PEMS04/PEMS04.npz\"\n",
    "csv_file = \"./data/PEMS04/distance.csv\"\n",
    "\n",
    "data = np.load(flow_file)\n",
    "df = pd.read_csv(csv_file)\n",
    "TS = data['data']\n",
    "flow = TS[:,:,0]\n",
    "# flow dict 100 time series is the sensor number and the value the traffic flow times serie\n",
    "flow_dict={k:flow[:,k] for k in range(100)}\n",
    "# list of the first 10 connected sensor, each sensor traffic flow is contained in PeMS \n",
    "PeMS = pd.DataFrame(flow_dict)\n",
    "# time serie of sensor k\n",
    "#creation of the datetime index\n",
    "start_date = \"2018-01-01 00:00:00\"\n",
    "end_date = \"2018-02-28 23:55:00\"\n",
    "interval = \"5min\"\n",
    "index = pd.date_range(start=start_date, end=end_date, freq=interval)\n",
    "PeMS = PeMS.set_index(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sort time series by mean traffic flow\n",
    "mean_flow = PeMS.mean().sort_values()\n",
    "#Index of sensor sort by mean traffic flow\n",
    "mean_flow_index = mean_flow.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_order = list(mean_flow_index)\n",
    "PeMS =PeMS.reindex(columns=column_order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# Define the sliding window size and stride\n",
    "window_size = 7\n",
    "stride = 1\n",
    "layers = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a PyTorch dataset to generate input/target pairs for the LSTM model\n",
    "class TimeSeriesDataset(Dataset):\n",
    "    def __init__(self, data, window_size, stride):\n",
    "        self.data = data\n",
    "        self.window_size = window_size\n",
    "        self.stride = stride\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data) - self.window_size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        inputs = self.data[idx:idx+self.window_size]\n",
    "        target = self.data[idx+self.window_size]\n",
    "        return inputs, target\n",
    "\n",
    "# Define your LSTM model here with 6 LSTM layers and 1 fully connected layer\n",
    "class LSTMModel(torch.nn.Module):\n",
    "    def __init__(self, input_size, hidden_size,output_size, num_layers=6):\n",
    "        super().__init__()\n",
    "        self.lstm = torch.nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = torch.nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, _ = self.lstm(x)\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(PeMS.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "def experiment_dataset(cluster_size,df):\n",
    "    cluster_dict={\"size\":cluster_size}\n",
    "    for i in range(len(PeMS.columns)+1-cluster_size):\n",
    "        model = LSTMModel(input_size=cluster_size, hidden_size=32, num_layers=layers, output_size=cluster_size)\n",
    "        train_data= df[df.columns[i:i+cluster_size]][:'2018-02-10 00:00:00']\n",
    "        val_data =  df[df.columns[i:i+cluster_size]]['2018-02-10 00:00:00':'2018-02-14 00:00:00']\n",
    "        test_data = df[df.columns[i:i+cluster_size]]['2018-02-14 00:00:00':]\n",
    "        \n",
    "        train_dataset = TimeSeriesDataset(train_data.values, window_size, stride)\n",
    "        val_dataset = TimeSeriesDataset(val_data.values, window_size, stride)\n",
    "        test_dataset = TimeSeriesDataset(test_data.values, window_size, stride)\n",
    "\n",
    "        train_loader = DataLoader(train_dataset, batch_size=32, shuffle=False)\n",
    "        train_loader = [(inputs.to(device), targets.to(device)) for inputs, targets in train_loader]\n",
    "\n",
    "        val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "        val_loader = [(inputs.to(device), targets.to(device)) for inputs, targets in val_loader]\n",
    "\n",
    "        test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "        test_loader = [(inputs.to(device), targets.to(device)) for inputs, targets in test_loader]\n",
    "        cluster_dict[i]={\"model\":model,\"train\":train_loader,\"val\":val_loader,\"test\":test_loader}\n",
    "    with open('./experiment/clusterS{}.pkl'.format(cluster_size), 'wb') as f:\n",
    "        pickle.dump(cluster_dict, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.cuda\n",
    "def train_model(model,train_loader, val_loader):\n",
    "  # Train your model and evaluate on the validation set\n",
    "    num_epochs = 200\n",
    "    # Define the loss function and optimizer\n",
    "    criterion = torch.nn.MSELoss()\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    best_val_loss = float('inf')\n",
    "    train_losses = []\n",
    "    valid_losses = []\n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss = 0.0\n",
    "        for inputs, targets in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = model(inputs.float())\n",
    "            loss = criterion(outputs, targets.float())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "            train_losses.append(loss.item())\n",
    "        val_loss = 0.0\n",
    "        \n",
    "        for inputs, targets in val_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = model(inputs.float())\n",
    "            loss = criterion(outputs, targets.float())\n",
    "            val_loss += loss.item()            \n",
    "        val_loss /= len(val_loader)\n",
    "        valid_losses.append(val_loss)\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), 'best_model.pth')\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, Training Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}')\n",
    "    best_model =  copy.deepcopy(model)\n",
    "    best_model.load_state_dict(torch.load('best_model.pth'))\n",
    "    return best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./experiment/clusterS0.pkl', 'rb') as f:\n",
    "    my_dict = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize the experiment datasets as pickle object\n",
    "for i in range(15):\n",
    "    experiment_dataset(i+1,PeMS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# iterate on cluster size i\n",
    "for i in range(1,16):\n",
    "# load the experiment datasets from pickle object \n",
    "    with open('./experiment/clusterS{}.pkl'.format(i), 'rb') as f:\n",
    "        my_dict = pickle.load(f)\n",
    "        # iterate on number of cluster 100-i+1\n",
    "        for j in range(100-i+1):\n",
    "            train = my_dict[j][\"train\"]\n",
    "            val = my_dict[j][\"val\"]\n",
    "            model = my_dict[j][\"model\"]\n",
    "            model = train_model(model,train, val)\n",
    "            my_dict[j][\"model\"]=copy.deepcopy(model)\n",
    "    with open('./experiment/clusterS{}.pkl'.format(i), 'wb') as f:\n",
    "        pickle.dump(my_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5838ee9d1e27b0c8776a2c47d844e546f7c02646b3dffe8b9f56b6c67f7dd71a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
