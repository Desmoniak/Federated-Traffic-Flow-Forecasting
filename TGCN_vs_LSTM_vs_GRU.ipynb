{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from src.utils_data import load_PeMS04_flow_data, preprocess_PeMS_data, createLoaders, TimeSeriesDataset\n",
    "from src.utils_graph import compute_laplacian_with_self_loop\n",
    "from src.models import TGCN, GRUModel, LSTMModel\n",
    "from src.utils_training import train_model\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Print Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def result_prediction(predictions, actuals):\n",
    "    from sklearn.metrics import mean_absolute_error, mean_squared_error, mean_absolute_percentage_error\n",
    "    import numpy as np\n",
    "    \n",
    "    indices_by_month = []\n",
    "    EPSILON = 1e-5\n",
    "    # Créer une liste vide pour stocker les données du tableau\n",
    "    data = []\n",
    "    y_pred = predictions[:]\n",
    "    y_true = actuals[:]\n",
    "\n",
    "    signe = \"-\" if np.mean(y_pred - y_true) < 0 else \"+\"\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    mape = mean_absolute_percentage_error(y_true, y_pred)*100\n",
    "    if (mape > 1 or mape < 0):\n",
    "        mape = \"ERROR\"\n",
    "    smape = np.mean(2 * np.abs(y_pred - y_true) / (np.abs(y_true) + np.abs(y_pred)))*100\n",
    "    maape =  np.mean(np.arctan(np.abs((y_true - y_pred) / (y_true + EPSILON))))*100\n",
    "    \n",
    "    return [signe, mae, rmse, mape, smape, maape]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def highlight_rows(row, min_val, max_val):\n",
    "    if row[\"MAAPE\"] == min_val:\n",
    "        color = \"green\"\n",
    "    elif row[\"MAAPE\"] == max_val:\n",
    "        color = \"red\"\n",
    "    else:\n",
    "        color = \"\"\n",
    "    return [f'background-color: {color}'] * len(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def plot_losses(losses, title, label_line):\n",
    "    plt.figure(figsize=(25, 9))\n",
    "    plt.title(title)\n",
    "    plt.plot(losses, label=label_line)\n",
    "    plt.legend(fontsize=12)\n",
    "    plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## create_multiple_model_with_one_captor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "def create_multiple_model_with_one_captor(model, nb_model, name_dict, data, _windows_size, _stride):\n",
    "    dict_model = {}\n",
    "    for i in range(nb_model):\n",
    "        #print(f\"{name_dict}{i}\")\n",
    "        train_loader, val_loader, test_loader = createLoaders(pd.DataFrame(data.iloc[:, i]), window_size=_windows_size, stride=_stride, target_size=horizon)\n",
    "        dict_model[f\"{name_dict}{i}\"] = {\n",
    "                                \"model\": copy.deepcopy(model),\n",
    "                                \"train_loader\": train_loader,\n",
    "                                \"val_loader\": val_loader,\n",
    "                                \"test_loader\": test_loader\n",
    "        }\n",
    "    return dict_model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(best_model, test_loader):\n",
    "    import numpy as np\n",
    "    \n",
    "    # Load the best model and evaluate on the test set\n",
    "    criterion = torch.nn.MSELoss()\n",
    "    best_model.double()\n",
    "    best_model.eval()\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    best_model.to(device)\n",
    "\n",
    "    # Evaluate the model on the test set\n",
    "    test_loss = 0.0\n",
    "    predictions = []\n",
    "    actuals = []\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in test_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            batch_size, horizon_size, num_nodes = targets.size()\n",
    "            final_output = torch.empty((batch_size, 0, num_nodes)).to(device)\n",
    "            outputs = best_model(inputs.double())\n",
    "            final_output = torch.cat([final_output, outputs.unsqueeze(1)], dim=1)\n",
    "            for i in range(1, horizon_size):\n",
    "                outputs = best_model(torch.cat((inputs[:, i:, :], final_output[:, -1, :].unsqueeze(1)), dim=1))\n",
    "                final_output = torch.cat([final_output, outputs.unsqueeze(1)], dim=1)\n",
    "            loss = criterion(final_output, targets)\n",
    "            test_loss += loss.item()\n",
    "            # Save the predictions and actual values for plotting later\n",
    "            predictions.append(final_output.cpu().numpy())\n",
    "            actuals.append(targets.cpu().numpy())\n",
    "    test_loss /= len(test_loader)\n",
    "    # print(f\"Test Loss: {test_loss:.4f}\")\n",
    "    \n",
    "    # Concatenate the predictions and actuals\n",
    "    predictions = np.concatenate(predictions, axis=0)\n",
    "    actuals = np.concatenate(actuals, axis=0)\n",
    "    return (predictions, actuals)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "# EPOCHS\n",
    "################################################################################\n",
    "n_neighbors = 49\n",
    "normalization = \"center_and_reduce\"\n",
    "\n",
    "_window_size = 6\n",
    "horizon = 6\n",
    "_stride = 1\n",
    "\n",
    "################################################################################\n",
    "# PATH\n",
    "################################################################################\n",
    "path_save_model = f\"./{normalization}/nb_captor_{n_neighbors+1}/windows_{_window_size}_out_{horizon}\"\n",
    "\n",
    "################################################################################\n",
    "# EPOCHS\n",
    "################################################################################\n",
    "# Univariate\n",
    "num_epochs_LSTM_univariate = 3\n",
    "num_epochs_GRU_univaritate = 3\n",
    "\n",
    "# Multivariate\n",
    "num_epochs_TGCN = 300\n",
    "num_epochs_LSTM_multivariate = 300\n",
    "num_epochs_GRU_multivariate = 300"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gcogoni\\Documents\\Stage\\Federated-Traffic-Flow-Forecasting\\src\\utils_graph.py:98: FutureWarning: adjacency_matrix will return a scipy.sparse array instead of a matrix in Networkx 3.0.\n",
      "  return nx.adjacency_matrix(graph, nodelist=nodes_order, weight=None).toarray()\n"
     ]
    }
   ],
   "source": [
    "df_PeMS_old, df_distance  = load_PeMS04_flow_data()\n",
    "df_PeMS, adjacency_matrix_PeMS, meanstd_dict = preprocess_PeMS_data(df_PeMS_old, df_distance, init_node=0, n_neighbors=n_neighbors, center_and_reduce=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TGCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "def create_multiple_model_with_one_captor(model, nb_model, name_dict, data, _windows_size, _stride):\n",
    "    dict_model = {}\n",
    "    for i in range(nb_model):\n",
    "        #print(f\"{name_dict}{i}\")\n",
    "        train_loader, val_loader, test_loader = createLoaders(pd.DataFrame(data.iloc[:, i]), window_size=_windows_size, stride=_stride, target_size=horizon)\n",
    "        dict_model[f\"{name_dict}{i}\"] = {\n",
    "                                \"model\": copy.deepcopy(model),\n",
    "                                \"train_loader\": train_loader,\n",
    "                                \"val_loader\": val_loader,\n",
    "                                \"test_loader\": test_loader\n",
    "        }\n",
    "    return dict_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_loader_TGCN, val_loader_TGCN, test_loader_TGCN = createLoaders(df_PeMS, window_size=_window_size, stride=_stride, target_size=horizon)\n",
    "# model_TGCN = TGCN(adjacency_matrix_PeMS, hidden_dim=32, output_size=len(df_PeMS.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# model_path = f\"{path_save_model}/epoch_{num_epochs_TGCN}/TGCN_model.pkl\"\n",
    "# _ , valid_losses, train_losses = train_model(model_TGCN, train_loader_TGCN, val_loader_TGCN, model_path=model_path, num_epochs=num_epochs_TGCN, remove=False)\n",
    "\n",
    "# plot_losses(train_losses, \"TGCN train_losses\", \"train_losses\")\n",
    "# plot_losses(valid_losses, \"TGCN valid losses\", \"valid_losses\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Univariate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # LSTM\n",
    "# models_univaritate_LSTM = {}\n",
    "\n",
    "# # GRU\n",
    "# models_univaritate_GRU = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # LSTM\n",
    "# models_univaritate_LSTM = create_multiple_model_with_one_captor(LSTMModel(1,32,1), n_neighbors+1, \"LSTMModel\", df_PeMS, _window_size, _stride)\n",
    "\n",
    "# for i in range(n_neighbors+1):\n",
    "#     _, valid_losses, train_losses = train_model(models_univaritate_LSTM[f\"LSTMModel{i}\"][\"model\"], \n",
    "#                 models_univaritate_LSTM[f\"LSTMModel{i}\"][\"train_loader\"], \n",
    "#                 models_univaritate_LSTM[f\"LSTMModel{i}\"][\"val_loader\"], \n",
    "#                 f\"{path_save_model}/epoch_{num_epochs_LSTM_univariate}/univariate_LSTM_model_{i}.pkl\", num_epochs=num_epochs_LSTM_univariate, remove=False)\n",
    "#     plot_losses(train_losses, f\"LSTM train_losses captor {df_PeMS.columns[i]}\", \"train_losses\")\n",
    "#     plot_losses(valid_losses, f\"LSTM valid_losses captor {df_PeMS.columns[i]}\", \"valid_losses\")\n",
    "\n",
    "\n",
    "# # GRU\n",
    "# models_univaritate_GRU = create_multiple_model_with_one_captor(GRUModel(1, 32, 1), n_neighbors+1, \"GRUModel\", df_PeMS, _window_size, _stride)\n",
    "\n",
    "# for i in range(n_neighbors+1):\n",
    "#     _, valid_losses, train_losses = train_model(models_univaritate_GRU[f\"GRUModel{i}\"][\"model\"], \n",
    "#             models_univaritate_GRU[f\"GRUModel{i}\"][\"train_loader\"], \n",
    "#             models_univaritate_GRU[f\"GRUModel{i}\"][\"val_loader\"], \n",
    "#             f\"{path_save_model}/epoch_{num_epochs_GRU_univaritate}/univariate_GRU_model_{i}.pkl\", num_epochs=num_epochs_GRU_univaritate, remove=False)\n",
    "#     plot_losses(train_losses, f\"GRU train_losses captor {df_PeMS.columns[i]}\", \"train_losses\")\n",
    "#     plot_losses(valid_losses, f\"GRU valid_losses captor {df_PeMS.columns[i]}\", \"valid_losses\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results Multivariate vs Univariate (TGCN VS LSTM - GRU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# ######################################################################\n",
    "# # TGCN\n",
    "# ######################################################################\n",
    "# # load best model\n",
    "# model_TGCN.load_state_dict(torch.load(f\"{path_save_model}/epoch_{num_epochs_TGCN}/TGCN_model.pkl\".format(input)))\n",
    "\n",
    "# # Make predictions\n",
    "# predictions_TGCN, actuals_TGCN = test_model(model_TGCN, test_loader_TGCN)\n",
    "\n",
    "\n",
    "# for i in range(n_neighbors+1):\n",
    "#     # Save result for each captor\n",
    "#     temp = result_prediction(predictions_TGCN[:, i] * meanstd_dict[df_PeMS.columns[i]][\"std\"] + meanstd_dict[df_PeMS.columns[i]][\"mean\"], \n",
    "#                             actuals_TGCN[:, i] * meanstd_dict[df_PeMS.columns[i]][\"std\"] + meanstd_dict[df_PeMS.columns[i]][\"mean\"])\n",
    "#     temp.append(f\"TGCN Model captor {df_PeMS.columns[i]}\")\n",
    "#     data = [temp]\n",
    "\n",
    "\n",
    "# ######################################################################\n",
    "# # LSTM\n",
    "# ######################################################################\n",
    "#     # load best model\n",
    "#     models_univaritate_LSTM[f\"LSTMModel{i}\"][\"model\"].load_state_dict(torch.load(f\"{path_save_model}/epoch_{num_epochs_LSTM_univariate}/univariate_LSTM_model_{i}.pkl\".format(input)))\n",
    "    \n",
    "#     # Make predictions\n",
    "#     predictions_LSTM, actuals_LSTM = test_model(models_univaritate_LSTM[f\"LSTMModel{i}\"][\"model\"], \n",
    "#                                     models_univaritate_LSTM[f\"LSTMModel{i}\"][\"test_loader\"])\n",
    "\n",
    "#     # Save result\n",
    "#     temp = result_prediction(predictions_LSTM * meanstd_dict[df_PeMS.columns[i]][\"std\"] + meanstd_dict[df_PeMS.columns[i]][\"mean\"], \n",
    "#                             actuals_LSTM * meanstd_dict[df_PeMS.columns[i]][\"std\"] + meanstd_dict[df_PeMS.columns[i]][\"mean\"])\n",
    "\n",
    "#     temp.append(f\"LSTM Model captor {df_PeMS.columns[i]}\")\n",
    "#     data.append(temp)\n",
    "\n",
    "\n",
    "# ######################################################################\n",
    "# # GRU\n",
    "# ######################################################################\n",
    "#     # Load best model\n",
    "#     models_univaritate_GRU[f\"GRUModel{i}\"][\"model\"].load_state_dict(torch.load(f\"{path_save_model}/epoch_{num_epochs_GRU_univaritate}/univariate_GRU_model_{i}.pkl\".format(input)))\n",
    "#     predictions_GRU, actuals_GRU = test_model(models_univaritate_GRU[f\"GRUModel{i}\"][\"model\"], \n",
    "#                                     models_univaritate_GRU[f\"GRUModel{i}\"][\"test_loader\"])\n",
    "\n",
    "#     # Make predictions\n",
    "#     temp = result_prediction(predictions_GRU * meanstd_dict[df_PeMS.columns[i]][\"std\"] + meanstd_dict[df_PeMS.columns[i]][\"mean\"]\n",
    "#                             , actuals_GRU * meanstd_dict[df_PeMS.columns[i]][\"std\"] + meanstd_dict[df_PeMS.columns[i]][\"mean\"])\n",
    "\n",
    "#     # Save results\n",
    "#     temp.append(f\"GRU Model captor {df_PeMS.columns[i]}\")\n",
    "#     data.append(temp)\n",
    "\n",
    "\n",
    "# ######################################################################\n",
    "# # Dataframe\n",
    "# ######################################################################\n",
    "#     df = pd.DataFrame(data, columns=['Signe error', 'MAE', 'RMSE', 'MAPE', 'SMAPE', 'MAAPE', \"Captor\"])\n",
    "\n",
    "#     # Définir le mois comme index du DataFrame\n",
    "#     df.set_index('Captor', inplace=True)\n",
    "\n",
    "#     pd.set_option('display.max_columns', None)\n",
    "#     pd.set_option('display.max_rows', None)\n",
    "#     pd.set_option('display.width', None)\n",
    "\n",
    "#     min_val = df['MAAPE'].min()\n",
    "#     max_val = df['MAAPE'].max()\n",
    "\n",
    "#     df = df.style.apply(highlight_rows, axis=1, min_val=min_val, max_val=max_val)\n",
    "#     display(df)\n",
    "\n",
    "\n",
    "# ######################################################################\n",
    "# # Plots\n",
    "# ######################################################################\n",
    "#     plt.figure(figsize=(21, 10))\n",
    "#     plt.plot(predictions_TGCN[:, i] * meanstd_dict[df_PeMS.columns[i]][\"std\"] + meanstd_dict[df_PeMS.columns[i]][\"mean\"], color=\"orange\", label=\"TGCN pred\")\n",
    "#     plt.plot(actuals_LSTM * meanstd_dict[df_PeMS.columns[i]][\"std\"] + meanstd_dict[df_PeMS.columns[i]][\"mean\"], color=\"black\", label=\"true value\")\n",
    "#     plt.legend(fontsize=15)\n",
    "#     plt.show()\n",
    "    \n",
    "#     plt.figure(figsize=(21, 10))\n",
    "#     plt.plot(predictions_LSTM * meanstd_dict[df_PeMS.columns[i]][\"std\"] + meanstd_dict[df_PeMS.columns[i]][\"mean\"], color=\"magenta\", label=\"LSTM pred\")\n",
    "#     plt.plot(actuals_LSTM * meanstd_dict[df_PeMS.columns[i]][\"std\"] + meanstd_dict[df_PeMS.columns[i]][\"mean\"], color=\"black\", label=\"true value\")\n",
    "#     plt.legend(fontsize=15)\n",
    "#     plt.show()\n",
    "    \n",
    "#     plt.figure(figsize=(21, 10))\n",
    "#     plt.plot(predictions_GRU * meanstd_dict[df_PeMS.columns[i]][\"std\"] + meanstd_dict[df_PeMS.columns[i]][\"mean\"], color=\"red\", label=\"GRU pred\" )\n",
    "#     plt.plot(actuals_LSTM * meanstd_dict[df_PeMS.columns[i]][\"std\"] + meanstd_dict[df_PeMS.columns[i]][\"mean\"], color=\"black\", label=\"true value\")\n",
    "#     plt.legend(fontsize=15)\n",
    "#     plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multivariate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM\n",
    "num_epochs_LSTM_multivariate = 300\n",
    "\n",
    "# GRU\n",
    "num_epochs_GRU_multivariate = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # LSTM\n",
    "# model_multivariate_LSTM = LSTMModel(len(df_PeMS.columns), 32, len(df_PeMS.columns))\n",
    "# train_loader_LSTM, val_loader_LSTM, test_loader_LSTM = createLoaders(df_PeMS, window_size=_window_size, stride=_stride, target_size=horizon)\n",
    "# _ , valid_losses, train_losses = train_model(model_multivariate_LSTM, train_loader_LSTM, val_loader_LSTM, \n",
    "#                                             f\"./{path_save_model}/epoch_{num_epochs_LSTM_multivariate}/multivariate_LSTM_model.pkl\", \n",
    "#                                             num_epochs=num_epochs_LSTM_multivariate, remove=False)\n",
    "# plot_losses(train_losses, \"LSTM train_losses\", \"train_losses\")\n",
    "# plot_losses(valid_losses, \"LSTM valid losses\", \"valid_losses\")\n",
    "\n",
    "# # GRU\n",
    "# model_multivariate_GRU = GRUModel(len(df_PeMS.columns), 32, len(df_PeMS.columns))\n",
    "# train_loader_GRU, val_loader_GRU, test_loader_GRU = createLoaders(df_PeMS, window_size=_window_size, stride=_stride, target_size=horizon)\n",
    "# _ , valid_losses, train_losses = train_model(model_multivariate_GRU, train_loader_GRU, val_loader_GRU, \n",
    "#                                             f\"{path_save_model}/epoch_{num_epochs_GRU_multivariate}/multivariate_GRU_model.pkl\", \n",
    "#                                             num_epochs=num_epochs_GRU_multivariate, remove=False)\n",
    "# plot_losses(train_losses, \"GRU train_losses\", \"train_losses\")\n",
    "# plot_losses(valid_losses, \"GRU valid losses\", \"valid_losses\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results Multivariate vs Multivariate (TGCN VS LSTM - GRU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# final_resultats = []\n",
    "# diff_resultats = []\n",
    "\n",
    "# ######################################################################\n",
    "# # TGCN\n",
    "# ######################################################################\n",
    "# # load best model\n",
    "# model_TGCN.load_state_dict(torch.load(f\"{path_save_model}/epoch_{num_epochs_TGCN}/TGCN_model.pkl\".format(input)))\n",
    "\n",
    "# # Make predictions\n",
    "# predictions_TGCN, actuals_TGCN = test_model(model_TGCN, test_loader_TGCN)\n",
    "\n",
    "# nb_row, _, _ = predictions_TGCN.shape\n",
    "# predictions_TGCN = predictions_TGCN.reshape(nb_row * horizon, n_neighbors+1)\n",
    "# actuals_TGCN = actuals_TGCN.reshape(nb_row * horizon, n_neighbors+1)\n",
    "\n",
    "# ######################################################################\n",
    "# # LSTM\n",
    "# ######################################################################\n",
    "# # load best model\n",
    "# model_multivariate_LSTM.load_state_dict(torch.load(f\"{path_save_model}/epoch_{num_epochs_LSTM_multivariate}/multivariate_LSTM_model.pkl\".format(input)))\n",
    "    \n",
    "# # Make predictions\n",
    "# predictions_LSTM, actuals_LSTM = test_model(model_multivariate_LSTM, \n",
    "#                                 test_loader_LSTM)\n",
    "\n",
    "# predictions_LSTM = predictions_LSTM.reshape(nb_row * horizon, n_neighbors+1)\n",
    "# actuals_LSTM = actuals_LSTM.reshape(nb_row * horizon, n_neighbors+1)\n",
    "\n",
    "\n",
    "# ######################################################################\n",
    "# # GRU\n",
    "# ######################################################################\n",
    "# # Load best model\n",
    "# model_multivariate_GRU.load_state_dict(torch.load(f\"{path_save_model}/epoch_{num_epochs_GRU_multivariate}/multivariate_GRU_model.pkl\".format(input)))\n",
    "\n",
    "# # Make predictions\n",
    "# predictions_GRU, actuals_GRU = test_model(model_multivariate_GRU, \n",
    "#                                 test_loader_GRU)\n",
    "\n",
    "# predictions_GRU = predictions_GRU.reshape(nb_row * horizon, n_neighbors+1)\n",
    "# actuals_GRU = actuals_GRU.reshape(nb_row * horizon, n_neighbors+1)\n",
    "\n",
    "# for i in range(n_neighbors+1):\n",
    "\n",
    "# ######################################################################\n",
    "# # TGCN\n",
    "# ######################################################################\n",
    "#     # Save result for each captor\n",
    "#     temp = result_prediction(predictions_TGCN[:, i] * meanstd_dict[df_PeMS.columns[i]][\"std\"] + meanstd_dict[df_PeMS.columns[i]][\"mean\"], \n",
    "#                             actuals_TGCN[:, i] * meanstd_dict[df_PeMS.columns[i]][\"std\"] + meanstd_dict[df_PeMS.columns[i]][\"mean\"])\n",
    "#     temp.append(\"TGCN Model\")\n",
    "#     temp.append(f\"captor {df_PeMS.columns[i]}\")\n",
    "#     data = [temp]\n",
    "#     final_resultats.append(temp)\n",
    "\n",
    "# ######################################################################\n",
    "# # LSTM\n",
    "# ######################################################################\n",
    "#     # Save result\n",
    "#     temp = result_prediction(predictions_LSTM[:, i] * meanstd_dict[df_PeMS.columns[i]][\"std\"] + meanstd_dict[df_PeMS.columns[i]][\"mean\"], \n",
    "#                             actuals_LSTM[:, i] * meanstd_dict[df_PeMS.columns[i]][\"std\"] + meanstd_dict[df_PeMS.columns[i]][\"mean\"])\n",
    "\n",
    "#     temp.append(\"LSTM Model\")\n",
    "#     temp.append(f\"captor {df_PeMS.columns[i]}\")\n",
    "#     data.append(temp)\n",
    "#     final_resultats.append(temp)\n",
    "\n",
    "\n",
    "# ######################################################################\n",
    "# # GRU\n",
    "# ######################################################################\n",
    "#     # Save results\n",
    "#     temp = result_prediction(predictions_GRU[:, i] * meanstd_dict[df_PeMS.columns[i]][\"std\"] + meanstd_dict[df_PeMS.columns[i]][\"mean\"], \n",
    "#                             actuals_GRU[:, i] * meanstd_dict[df_PeMS.columns[i]][\"std\"] + meanstd_dict[df_PeMS.columns[i]][\"mean\"])\n",
    "#     temp.append(\"GRU Model\")\n",
    "#     temp.append(f\"captor {df_PeMS.columns[i]}\")\n",
    "#     data.append(temp)\n",
    "#     final_resultats.append(temp)\n",
    "\n",
    "\n",
    "# ######################################################################\n",
    "# # Dataframe\n",
    "# ######################################################################\n",
    "#     df = pd.DataFrame(data, columns=['Signe error', 'MAE', 'RMSE', 'MAPE', 'SMAPE', 'MAAPE', \"Model_Name\", \"Captor\"])\n",
    "\n",
    "#     df.set_index([\"Captor\", 'Model_Name'], inplace=True)\n",
    "\n",
    "#     pd.set_option('display.max_columns', None)\n",
    "#     pd.set_option('display.max_rows', None)\n",
    "#     pd.set_option('display.width', None)\n",
    "\n",
    "#     min_val = df['MAAPE'].min()\n",
    "#     max_val = df['MAAPE'].max()\n",
    "\n",
    "#     df = df.style.apply(highlight_rows, axis=1, min_val=min_val, max_val=max_val)\n",
    "\n",
    "# #######################################################################\n",
    "# # Plots\n",
    "# #######################################################################\n",
    "# final_resultats = pd.DataFrame(final_resultats, columns=['Signe error', 'MAE', 'RMSE', 'MAPE', 'SMAPE', 'MAAPE', \"Model_Name\", \"Captor\"])\n",
    "# final_resultats.set_index([\"Captor\", 'Model_Name'], inplace=True)\n",
    "# pd.set_option('display.max_columns', None)\n",
    "# pd.set_option('display.max_rows', None)\n",
    "# pd.set_option('display.width', None)\n",
    "# display(final_resultats.groupby(\"Model_Name\")[[\"RMSE\", \"SMAPE\", \"MAAPE\"]].describe())\n",
    "# final_resultats.to_pickle(f\"{path_save_model}/final_resultats.pkl\")\n",
    "\n",
    "# stats = {\n",
    "#     \"TGCN Model\" : {\"nb_win\" : 0},\n",
    "#     \"LSTM Model\" : {\"nb_win\" : 0},\n",
    "#     \"GRU Model\" :  {\"nb_win\" : 0}\n",
    "# }\n",
    "\n",
    "# for i in final_resultats.index.get_level_values(\"Captor\").unique():\n",
    "#     stats[final_resultats.loc[i][\"MAAPE\"].idxmin()][\"nb_win\"] = stats[final_resultats.loc[i][\"MAAPE\"].idxmin()][\"nb_win\"] + 1\n",
    "#     diff_resultats.append([\"TGCN vs LSTM\", round(final_resultats.loc[i].loc[\"TGCN Model\"][\"MAAPE\"] - final_resultats.loc[i].loc[\"LSTM Model\"][\"MAAPE\"], 2)])\n",
    "#     diff_resultats.append([\"TGCN vs GRU\", round(final_resultats.loc[i].loc[\"TGCN Model\"][\"MAAPE\"] - final_resultats.loc[i].loc[\"GRU Model\"][\"MAAPE\"], 2)])\n",
    "#     diff_resultats.append([\"LSTM vs TGCN\", round(final_resultats.loc[i].loc[\"LSTM Model\"][\"MAAPE\"] - final_resultats.loc[i].loc[\"TGCN Model\"][\"MAAPE\"], 2)])\n",
    "#     diff_resultats.append([\"LSTM vs GRU\", round(final_resultats.loc[i].loc[\"LSTM Model\"][\"MAAPE\"] - final_resultats.loc[i].loc[\"GRU Model\"][\"MAAPE\"], 2)])\n",
    "#     diff_resultats.append([\"GRU vs TGCN\", round(final_resultats.loc[i].loc[\"GRU Model\"][\"MAAPE\"] - final_resultats.loc[i].loc[\"TGCN Model\"][\"MAAPE\"], 2)])\n",
    "#     diff_resultats.append([\"GRU vs LSTM\", round(final_resultats.loc[i].loc[\"GRU Model\"][\"MAAPE\"] - final_resultats.loc[i].loc[\"LSTM Model\"][\"MAAPE\"], 2)])\n",
    "# diff_resultats = pd.DataFrame(diff_resultats, columns=[\"Diff\", \"MAAPE\"])\n",
    "# diff_resultats.set_index([\"Diff\"], inplace=True)\n",
    "# display(diff_resultats.groupby(\"Diff\").describe())\n",
    "# diff_resultats.groupby(\"Diff\").describe().to_pickle(f\"{path_save_model}/diff_results.pkl\")\n",
    "\n",
    "# import numpy as np\n",
    "\n",
    "# bar_plot_results = final_resultats.reset_index()\n",
    "# display(bar_plot_results.boxplot(column=\"MAAPE\", by=\"Model_Name\", fontsize=10, figsize=(8,8), ylabel=\"MAAPE values\", xlabel=\"Model Name\"))\n",
    "# plt.yticks(np.arange(0, bar_plot_results[\"MAAPE\"].max(), 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final_resultats = pd.read_pickle(f\"{path_save_model}/final_resultats.pkl\")\n",
    "# display(final_resultats.groupby(\"Model_Name\")[[\"RMSE\", \"SMAPE\", \"MAAPE\"]].describe())\n",
    "# diff_results = pd.read_pickle(f\"{path_save_model}/diff_results.pkl\")\n",
    "# display(diff_results)\n",
    "\n",
    "# bar_plot_results = final_resultats.reset_index()\n",
    "# display(bar_plot_results.boxplot(column=\"MAAPE\", by=\"Model_Name\", fontsize=10, figsize=(8,8), ylabel=\"MAAPE values\", xlabel=\"Model Name\"))\n",
    "# plt.yticks(np.arange(0, bar_plot_results[\"MAAPE\"].max(), 4))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi_Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gcogoni\\Documents\\Stage\\Federated-Traffic-Flow-Forecasting\\src\\utils_graph.py:98: FutureWarning: adjacency_matrix will return a scipy.sparse array instead of a matrix in Networkx 3.0.\n",
      "  return nx.adjacency_matrix(graph, nodelist=nodes_order, weight=None).toarray()\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[31], line 27\u001b[0m\n\u001b[0;32m     25\u001b[0m train_loader_TGCN, val_loader_TGCN, test_loader_TGCN, _ \u001b[39m=\u001b[39m createLoaders(df_PeMS, window_size\u001b[39m=\u001b[39m_window_size, stride\u001b[39m=\u001b[39m_stride, prediction_horizon\u001b[39m=\u001b[39mhorizon)\n\u001b[0;32m     26\u001b[0m model_path \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mpath_save_model\u001b[39m}\u001b[39;00m\u001b[39m/epoch_\u001b[39m\u001b[39m{\u001b[39;00mnum_epochs_TGCN\u001b[39m}\u001b[39;00m\u001b[39m/TGCN_model.pkl\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m---> 27\u001b[0m _ , _, _ \u001b[39m=\u001b[39m train_model(model_TGCN, train_loader_TGCN, val_loader_TGCN, model_path\u001b[39m=\u001b[39;49mmodel_path, num_epochs\u001b[39m=\u001b[39;49mnum_epochs_TGCN, remove\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[0;32m     30\u001b[0m \u001b[39m# LSTM Model\u001b[39;00m\n\u001b[0;32m     31\u001b[0m model_multivariate_LSTM \u001b[39m=\u001b[39m LSTMModel(\u001b[39mlen\u001b[39m(df_PeMS\u001b[39m.\u001b[39mcolumns), \u001b[39m32\u001b[39m, \u001b[39mlen\u001b[39m(df_PeMS\u001b[39m.\u001b[39mcolumns))\n",
      "File \u001b[1;32mc:\\Users\\gcogoni\\Documents\\Stage\\Federated-Traffic-Flow-Forecasting\\src\\utils_training.py:53\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(model, train_loader, val_loader, model_path, num_epochs, remove, learning_rate)\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(num_epochs):\n\u001b[0;32m     51\u001b[0m     train_loss \u001b[39m=\u001b[39m \u001b[39m0.0\u001b[39m\n\u001b[1;32m---> 53\u001b[0m     \u001b[39mfor\u001b[39;00m inputs, targets \u001b[39min\u001b[39;00m train_loader:\n\u001b[0;32m     55\u001b[0m         inputs, targets \u001b[39m=\u001b[39m inputs\u001b[39m.\u001b[39mto(device), targets\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m     56\u001b[0m         batch_size, horizon_size, num_nodes \u001b[39m=\u001b[39m targets\u001b[39m.\u001b[39msize()\n",
      "File \u001b[1;32mc:\\Users\\gcogoni\\Documents\\Stage\\Federated-Traffic-Flow-Forecasting\\env\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:634\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    631\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    632\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    633\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 634\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[0;32m    635\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    636\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    637\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    638\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\gcogoni\\Documents\\Stage\\Federated-Traffic-Flow-Forecasting\\env\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:678\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    676\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    677\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 678\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    679\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[0;32m    680\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\gcogoni\\Documents\\Stage\\Federated-Traffic-Flow-Forecasting\\env\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\gcogoni\\Documents\\Stage\\Federated-Traffic-Flow-Forecasting\\env\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\gcogoni\\Documents\\Stage\\Federated-Traffic-Flow-Forecasting\\src\\utils_data.py:161\u001b[0m, in \u001b[0;36mTimeSeriesDataset.__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m    158\u001b[0m     inputs \u001b[39m=\u001b[39m inputs[:\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwindow_size]\n\u001b[0;32m    160\u001b[0m \u001b[39m# Convertir les données d'entrée en tenseur PyTorch\u001b[39;00m\n\u001b[1;32m--> 161\u001b[0m inputs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mfrom_numpy(inputs)\u001b[39m.\u001b[39;49mfloat()\n\u001b[0;32m    163\u001b[0m \u001b[39m# Calculer le début et la fin de la fenêtre de sortie\u001b[39;00m\n\u001b[0;32m    164\u001b[0m start \u001b[39m=\u001b[39m end\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Not a parameter just the name of normalization use in preprocess_PeMS_data()\n",
    "normalization = \"center_and_reduce\"\n",
    "\n",
    "# Define the sliding window size, stride and horizon\n",
    "_window_size = 6\n",
    "horizon = 6\n",
    "_stride = 1\n",
    "\n",
    "# TGCN\n",
    "num_epochs_TGCN = 300\n",
    "# LSTM\n",
    "num_epochs_LSTM_multivariate = 300\n",
    "# GRU\n",
    "num_epochs_GRU_multivariate = 300\n",
    "\n",
    "for i in range(10, 100, 10):\n",
    "    n_neighbors = i\n",
    "    path_save_model = f\"./{normalization}/nb_captor_{n_neighbors+1}/windows_{_window_size}_out_{horizon}\"\n",
    "    df_PeMS, adjacency_matrix_PeMS, meanstd_dict = preprocess_PeMS_data(df_PeMS_old, df_distance, init_node=0, n_neighbors=n_neighbors, center_and_reduce=True)\n",
    "    \n",
    "    # TGCN Model\n",
    "    model_TGCN = TGCN(adjacency_matrix_PeMS, hidden_dim=32, output_size=len(df_PeMS.columns))\n",
    "    train_loader_TGCN, val_loader_TGCN, test_loader_TGCN, _ = createLoaders(df_PeMS, window_size=_window_size, stride=_stride, prediction_horizon=horizon)\n",
    "    model_path = f\"{path_save_model}/epoch_{num_epochs_TGCN}/TGCN_model.pkl\"\n",
    "    _ , _, _ = train_model(model_TGCN, train_loader_TGCN, val_loader_TGCN, model_path=model_path, num_epochs=num_epochs_TGCN, remove=False)\n",
    "\n",
    "\n",
    "    # LSTM Model\n",
    "    model_multivariate_LSTM = LSTMModel(len(df_PeMS.columns), 32, len(df_PeMS.columns))\n",
    "    train_loader_LSTM, val_loader_LSTM, test_loader_LSTM, _ = createLoaders(df_PeMS, window_size=_window_size, stride=_stride, prediction_horizon=horizon)\n",
    "    _ , _, _ = train_model(model_multivariate_LSTM, train_loader_LSTM, val_loader_LSTM, \n",
    "                                                f\"{path_save_model}/epoch_{num_epochs_LSTM_multivariate}/multivariate_LSTM_model.pkl\", \n",
    "                                                num_epochs=num_epochs_LSTM_multivariate, remove=False)\n",
    "\n",
    "    # GRU Model\n",
    "    model_multivariate_GRU = GRUModel(len(df_PeMS.columns), 32, len(df_PeMS.columns))\n",
    "    train_loader_GRU, val_loader_GRU, test_loader_GRU, _ = createLoaders(df_PeMS, window_size=_window_size, stride=_stride, prediction_horizon=horizon)\n",
    "    _ , _, _ = train_model(model_multivariate_GRU, train_loader_GRU, val_loader_GRU, \n",
    "                                                f\"{path_save_model}/epoch_{num_epochs_GRU_multivariate}/multivariate_GRU_model.pkl\", \n",
    "                                                num_epochs=num_epochs_GRU_multivariate, remove=False)\n",
    "\n",
    "\n",
    "    final_resultats = []\n",
    "    diff_resultats = []\n",
    "\n",
    "\n",
    "    ######################################################################\n",
    "    # TGCN\n",
    "    ######################################################################\n",
    "    # load best model\n",
    "    model_TGCN.load_state_dict(torch.load(f\"{path_save_model}/epoch_{num_epochs_TGCN}/TGCN_model.pkl\".format(input)))\n",
    "\n",
    "    # Make predictions\n",
    "    predictions_TGCN, actuals_TGCN = test_model(model_TGCN, test_loader_TGCN)\n",
    "\n",
    "    nb_row, _, _ = predictions_TGCN.shape\n",
    "    predictions_TGCN = predictions_TGCN.reshape(nb_row * horizon, n_neighbors+1)\n",
    "    actuals_TGCN = actuals_TGCN.reshape(nb_row * horizon, n_neighbors+1)\n",
    "\n",
    "\n",
    "    ######################################################################\n",
    "    # LSTM\n",
    "    ######################################################################\n",
    "    # load best model\n",
    "    model_multivariate_LSTM.load_state_dict(torch.load(f\"{path_save_model}/epoch_{num_epochs_LSTM_multivariate}/multivariate_LSTM_model.pkl\".format(input)))\n",
    "        \n",
    "    # Make predictions\n",
    "    predictions_LSTM, actuals_LSTM = test_model(model_multivariate_LSTM, \n",
    "                                    test_loader_LSTM)\n",
    "    \n",
    "    predictions_LSTM = predictions_LSTM.reshape(nb_row * horizon, n_neighbors+1)\n",
    "    actuals_LSTM = actuals_LSTM.reshape(nb_row * horizon, n_neighbors+1)\n",
    "\n",
    "\n",
    "    ######################################################################\n",
    "    # GRU\n",
    "    ######################################################################\n",
    "    # Load best model\n",
    "    model_multivariate_GRU.load_state_dict(torch.load(f\"{path_save_model}/epoch_{num_epochs_GRU_multivariate}/multivariate_GRU_model.pkl\".format(input)))\n",
    "\n",
    "    # Make predictions\n",
    "    predictions_GRU, actuals_GRU = test_model(model_multivariate_GRU, \n",
    "                                    test_loader_GRU)\n",
    "    \n",
    "    predictions_GRU = predictions_GRU.reshape(nb_row * horizon, n_neighbors+1)\n",
    "    actuals_GRU = actuals_GRU.reshape(nb_row * horizon, n_neighbors+1)\n",
    "\n",
    "    for i in range(n_neighbors+1):\n",
    "\n",
    "        # TGCN\n",
    "        # Save result for each captor\n",
    "        temp = result_prediction(predictions_TGCN[:, i] * meanstd_dict[df_PeMS.columns[i]][\"std\"] + meanstd_dict[df_PeMS.columns[i]][\"mean\"], \n",
    "                                actuals_TGCN[:, i] * meanstd_dict[df_PeMS.columns[i]][\"std\"] + meanstd_dict[df_PeMS.columns[i]][\"mean\"])\n",
    "        temp.append(\"TGCN Model\")\n",
    "        temp.append(f\"captor {df_PeMS.columns[i]}\")\n",
    "        data = [temp]\n",
    "        final_resultats.append(temp)\n",
    "\n",
    "        # LSTM\n",
    "        # Save result for each captor\n",
    "        temp = result_prediction(predictions_LSTM[:, i] * meanstd_dict[df_PeMS.columns[i]][\"std\"] + meanstd_dict[df_PeMS.columns[i]][\"mean\"], \n",
    "                                actuals_LSTM[:, i] * meanstd_dict[df_PeMS.columns[i]][\"std\"] + meanstd_dict[df_PeMS.columns[i]][\"mean\"])\n",
    "\n",
    "        temp.append(\"LSTM Model\")\n",
    "        temp.append(f\"captor {df_PeMS.columns[i]}\")\n",
    "        data.append(temp)\n",
    "        final_resultats.append(temp)\n",
    "\n",
    "\n",
    "        # GRU\n",
    "        # Save result for each captor\n",
    "        temp = result_prediction(predictions_GRU[:, i] * meanstd_dict[df_PeMS.columns[i]][\"std\"] + meanstd_dict[df_PeMS.columns[i]][\"mean\"], \n",
    "                                actuals_GRU[:, i] * meanstd_dict[df_PeMS.columns[i]][\"std\"] + meanstd_dict[df_PeMS.columns[i]][\"mean\"])\n",
    "        temp.append(\"GRU Model\")\n",
    "        temp.append(f\"captor {df_PeMS.columns[i]}\")\n",
    "        data.append(temp)\n",
    "        final_resultats.append(temp)\n",
    "\n",
    "\n",
    "    ######################################################################\n",
    "    # Dataframe\n",
    "    ######################################################################\n",
    "        df = pd.DataFrame(data, columns=['Signe error', 'MAE', 'RMSE', 'MAPE', 'SMAPE', 'MAAPE', \"Model_Name\", \"Captor\"])\n",
    "\n",
    "        df.set_index([\"Captor\", 'Model_Name'], inplace=True)\n",
    "\n",
    "        pd.set_option('display.max_columns', None)\n",
    "        pd.set_option('display.max_rows', None)\n",
    "        pd.set_option('display.width', None)\n",
    "\n",
    "        min_val = df['MAAPE'].min()\n",
    "        max_val = df['MAAPE'].max()\n",
    "\n",
    "        df = df.style.apply(highlight_rows, axis=1, min_val=min_val, max_val=max_val)\n",
    "\n",
    "\n",
    "    final_resultats = pd.DataFrame(final_resultats, columns=['Signe error', 'MAE', 'RMSE', 'MAPE', 'SMAPE', 'MAAPE', \"Model_Name\", \"Captor\"])\n",
    "    final_resultats.set_index([\"Captor\", 'Model_Name'], inplace=True)\n",
    "    pd.set_option('display.max_columns', None)\n",
    "    pd.set_option('display.max_rows', None)\n",
    "    pd.set_option('display.width', None)\n",
    "    display(final_resultats.groupby(\"Model_Name\")[[\"RMSE\", \"SMAPE\", \"MAAPE\"]].describe())\n",
    "    final_resultats.to_pickle(f\"{path_save_model}/final_resultats.pkl\")\n",
    "\n",
    "    stats = {\n",
    "        \"TGCN Model\" : {\"nb_win\" : 0},\n",
    "        \"LSTM Model\" : {\"nb_win\" : 0},\n",
    "        \"GRU Model\" :  {\"nb_win\" : 0}\n",
    "    }\n",
    "\n",
    "    for i in final_resultats.index.get_level_values(\"Captor\").unique():\n",
    "        stats[final_resultats.loc[i][\"MAAPE\"].idxmin()][\"nb_win\"] = stats[final_resultats.loc[i][\"MAAPE\"].idxmin()][\"nb_win\"] + 1\n",
    "        diff_resultats.append([\"TGCN vs LSTM\", round(final_resultats.loc[i].loc[\"TGCN Model\"][\"MAAPE\"] - final_resultats.loc[i].loc[\"LSTM Model\"][\"MAAPE\"], 2)])\n",
    "        diff_resultats.append([\"TGCN vs GRU\", round(final_resultats.loc[i].loc[\"TGCN Model\"][\"MAAPE\"] - final_resultats.loc[i].loc[\"GRU Model\"][\"MAAPE\"], 2)])\n",
    "        diff_resultats.append([\"LSTM vs TGCN\", round(final_resultats.loc[i].loc[\"LSTM Model\"][\"MAAPE\"] - final_resultats.loc[i].loc[\"TGCN Model\"][\"MAAPE\"], 2)])\n",
    "        diff_resultats.append([\"LSTM vs GRU\", round(final_resultats.loc[i].loc[\"LSTM Model\"][\"MAAPE\"] - final_resultats.loc[i].loc[\"GRU Model\"][\"MAAPE\"], 2)])\n",
    "        diff_resultats.append([\"GRU vs TGCN\", round(final_resultats.loc[i].loc[\"GRU Model\"][\"MAAPE\"] - final_resultats.loc[i].loc[\"TGCN Model\"][\"MAAPE\"], 2)])\n",
    "        diff_resultats.append([\"GRU vs LSTM\", round(final_resultats.loc[i].loc[\"GRU Model\"][\"MAAPE\"] - final_resultats.loc[i].loc[\"LSTM Model\"][\"MAAPE\"], 2)])\n",
    "    diff_resultats = pd.DataFrame(diff_resultats, columns=[\"Diff\", \"MAAPE\"])\n",
    "    diff_resultats.set_index([\"Diff\"], inplace=True)\n",
    "    display(diff_resultats.groupby(\"Diff\").describe())\n",
    "    diff_resultats.groupby(\"Diff\").describe().to_pickle(f\"{path_save_model}/diff_results.pkl\")\n",
    "\n",
    "    #######################################################################\n",
    "    # Plots\n",
    "    #######################################################################\n",
    "    import numpy as np\n",
    "\n",
    "    bar_plot_results = final_resultats.reset_index()\n",
    "    display(bar_plot_results.boxplot(column=\"MAAPE\", by=\"Model_Name\", fontsize=10, figsize=(8,8), ylabel=\"MAAPE values\", xlabel=\"Model Name\"))\n",
    "    plt.yticks(np.arange(0, bar_plot_results[\"MAAPE\"].max(), 4))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
