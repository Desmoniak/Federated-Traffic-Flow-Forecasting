{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the sliding window size and stride\n",
    "# Define a PyTorch dataset to generate input/target pairs for the LSTM model\n",
    "class TimeSeriesDataset(Dataset):\n",
    "    def __init__(self, data, window_size=7, stride=1):\n",
    "        self.data = data\n",
    "        self.window_size = window_size\n",
    "        self.stride = stride\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data) - self.window_size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        inputs = self.data[idx:idx+self.window_size]\n",
    "        target = self.data[idx+self.window_size]\n",
    "        return inputs, target\n",
    "\n",
    "# Define your LSTM model here with 6 LSTM layers and 1 fully connected layer\n",
    "class LSTMModel(torch.nn.Module):\n",
    "    def __init__(self, input_size, hidden_size,output_size, num_layers=6):\n",
    "        super().__init__()\n",
    "        self.lstm = torch.nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = torch.nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, _ = self.lstm(x)\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.holtwinters import ExponentialSmoothing\n",
    "def ExpSmooth(PeMS,alpha=0.2):\n",
    "    \n",
    "    # Apply exponential smoothing to the time serie\n",
    "    for i in range(len(PeMS.columns)):\n",
    "        y = PeMS[PeMS.columns[i]]\n",
    "        model = ExponentialSmoothing(y).fit(smoothing_level=alpha)\n",
    "        smooth = model.fittedvalues\n",
    "        PeMS[PeMS.columns[i]] = smooth\n",
    "    return PeMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_data_loader(data, window_size = 7, stride = 1):\n",
    "    dataset = TimeSeriesDataset(data.values, window_size, stride)\n",
    "    loader = DataLoader(dataset, batch_size=32, shuffle=False)\n",
    "    loader = [(inputs.to(device), targets.to(device)) for inputs, targets in loader]\n",
    "    return loader\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def experiment_dataset(cluster_size, df, train_len=0):\n",
    "    cluster_dict={\"size\":cluster_size}\n",
    "    for i in range(len(PeMS.columns)+1-cluster_size):\n",
    "        model = LSTMModel(input_size=cluster_size, hidden_size=32, num_layers=layers, output_size=cluster_size)\n",
    "        if train_len == 0 :\n",
    "            train_data= df[df.columns[i:i+cluster_size]][:'2018-02-10 00:00:00']\n",
    "            val_data =  df[df.columns[i:i+cluster_size]]['2018-02-10 00:00:00':'2018-02-14 00:00:00']\n",
    "            test_data = df[df.columns[i:i+cluster_size]]['2018-02-14 00:00:00':] \n",
    "        else :\n",
    "            train_data= df[df.columns[i:i+cluster_size]][:train_len]\n",
    "            val_data =  df[df.columns[i:i+cluster_size]][train_len: train_len*1.2]\n",
    "            test_data = df[df.columns[i:i+cluster_size]][train_len*1.2:train_len*1.4] \n",
    "\n",
    "        train_loader = my_data_loader(train_data)\n",
    "        val_loader = my_data_loader(val_data)\n",
    "        test_loader = my_data_loader(test_data) \n",
    "            \n",
    "        cluster_dict[i]={\"model\":model,\"train\":train_loader,\"val\":val_loader,\"test\":test_loader}\n",
    "    with open('./experiment/clusterS{}l{}.pkl'.format(cluster_size,train_len), 'wb') as f:\n",
    "        pickle.dump(cluster_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5838ee9d1e27b0c8776a2c47d844e546f7c02646b3dffe8b9f56b6c67f7dd71a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
