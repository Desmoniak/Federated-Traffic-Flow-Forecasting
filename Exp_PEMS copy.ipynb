{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "flow_file= \"./data/PEMS04/PEMS04.npz\"\n",
    "csv_file = \"./data/PEMS04/distance.csv\"\n",
    "\n",
    "data = np.load(flow_file)\n",
    "df = pd.read_csv(csv_file)\n",
    "TS = data['data']\n",
    "flow = TS[:,:,0]\n",
    "# flow dict 100 time series is the sensor number and the value the traffic flow times serie\n",
    "flow_dict={k:flow[:,k] for k in range(307)}\n",
    "# list of the first 10 connected sensor, each sensor traffic flow is contained in PeMS \n",
    "PeMS = pd.DataFrame(flow_dict)\n",
    "# time serie of sensor k\n",
    "#creation of the datetime index\n",
    "start_date = \"2018-01-01 00:00:00\"\n",
    "end_date = \"2018-02-28 23:55:00\"\n",
    "interval = \"5min\"\n",
    "index = pd.date_range(start=start_date, end=end_date, freq=interval)\n",
    "PeMS = PeMS.set_index(index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "# Create a new NetworkX graph object\n",
    "G = nx.Graph()\n",
    "\n",
    "# Iterate over each row in the DataFrame and add nodes and edges to the graph\n",
    "for i, row in df.iterrows():\n",
    "    # Add the \"from\" node to the graph if it doesn't already exist\n",
    "    if not G.has_node(row[\"from\"]):\n",
    "        G.add_node(row[\"from\"])\n",
    "    # Add the \"to\" node to the graph if it doesn't already exist\n",
    "    if not G.has_node(row[\"to\"]):\n",
    "        G.add_node(row[\"to\"])\n",
    "    # Add the edge between the \"from\" and \"to\" nodes with the cost as the edge weight\n",
    "    G.add_edge(row[\"from\"], row[\"to\"], weight=row[\"cost\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the shortest path from node 0 to all other nodes\n",
    "distances = nx.single_source_dijkstra_path_length(G, 0)\n",
    "\n",
    "# Sort the nodes based on their distance from node 0\n",
    "nearest_nodes = sorted(distances, key=distances.get)[:30]\n",
    "\n",
    "# Construct the subgraph by including only the selected nodes and their edges\n",
    "subgraph = G.subgraph(nearest_nodes)\n",
    "G = subgraph\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from statsmodels.tsa.holtwinters import ExponentialSmoothing\n",
    "# Apply exponential smoothing to the time serie\n",
    "# Define the smoothing parameter alpha\n",
    "alpha = 0.2\n",
    "for i in range(30):\n",
    "    y = PeMS[PeMS.columns[i]]\n",
    "    model = ExponentialSmoothing(y).fit(smoothing_level=alpha)\n",
    "    smooth = model.fittedvalues\n",
    "    PeMS[PeMS.columns[i]] = smooth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change PeMS only to the subgraph\n",
    "PeMS = PeMS[list(subgraph.nodes)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sort time series by mean traffic flow\n",
    "mean_flow = PeMS.mean().sort_values()\n",
    "#Index of sensor sort by mean traffic flow\n",
    "mean_flow_index = mean_flow.index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_order = list(mean_flow_index)\n",
    "PeMS =PeMS.reindex(columns=column_order)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# Define the sliding window size and stride\n",
    "window_size = 7\n",
    "stride = 1\n",
    "layers = 6\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a PyTorch dataset to generate input/target pairs for the LSTM model\n",
    "class TimeSeriesDataset(Dataset):\n",
    "    def __init__(self, data, window_size, stride):\n",
    "        self.data = data\n",
    "        self.window_size = window_size\n",
    "        self.stride = stride\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data) - self.window_size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        inputs = self.data[idx:idx+self.window_size]\n",
    "        target = self.data[idx+self.window_size]\n",
    "        return inputs, target\n",
    "\n",
    "# Define your LSTM model here with 6 LSTM layers and 1 fully connected layer\n",
    "class LSTMModel(torch.nn.Module):\n",
    "    def __init__(self, input_size, hidden_size,output_size, num_layers=6):\n",
    "        super().__init__()\n",
    "        self.lstm = torch.nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = torch.nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, _ = self.lstm(x)\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.holtwinters import ExponentialSmoothing\n",
    "# Apply exponential smoothing to the time serie\n",
    "# Define the smoothing parameter alpha\n",
    "alpha = 0.2\n",
    "for i in range(30):\n",
    "    y = PeMS[PeMS.columns[i]]\n",
    "    model = ExponentialSmoothing(y).fit(smoothing_level=alpha)\n",
    "    smooth = model.fittedvalues\n",
    "    PeMS[PeMS.columns[i]] = smooth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "def experiment_dataset(cluster_size,df):\n",
    "    cluster_dict={\"size\":cluster_size}\n",
    "    for i in range(len(PeMS.columns)+1-cluster_size):\n",
    "        model = LSTMModel(input_size=cluster_size, hidden_size=32, num_layers=layers, output_size=cluster_size)\n",
    "        train_data= df[df.columns[i:i+cluster_size]][:'2018-02-10 00:00:00']\n",
    "        val_data =  df[df.columns[i:i+cluster_size]]['2018-02-10 00:00:00':'2018-02-14 00:00:00']\n",
    "        test_data = df[df.columns[i:i+cluster_size]]['2018-02-14 00:00:00':]\n",
    "        \n",
    "        train_dataset = TimeSeriesDataset(train_data.values, window_size, stride)\n",
    "        val_dataset = TimeSeriesDataset(val_data.values, window_size, stride)\n",
    "        test_dataset = TimeSeriesDataset(test_data.values, window_size, stride)\n",
    "\n",
    "        train_loader = DataLoader(train_dataset, batch_size=32, shuffle=False)\n",
    "        train_loader = [(inputs.to(device), targets.to(device)) for inputs, targets in train_loader]\n",
    "\n",
    "        val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "        val_loader = [(inputs.to(device), targets.to(device)) for inputs, targets in val_loader]\n",
    "\n",
    "        test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "        test_loader = [(inputs.to(device), targets.to(device)) for inputs, targets in test_loader]\n",
    "        cluster_dict[i]={\"model\":model,\"train\":train_loader,\"val\":val_loader,\"test\":test_loader}\n",
    "    with open('./experiment/clusterS{}.pkl'.format(cluster_size), 'wb') as f:\n",
    "        pickle.dump(cluster_dict, f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "def experiment_dataset_subgraph(cluster_size,df):\n",
    "    cluster_dict={\"size\":cluster_size}\n",
    "    for i in PeMS.columns:\n",
    "        # Compute the shortest path from node 0 to all other nodes\n",
    "        distances = nx.single_source_dijkstra_path_length(G, i)\n",
    "\n",
    "        # Sort the nodes based on their distance from node 0\n",
    "        nearest_nodes = sorted(distances, key=distances.get)[:cluster_size]\n",
    "\n",
    "        # Construct the subgraph by including only the selected nodes and their edges\n",
    "        subgraph = G.subgraph(nearest_nodes)\n",
    "        nodes = list(subgraph.nodes)\n",
    "        model = LSTMModel(input_size=cluster_size, hidden_size=32, num_layers=layers, output_size=cluster_size)\n",
    "        \n",
    "        train_data= df[nodes][:'2018-02-10 00:00:00']\n",
    "        val_data =  df[nodes]['2018-02-10 00:00:00':'2018-02-14 00:00:00']\n",
    "        test_data = df[nodes]['2018-02-14 00:00:00':]\n",
    "        \n",
    "        train_dataset = TimeSeriesDataset(train_data.values, window_size, stride)\n",
    "        val_dataset = TimeSeriesDataset(val_data.values, window_size, stride)\n",
    "        test_dataset = TimeSeriesDataset(test_data.values, window_size, stride)\n",
    "\n",
    "        train_loader = DataLoader(train_dataset, batch_size=32, shuffle=False)\n",
    "        train_loader = [(inputs.to(device), targets.to(device)) for inputs, targets in train_loader]\n",
    "\n",
    "        val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "        val_loader = [(inputs.to(device), targets.to(device)) for inputs, targets in val_loader]\n",
    "\n",
    "        test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "        test_loader = [(inputs.to(device), targets.to(device)) for inputs, targets in test_loader]\n",
    "        cluster_dict[i]={\"model\":model,\"train\":train_loader,\"val\":val_loader,\"test\":test_loader,\"nodes\": nodes}\n",
    "    with open('./experiment/clusterGsize{}.pkl'.format(cluster_size), 'wb') as f:\n",
    "        pickle.dump(cluster_dict, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.cuda\n",
    "def train_model(model,train_loader, val_loader):\n",
    "  # Train your model and evaluate on the validation set\n",
    "    num_epochs = 100\n",
    "    # Define the loss function and optimizer\n",
    "    criterion = torch.nn.MSELoss()\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    best_val_loss = float('inf')\n",
    "    train_losses = []\n",
    "    valid_losses = []\n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss = 0.0\n",
    "        for inputs, targets in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = model(inputs.float())\n",
    "            loss = criterion(outputs, targets.float())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "            train_losses.append(loss.item())\n",
    "        val_loss = 0.0\n",
    "        \n",
    "        for inputs, targets in val_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = model(inputs.float())\n",
    "            loss = criterion(outputs, targets.float())\n",
    "            val_loss += loss.item()            \n",
    "        val_loss /= len(val_loader)\n",
    "        valid_losses.append(val_loss)\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), 'best_model.pth')\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, Training Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}')\n",
    "    best_model =  copy.deepcopy(model)\n",
    "    best_model.load_state_dict(torch.load('best_model.pth'))\n",
    "    return best_model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Exp using mean flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize the experiment datasets as pickle object\n",
    "experiment_dataset(5,PeMS)\n",
    "# iterate on cluster size i\n",
    "\n",
    "# load the experiment datasets from pickle object \n",
    "with open('./experiment/clusterS5.pkl', 'rb') as f:\n",
    "    my_dict = pickle.load(f)\n",
    "    # iterate on number of cluster 100-i+1\n",
    "    for j in range(30-5+1):\n",
    "        train = my_dict[j][\"train\"]\n",
    "        val = my_dict[j][\"val\"]\n",
    "        model = my_dict[j][\"model\"]\n",
    "        model = train_model(model,train, val)\n",
    "        my_dict[j][\"model\"]=copy.deepcopy(model)\n",
    "with open('./experiment/clusterS5.pkl'.format(i), 'wb') as f:\n",
    "    pickle.dump(my_dict, f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Exp using graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize the experiment datasets as pickle object\n",
    "experiment_dataset_subgraph(5,PeMS)\n",
    "# load the experiment datasets from pickle object \n",
    "with open('./experiment/clusterGsize5.pkl', 'rb') as f:\n",
    "    my_dict = pickle.load(f)\n",
    "    # iterate on number of cluster 100-i+1\n",
    "    for j in range(30-i+1):\n",
    "        train = my_dict[j][\"train\"]\n",
    "        val = my_dict[j][\"val\"]\n",
    "        model = my_dict[j][\"model\"]\n",
    "        model = train_model(model,train, val)\n",
    "        my_dict[j][\"model\"]=copy.deepcopy(model)\n",
    "with open('./experiment/clusterGsize5.pkl'.format(i), 'wb') as f:\n",
    "    pickle.dump(my_dict, f)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5838ee9d1e27b0c8776a2c47d844e546f7c02646b3dffe8b9f56b6c67f7dd71a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
