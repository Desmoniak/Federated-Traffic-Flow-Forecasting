{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from src.utils_data import load_PeMS04_flow_data, preprocess_PeMS_data, createLoaders\n",
    "from src.models import TGCN\n",
    "from src.utils_training import train_model, testmodel\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_unknown_nodes(df_PeMS, list_nodes_known=None):\n",
    "    if list_nodes_known is None:\n",
    "        return df_PeMS, None\n",
    "    mask = []\n",
    "    temp = df_PeMS.copy()\n",
    "    nb_values = temp.shape[0]\n",
    "    for node in temp:\n",
    "        if(node not in list_nodes_known):\n",
    "            temp[node] = [0 for _ in range(nb_values)]\n",
    "            mask.append(0)\n",
    "        else:\n",
    "            mask.append(1)\n",
    "    return temp, torch.tensor(mask).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def result_prediction(predictions, actuals):\n",
    "    from sklearn.metrics import mean_absolute_error, mean_squared_error, mean_absolute_percentage_error\n",
    "    import numpy as np\n",
    "    \n",
    "    indices_by_month = []\n",
    "    EPSILON = 1e-5\n",
    "    # Créer une liste vide pour stocker les données du tableau\n",
    "    data = []\n",
    "    y_pred = predictions[:]\n",
    "    y_true = actuals[:]\n",
    "\n",
    "    signe = \"-\" if np.mean(y_pred - y_true) < 0 else \"+\"\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    mape = mean_absolute_percentage_error(y_true, y_pred)*100\n",
    "    if (mape > 1 or mape < 0):\n",
    "        mape = \"ERROR\"\n",
    "    smape = np.mean(2 * np.abs(y_pred - y_true) / (np.abs(y_true) + np.abs(y_pred)))*100\n",
    "    maape =  np.mean(np.arctan(np.abs((y_true - y_pred) / (y_true + EPSILON))))*100\n",
    "    \n",
    "    return [signe, mae, rmse, mape, smape, maape]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# from sklearn.metrics import mean_absolute_error, mean_squared_error, mean_absolute_percentage_error\n",
    "\n",
    "\n",
    "# _window_size = 7\n",
    "# horizon = 1\n",
    "# _stride = 1\n",
    "\n",
    "# # TGCN\n",
    "# num_epochs_TGCN = 400\n",
    "\n",
    "# n_neighbors = 10\n",
    "\n",
    "# df_PeMS_old, df_distance  = load_PeMS04_flow_data()\n",
    "# df_PeMS, adjacency_matrix_PeMS, meanstd_dict = preprocess_PeMS_data(df_PeMS_old, df_distance, init_node=0, n_neighbors=n_neighbors, center_and_reduce=True)\n",
    "# print(df_PeMS.columns)\n",
    "# node_0, mask_node_0 = mask_unknown_nodes(df_PeMS=df_PeMS, list_nodes_known=[68, 62, 242, 243, 117, 28, 170])\n",
    "# node_1, mask_node_1 = mask_unknown_nodes(df_PeMS=df_PeMS, list_nodes_known=[0, 173, 92, 57])\n",
    "\n",
    "# # TGCN Model\n",
    "# model_TGCN = TGCN(adjacency_matrix_PeMS, hidden_dim=32, output_size=len(df_PeMS.columns))\n",
    "# train_loader_TGCN, val_loader_TGCN, test_loader_TGCN, _ = createLoaders(node_0, window_size=_window_size, stride=_stride, prediction_horizon=horizon)\n",
    "# model_path = \"./test1.pkl\"\n",
    "# best_model , train_losses, valid_losses = train_model(model_TGCN, train_loader_TGCN, val_loader_TGCN, model_path=model_path, mask=mask_node_0, num_epochs=num_epochs_TGCN, remove=False)\n",
    "\n",
    "# y_true, y_pred = testmodel(best_model=best_model, test_loader=test_loader_TGCN, meanstd_dict=meanstd_dict)\n",
    "# plt.figure(figsize=(49,9))\n",
    "# plt.plot(y_true[:,:,1])\n",
    "# plt.plot(y_pred[:,:,1])\n",
    "# plt.show()\n",
    "# print(y_true.shape)\n",
    "# rmse = 0\n",
    "# for i in range(n_neighbors+1):\n",
    "#     rmse = rmse + np.sqrt(mean_squared_error(y_true[:,:,i], y_pred[:,:,i]))\n",
    "# print(rmse/n_neighbors+1)\n",
    "\n",
    "# # TGCN Model\n",
    "# model_TGCN = TGCN(adjacency_matrix_PeMS, hidden_dim=32, output_size=len(df_PeMS.columns))\n",
    "# train_loader_TGCN, val_loader_TGCN, test_loader_TGCN, _ = createLoaders(node_0, window_size=_window_size, stride=_stride, prediction_horizon=horizon)\n",
    "# model_path = \"./test2.pkl\"\n",
    "# best_model , train_losses, valid_losses = train_model(model_TGCN, train_loader_TGCN, val_loader_TGCN, model_path=model_path, mask=None, num_epochs=num_epochs_TGCN, remove=False)\n",
    "\n",
    "# y_true, y_pred = testmodel(best_model=best_model, test_loader=test_loader_TGCN, meanstd_dict=meanstd_dict)\n",
    "# plt.figure(figsize=(49,9))\n",
    "# plt.plot(y_true[:,:,1])\n",
    "# plt.plot(y_pred[:,:,1])\n",
    "# plt.show()\n",
    "# rmse = 0\n",
    "# for i in range(n_neighbors+1):\n",
    "#     rmse = rmse + np.sqrt(mean_squared_error(y_true[:,:,i], y_pred[:,:,i]))\n",
    "# print(rmse/n_neighbors+1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (All variables) vs (Dummy Variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gcogoni\\Documents\\Stage\\Federated-Traffic-Flow-Forecasting\\src\\utils_graph.py:98: FutureWarning: adjacency_matrix will return a scipy.sparse array instead of a matrix in Networkx 3.0.\n",
      "  return nx.adjacency_matrix(graph, nodelist=nodes_order, weight=None).toarray()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Int64Index([117, 0, 92], dtype='int64')\n",
      "Epoch 1/350, Training Loss: 0.1123, Validation Loss: 0.0330\n",
      "Epoch 2/350, Training Loss: 0.0507, Validation Loss: 0.0190\n",
      "Epoch 3/350, Training Loss: 0.0372, Validation Loss: 0.0150\n",
      "Epoch 4/350, Training Loss: 0.0329, Validation Loss: 0.0138\n",
      "Epoch 5/350, Training Loss: 0.0308, Validation Loss: 0.0131\n",
      "Epoch 6/350, Training Loss: 0.0293, Validation Loss: 0.0128\n",
      "Epoch 7/350, Training Loss: 0.0280, Validation Loss: 0.0129\n",
      "Epoch 8/350, Training Loss: 0.0266, Validation Loss: 0.0132\n",
      "Epoch 9/350, Training Loss: 0.0254, Validation Loss: 0.0137\n",
      "Epoch 10/350, Training Loss: 0.0243, Validation Loss: 0.0139\n",
      "Epoch 11/350, Training Loss: 0.0232, Validation Loss: 0.0137\n",
      "Epoch 12/350, Training Loss: 0.0222, Validation Loss: 0.0132\n",
      "Epoch 13/350, Training Loss: 0.0211, Validation Loss: 0.0128\n",
      "Epoch 14/350, Training Loss: 0.0200, Validation Loss: 0.0124\n",
      "Epoch 15/350, Training Loss: 0.0189, Validation Loss: 0.0123\n",
      "Epoch 16/350, Training Loss: 0.0178, Validation Loss: 0.0126\n",
      "Epoch 17/350, Training Loss: 0.0168, Validation Loss: 0.0135\n",
      "Epoch 18/350, Training Loss: 0.0159, Validation Loss: 0.0142\n",
      "Epoch 19/350, Training Loss: 0.0150, Validation Loss: 0.0144\n",
      "Epoch 20/350, Training Loss: 0.0141, Validation Loss: 0.0142\n",
      "Epoch 21/350, Training Loss: 0.0132, Validation Loss: 0.0138\n",
      "Epoch 22/350, Training Loss: 0.0124, Validation Loss: 0.0134\n",
      "Epoch 23/350, Training Loss: 0.0116, Validation Loss: 0.0130\n",
      "Epoch 24/350, Training Loss: 0.0109, Validation Loss: 0.0126\n",
      "Epoch 25/350, Training Loss: 0.0103, Validation Loss: 0.0122\n",
      "Epoch 26/350, Training Loss: 0.0097, Validation Loss: 0.0118\n",
      "Epoch 27/350, Training Loss: 0.0092, Validation Loss: 0.0113\n",
      "Epoch 28/350, Training Loss: 0.0087, Validation Loss: 0.0108\n",
      "Epoch 29/350, Training Loss: 0.0084, Validation Loss: 0.0102\n",
      "Epoch 30/350, Training Loss: 0.0081, Validation Loss: 0.0098\n",
      "Epoch 31/350, Training Loss: 0.0079, Validation Loss: 0.0097\n",
      "Epoch 32/350, Training Loss: 0.0077, Validation Loss: 0.0098\n",
      "Epoch 33/350, Training Loss: 0.0076, Validation Loss: 0.0099\n",
      "Epoch 34/350, Training Loss: 0.0075, Validation Loss: 0.0099\n",
      "Epoch 35/350, Training Loss: 0.0074, Validation Loss: 0.0100\n",
      "Epoch 36/350, Training Loss: 0.0073, Validation Loss: 0.0101\n",
      "Epoch 37/350, Training Loss: 0.0072, Validation Loss: 0.0102\n",
      "Epoch 38/350, Training Loss: 0.0072, Validation Loss: 0.0103\n",
      "Epoch 39/350, Training Loss: 0.0071, Validation Loss: 0.0103\n",
      "Epoch 40/350, Training Loss: 0.0071, Validation Loss: 0.0102\n",
      "Epoch 41/350, Training Loss: 0.0070, Validation Loss: 0.0102\n",
      "Epoch 42/350, Training Loss: 0.0070, Validation Loss: 0.0101\n",
      "Epoch 43/350, Training Loss: 0.0069, Validation Loss: 0.0100\n",
      "Epoch 44/350, Training Loss: 0.0069, Validation Loss: 0.0099\n",
      "Epoch 45/350, Training Loss: 0.0068, Validation Loss: 0.0099\n",
      "Epoch 46/350, Training Loss: 0.0068, Validation Loss: 0.0098\n",
      "Epoch 47/350, Training Loss: 0.0068, Validation Loss: 0.0097\n",
      "Epoch 48/350, Training Loss: 0.0067, Validation Loss: 0.0097\n",
      "Epoch 49/350, Training Loss: 0.0067, Validation Loss: 0.0096\n",
      "Epoch 50/350, Training Loss: 0.0067, Validation Loss: 0.0096\n",
      "Epoch 51/350, Training Loss: 0.0066, Validation Loss: 0.0095\n",
      "Epoch 52/350, Training Loss: 0.0066, Validation Loss: 0.0095\n",
      "Epoch 53/350, Training Loss: 0.0066, Validation Loss: 0.0095\n",
      "Epoch 54/350, Training Loss: 0.0066, Validation Loss: 0.0095\n",
      "Epoch 55/350, Training Loss: 0.0065, Validation Loss: 0.0094\n",
      "Epoch 56/350, Training Loss: 0.0065, Validation Loss: 0.0094\n",
      "Epoch 57/350, Training Loss: 0.0065, Validation Loss: 0.0094\n",
      "Epoch 58/350, Training Loss: 0.0065, Validation Loss: 0.0094\n",
      "Epoch 59/350, Training Loss: 0.0064, Validation Loss: 0.0094\n",
      "Epoch 60/350, Training Loss: 0.0064, Validation Loss: 0.0093\n",
      "Epoch 61/350, Training Loss: 0.0064, Validation Loss: 0.0093\n",
      "Epoch 62/350, Training Loss: 0.0064, Validation Loss: 0.0093\n",
      "Epoch 63/350, Training Loss: 0.0064, Validation Loss: 0.0093\n",
      "Epoch 64/350, Training Loss: 0.0063, Validation Loss: 0.0092\n",
      "Epoch 65/350, Training Loss: 0.0063, Validation Loss: 0.0092\n",
      "Epoch 66/350, Training Loss: 0.0063, Validation Loss: 0.0092\n",
      "Epoch 67/350, Training Loss: 0.0063, Validation Loss: 0.0092\n",
      "Epoch 68/350, Training Loss: 0.0063, Validation Loss: 0.0092\n",
      "Epoch 69/350, Training Loss: 0.0062, Validation Loss: 0.0091\n",
      "Epoch 70/350, Training Loss: 0.0062, Validation Loss: 0.0091\n",
      "Epoch 71/350, Training Loss: 0.0062, Validation Loss: 0.0091\n",
      "Epoch 72/350, Training Loss: 0.0062, Validation Loss: 0.0091\n",
      "Epoch 73/350, Training Loss: 0.0062, Validation Loss: 0.0091\n",
      "Epoch 74/350, Training Loss: 0.0061, Validation Loss: 0.0091\n",
      "Epoch 75/350, Training Loss: 0.0061, Validation Loss: 0.0090\n",
      "Epoch 76/350, Training Loss: 0.0061, Validation Loss: 0.0090\n",
      "Epoch 77/350, Training Loss: 0.0061, Validation Loss: 0.0090\n",
      "Epoch 78/350, Training Loss: 0.0061, Validation Loss: 0.0090\n",
      "Epoch 79/350, Training Loss: 0.0060, Validation Loss: 0.0090\n",
      "Epoch 80/350, Training Loss: 0.0060, Validation Loss: 0.0090\n",
      "Epoch 81/350, Training Loss: 0.0060, Validation Loss: 0.0090\n",
      "Epoch 82/350, Training Loss: 0.0060, Validation Loss: 0.0090\n",
      "Epoch 83/350, Training Loss: 0.0060, Validation Loss: 0.0090\n",
      "Epoch 84/350, Training Loss: 0.0060, Validation Loss: 0.0089\n",
      "Epoch 85/350, Training Loss: 0.0059, Validation Loss: 0.0089\n",
      "Epoch 86/350, Training Loss: 0.0059, Validation Loss: 0.0089\n",
      "Epoch 87/350, Training Loss: 0.0059, Validation Loss: 0.0089\n",
      "Epoch 88/350, Training Loss: 0.0059, Validation Loss: 0.0089\n",
      "Epoch 89/350, Training Loss: 0.0059, Validation Loss: 0.0089\n",
      "Epoch 90/350, Training Loss: 0.0058, Validation Loss: 0.0089\n",
      "Epoch 91/350, Training Loss: 0.0058, Validation Loss: 0.0089\n",
      "Epoch 92/350, Training Loss: 0.0058, Validation Loss: 0.0089\n",
      "Epoch 93/350, Training Loss: 0.0058, Validation Loss: 0.0089\n",
      "Epoch 94/350, Training Loss: 0.0058, Validation Loss: 0.0089\n",
      "Epoch 95/350, Training Loss: 0.0058, Validation Loss: 0.0089\n",
      "Epoch 96/350, Training Loss: 0.0057, Validation Loss: 0.0089\n",
      "Epoch 97/350, Training Loss: 0.0057, Validation Loss: 0.0089\n",
      "Epoch 98/350, Training Loss: 0.0057, Validation Loss: 0.0089\n",
      "Epoch 99/350, Training Loss: 0.0057, Validation Loss: 0.0089\n",
      "Epoch 100/350, Training Loss: 0.0057, Validation Loss: 0.0089\n",
      "Epoch 101/350, Training Loss: 0.0057, Validation Loss: 0.0089\n",
      "Epoch 102/350, Training Loss: 0.0056, Validation Loss: 0.0089\n",
      "Epoch 103/350, Training Loss: 0.0056, Validation Loss: 0.0089\n",
      "Epoch 104/350, Training Loss: 0.0056, Validation Loss: 0.0089\n",
      "Epoch 105/350, Training Loss: 0.0056, Validation Loss: 0.0089\n",
      "Epoch 106/350, Training Loss: 0.0056, Validation Loss: 0.0089\n",
      "Epoch 107/350, Training Loss: 0.0056, Validation Loss: 0.0089\n",
      "Epoch 108/350, Training Loss: 0.0055, Validation Loss: 0.0089\n",
      "Epoch 109/350, Training Loss: 0.0055, Validation Loss: 0.0089\n",
      "Epoch 110/350, Training Loss: 0.0055, Validation Loss: 0.0089\n",
      "Epoch 111/350, Training Loss: 0.0055, Validation Loss: 0.0088\n",
      "Epoch 112/350, Training Loss: 0.0055, Validation Loss: 0.0088\n",
      "Epoch 113/350, Training Loss: 0.0055, Validation Loss: 0.0088\n",
      "Epoch 114/350, Training Loss: 0.0054, Validation Loss: 0.0088\n",
      "Epoch 115/350, Training Loss: 0.0054, Validation Loss: 0.0088\n",
      "Epoch 116/350, Training Loss: 0.0054, Validation Loss: 0.0087\n",
      "Epoch 117/350, Training Loss: 0.0054, Validation Loss: 0.0087\n",
      "Epoch 118/350, Training Loss: 0.0054, Validation Loss: 0.0087\n",
      "Epoch 119/350, Training Loss: 0.0054, Validation Loss: 0.0087\n",
      "Epoch 120/350, Training Loss: 0.0053, Validation Loss: 0.0086\n",
      "Epoch 121/350, Training Loss: 0.0053, Validation Loss: 0.0086\n",
      "Epoch 122/350, Training Loss: 0.0053, Validation Loss: 0.0086\n",
      "Epoch 123/350, Training Loss: 0.0053, Validation Loss: 0.0085\n",
      "Epoch 124/350, Training Loss: 0.0053, Validation Loss: 0.0085\n",
      "Epoch 125/350, Training Loss: 0.0053, Validation Loss: 0.0085\n",
      "Epoch 126/350, Training Loss: 0.0052, Validation Loss: 0.0084\n",
      "Epoch 127/350, Training Loss: 0.0052, Validation Loss: 0.0084\n",
      "Epoch 128/350, Training Loss: 0.0052, Validation Loss: 0.0083\n",
      "Epoch 129/350, Training Loss: 0.0052, Validation Loss: 0.0083\n",
      "Epoch 130/350, Training Loss: 0.0052, Validation Loss: 0.0083\n",
      "Epoch 131/350, Training Loss: 0.0052, Validation Loss: 0.0082\n",
      "Epoch 132/350, Training Loss: 0.0052, Validation Loss: 0.0082\n",
      "Epoch 133/350, Training Loss: 0.0051, Validation Loss: 0.0081\n",
      "Epoch 134/350, Training Loss: 0.0051, Validation Loss: 0.0081\n",
      "Epoch 135/350, Training Loss: 0.0051, Validation Loss: 0.0080\n",
      "Epoch 136/350, Training Loss: 0.0051, Validation Loss: 0.0080\n",
      "Epoch 137/350, Training Loss: 0.0051, Validation Loss: 0.0080\n",
      "Epoch 138/350, Training Loss: 0.0051, Validation Loss: 0.0079\n",
      "Epoch 139/350, Training Loss: 0.0051, Validation Loss: 0.0079\n",
      "Epoch 140/350, Training Loss: 0.0050, Validation Loss: 0.0078\n",
      "Epoch 141/350, Training Loss: 0.0050, Validation Loss: 0.0078\n",
      "Epoch 142/350, Training Loss: 0.0050, Validation Loss: 0.0077\n",
      "Epoch 143/350, Training Loss: 0.0050, Validation Loss: 0.0077\n",
      "Epoch 144/350, Training Loss: 0.0050, Validation Loss: 0.0077\n",
      "Epoch 145/350, Training Loss: 0.0050, Validation Loss: 0.0076\n",
      "Epoch 146/350, Training Loss: 0.0050, Validation Loss: 0.0076\n",
      "Epoch 147/350, Training Loss: 0.0050, Validation Loss: 0.0076\n",
      "Epoch 148/350, Training Loss: 0.0049, Validation Loss: 0.0075\n",
      "Epoch 149/350, Training Loss: 0.0049, Validation Loss: 0.0075\n",
      "Epoch 150/350, Training Loss: 0.0049, Validation Loss: 0.0074\n",
      "Epoch 151/350, Training Loss: 0.0049, Validation Loss: 0.0074\n",
      "Epoch 152/350, Training Loss: 0.0049, Validation Loss: 0.0073\n",
      "Epoch 153/350, Training Loss: 0.0049, Validation Loss: 0.0072\n",
      "Epoch 154/350, Training Loss: 0.0049, Validation Loss: 0.0070\n",
      "Epoch 155/350, Training Loss: 0.0048, Validation Loss: 0.0067\n",
      "Epoch 156/350, Training Loss: 0.0048, Validation Loss: 0.0057\n",
      "Epoch 157/350, Training Loss: 0.0048, Validation Loss: 0.0060\n",
      "Epoch 158/350, Training Loss: 0.0051, Validation Loss: 0.0065\n",
      "Epoch 159/350, Training Loss: 0.0047, Validation Loss: 0.0053\n",
      "Epoch 160/350, Training Loss: 0.0049, Validation Loss: 0.0069\n",
      "Epoch 161/350, Training Loss: 0.0047, Validation Loss: 0.0053\n",
      "Epoch 162/350, Training Loss: 0.0049, Validation Loss: 0.0068\n",
      "Epoch 163/350, Training Loss: 0.0047, Validation Loss: 0.0054\n",
      "Epoch 164/350, Training Loss: 0.0048, Validation Loss: 0.0061\n",
      "Epoch 165/350, Training Loss: 0.0047, Validation Loss: 0.0053\n",
      "Epoch 166/350, Training Loss: 0.0049, Validation Loss: 0.0069\n",
      "Epoch 167/350, Training Loss: 0.0047, Validation Loss: 0.0055\n",
      "Epoch 168/350, Training Loss: 0.0049, Validation Loss: 0.0086\n",
      "Epoch 169/350, Training Loss: 0.0047, Validation Loss: 0.0053\n",
      "Epoch 170/350, Training Loss: 0.0048, Validation Loss: 0.0062\n",
      "Epoch 171/350, Training Loss: 0.0046, Validation Loss: 0.0054\n",
      "Epoch 172/350, Training Loss: 0.0048, Validation Loss: 0.0061\n",
      "Epoch 173/350, Training Loss: 0.0046, Validation Loss: 0.0052\n",
      "Epoch 174/350, Training Loss: 0.0047, Validation Loss: 0.0055\n",
      "Epoch 175/350, Training Loss: 0.0049, Validation Loss: 0.0078\n",
      "Epoch 176/350, Training Loss: 0.0046, Validation Loss: 0.0052\n",
      "Epoch 177/350, Training Loss: 0.0048, Validation Loss: 0.0052\n",
      "Epoch 178/350, Training Loss: 0.0045, Validation Loss: 0.0056\n",
      "Epoch 179/350, Training Loss: 0.0048, Validation Loss: 0.0072\n",
      "Epoch 180/350, Training Loss: 0.0046, Validation Loss: 0.0056\n",
      "Epoch 181/350, Training Loss: 0.0048, Validation Loss: 0.0067\n",
      "Epoch 182/350, Training Loss: 0.0045, Validation Loss: 0.0052\n",
      "Epoch 183/350, Training Loss: 0.0047, Validation Loss: 0.0065\n",
      "Epoch 184/350, Training Loss: 0.0045, Validation Loss: 0.0053\n",
      "Epoch 185/350, Training Loss: 0.0047, Validation Loss: 0.0066\n",
      "Epoch 186/350, Training Loss: 0.0045, Validation Loss: 0.0061\n",
      "Epoch 187/350, Training Loss: 0.0047, Validation Loss: 0.0070\n",
      "Epoch 188/350, Training Loss: 0.0045, Validation Loss: 0.0051\n",
      "Epoch 189/350, Training Loss: 0.0046, Validation Loss: 0.0067\n",
      "Epoch 190/350, Training Loss: 0.0045, Validation Loss: 0.0055\n",
      "Epoch 191/350, Training Loss: 0.0047, Validation Loss: 0.0074\n",
      "Epoch 192/350, Training Loss: 0.0045, Validation Loss: 0.0051\n",
      "Epoch 193/350, Training Loss: 0.0045, Validation Loss: 0.0051\n",
      "Epoch 194/350, Training Loss: 0.0045, Validation Loss: 0.0058\n",
      "Epoch 195/350, Training Loss: 0.0047, Validation Loss: 0.0054\n",
      "Epoch 196/350, Training Loss: 0.0044, Validation Loss: 0.0057\n",
      "Epoch 197/350, Training Loss: 0.0047, Validation Loss: 0.0051\n",
      "Epoch 198/350, Training Loss: 0.0044, Validation Loss: 0.0051\n",
      "Epoch 199/350, Training Loss: 0.0061, Validation Loss: 0.0062\n",
      "Epoch 200/350, Training Loss: 0.0045, Validation Loss: 0.0050\n",
      "Epoch 201/350, Training Loss: 0.0043, Validation Loss: 0.0050\n",
      "Epoch 202/350, Training Loss: 0.0043, Validation Loss: 0.0050\n",
      "Epoch 203/350, Training Loss: 0.0044, Validation Loss: 0.0051\n",
      "Epoch 204/350, Training Loss: 0.0045, Validation Loss: 0.0061\n",
      "Epoch 205/350, Training Loss: 0.0044, Validation Loss: 0.0051\n",
      "Epoch 206/350, Training Loss: 0.0049, Validation Loss: 0.0069\n",
      "Epoch 207/350, Training Loss: 0.0044, Validation Loss: 0.0050\n",
      "Epoch 208/350, Training Loss: 0.0043, Validation Loss: 0.0052\n",
      "Epoch 209/350, Training Loss: 0.0046, Validation Loss: 0.0081\n",
      "Epoch 210/350, Training Loss: 0.0044, Validation Loss: 0.0051\n",
      "Epoch 211/350, Training Loss: 0.0044, Validation Loss: 0.0066\n",
      "Epoch 212/350, Training Loss: 0.0045, Validation Loss: 0.0073\n",
      "Epoch 213/350, Training Loss: 0.0044, Validation Loss: 0.0050\n",
      "Epoch 214/350, Training Loss: 0.0046, Validation Loss: 0.0089\n",
      "Epoch 215/350, Training Loss: 0.0044, Validation Loss: 0.0050\n",
      "Epoch 216/350, Training Loss: 0.0043, Validation Loss: 0.0054\n",
      "Epoch 217/350, Training Loss: 0.0045, Validation Loss: 0.0052\n",
      "Epoch 218/350, Training Loss: 0.0046, Validation Loss: 0.0079\n",
      "Epoch 219/350, Training Loss: 0.0044, Validation Loss: 0.0050\n",
      "Epoch 220/350, Training Loss: 0.0043, Validation Loss: 0.0050\n",
      "Epoch 221/350, Training Loss: 0.0048, Validation Loss: 0.0049\n",
      "Epoch 222/350, Training Loss: 0.0043, Validation Loss: 0.0051\n",
      "Epoch 223/350, Training Loss: 0.0043, Validation Loss: 0.0055\n",
      "Epoch 224/350, Training Loss: 0.0044, Validation Loss: 0.0062\n",
      "Epoch 225/350, Training Loss: 0.0044, Validation Loss: 0.0058\n",
      "Epoch 226/350, Training Loss: 0.0044, Validation Loss: 0.0050\n",
      "Epoch 227/350, Training Loss: 0.0044, Validation Loss: 0.0057\n",
      "Epoch 228/350, Training Loss: 0.0043, Validation Loss: 0.0054\n",
      "Epoch 229/350, Training Loss: 0.0045, Validation Loss: 0.0049\n",
      "Epoch 230/350, Training Loss: 0.0043, Validation Loss: 0.0057\n",
      "Epoch 231/350, Training Loss: 0.0044, Validation Loss: 0.0051\n",
      "Epoch 232/350, Training Loss: 0.0043, Validation Loss: 0.0062\n",
      "Epoch 233/350, Training Loss: 0.0043, Validation Loss: 0.0052\n",
      "Epoch 234/350, Training Loss: 0.0049, Validation Loss: 0.0066\n",
      "Epoch 235/350, Training Loss: 0.0043, Validation Loss: 0.0049\n",
      "Epoch 236/350, Training Loss: 0.0042, Validation Loss: 0.0051\n",
      "Epoch 237/350, Training Loss: 0.0043, Validation Loss: 0.0056\n",
      "Epoch 238/350, Training Loss: 0.0043, Validation Loss: 0.0055\n",
      "Epoch 239/350, Training Loss: 0.0044, Validation Loss: 0.0072\n",
      "Epoch 240/350, Training Loss: 0.0043, Validation Loss: 0.0050\n",
      "Epoch 241/350, Training Loss: 0.0042, Validation Loss: 0.0050\n",
      "Epoch 242/350, Training Loss: 0.0045, Validation Loss: 0.0049\n",
      "Epoch 243/350, Training Loss: 0.0042, Validation Loss: 0.0050\n",
      "Epoch 244/350, Training Loss: 0.0044, Validation Loss: 0.0063\n",
      "Epoch 245/350, Training Loss: 0.0042, Validation Loss: 0.0051\n",
      "Epoch 246/350, Training Loss: 0.0045, Validation Loss: 0.0072\n",
      "Epoch 247/350, Training Loss: 0.0042, Validation Loss: 0.0050\n",
      "Epoch 248/350, Training Loss: 0.0042, Validation Loss: 0.0049\n",
      "Epoch 249/350, Training Loss: 0.0044, Validation Loss: 0.0067\n",
      "Epoch 250/350, Training Loss: 0.0042, Validation Loss: 0.0049\n",
      "Epoch 251/350, Training Loss: 0.0042, Validation Loss: 0.0050\n",
      "Epoch 252/350, Training Loss: 0.0043, Validation Loss: 0.0066\n",
      "Epoch 253/350, Training Loss: 0.0043, Validation Loss: 0.0064\n",
      "Epoch 254/350, Training Loss: 0.0042, Validation Loss: 0.0057\n",
      "Epoch 255/350, Training Loss: 0.0043, Validation Loss: 0.0072\n",
      "Epoch 256/350, Training Loss: 0.0042, Validation Loss: 0.0049\n",
      "Epoch 257/350, Training Loss: 0.0042, Validation Loss: 0.0049\n",
      "Epoch 258/350, Training Loss: 0.0042, Validation Loss: 0.0049\n",
      "Epoch 259/350, Training Loss: 0.0042, Validation Loss: 0.0048\n",
      "Epoch 260/350, Training Loss: 0.0046, Validation Loss: 0.0064\n",
      "Epoch 261/350, Training Loss: 0.0042, Validation Loss: 0.0049\n",
      "Epoch 262/350, Training Loss: 0.0041, Validation Loss: 0.0048\n",
      "Epoch 263/350, Training Loss: 0.0042, Validation Loss: 0.0063\n",
      "Epoch 264/350, Training Loss: 0.0042, Validation Loss: 0.0059\n",
      "Epoch 265/350, Training Loss: 0.0043, Validation Loss: 0.0072\n",
      "Epoch 266/350, Training Loss: 0.0042, Validation Loss: 0.0051\n",
      "Epoch 267/350, Training Loss: 0.0045, Validation Loss: 0.0083\n",
      "Epoch 268/350, Training Loss: 0.0042, Validation Loss: 0.0048\n",
      "Epoch 269/350, Training Loss: 0.0041, Validation Loss: 0.0048\n",
      "Epoch 270/350, Training Loss: 0.0042, Validation Loss: 0.0053\n",
      "Epoch 271/350, Training Loss: 0.0043, Validation Loss: 0.0063\n",
      "Epoch 272/350, Training Loss: 0.0041, Validation Loss: 0.0050\n",
      "Epoch 273/350, Training Loss: 0.0041, Validation Loss: 0.0049\n",
      "Epoch 274/350, Training Loss: 0.0044, Validation Loss: 0.0048\n",
      "Epoch 275/350, Training Loss: 0.0041, Validation Loss: 0.0053\n",
      "Epoch 276/350, Training Loss: 0.0043, Validation Loss: 0.0063\n",
      "Epoch 277/350, Training Loss: 0.0041, Validation Loss: 0.0049\n",
      "Epoch 278/350, Training Loss: 0.0041, Validation Loss: 0.0051\n",
      "Epoch 279/350, Training Loss: 0.0047, Validation Loss: 0.0048\n",
      "Epoch 280/350, Training Loss: 0.0041, Validation Loss: 0.0049\n",
      "Epoch 281/350, Training Loss: 0.0041, Validation Loss: 0.0048\n",
      "Epoch 282/350, Training Loss: 0.0041, Validation Loss: 0.0050\n",
      "Epoch 283/350, Training Loss: 0.0043, Validation Loss: 0.0066\n",
      "Epoch 284/350, Training Loss: 0.0041, Validation Loss: 0.0048\n",
      "Epoch 285/350, Training Loss: 0.0041, Validation Loss: 0.0050\n",
      "Epoch 286/350, Training Loss: 0.0041, Validation Loss: 0.0072\n",
      "Epoch 287/350, Training Loss: 0.0043, Validation Loss: 0.0051\n",
      "Epoch 288/350, Training Loss: 0.0041, Validation Loss: 0.0050\n",
      "Epoch 289/350, Training Loss: 0.0042, Validation Loss: 0.0068\n",
      "Epoch 290/350, Training Loss: 0.0041, Validation Loss: 0.0050\n",
      "Epoch 291/350, Training Loss: 0.0041, Validation Loss: 0.0049\n",
      "Epoch 292/350, Training Loss: 0.0044, Validation Loss: 0.0077\n",
      "Epoch 293/350, Training Loss: 0.0041, Validation Loss: 0.0050\n",
      "Epoch 294/350, Training Loss: 0.0041, Validation Loss: 0.0048\n",
      "Epoch 295/350, Training Loss: 0.0041, Validation Loss: 0.0051\n",
      "Epoch 296/350, Training Loss: 0.0042, Validation Loss: 0.0059\n",
      "Epoch 297/350, Training Loss: 0.0041, Validation Loss: 0.0054\n",
      "Epoch 298/350, Training Loss: 0.0041, Validation Loss: 0.0067\n",
      "Epoch 299/350, Training Loss: 0.0041, Validation Loss: 0.0051\n",
      "Epoch 300/350, Training Loss: 0.0040, Validation Loss: 0.0053\n",
      "Epoch 301/350, Training Loss: 0.0044, Validation Loss: 0.0052\n",
      "Epoch 302/350, Training Loss: 0.0040, Validation Loss: 0.0047\n",
      "Epoch 303/350, Training Loss: 0.0041, Validation Loss: 0.0048\n",
      "Epoch 304/350, Training Loss: 0.0041, Validation Loss: 0.0052\n",
      "Epoch 305/350, Training Loss: 0.0042, Validation Loss: 0.0061\n",
      "Epoch 306/350, Training Loss: 0.0041, Validation Loss: 0.0048\n",
      "Epoch 307/350, Training Loss: 0.0040, Validation Loss: 0.0051\n",
      "Epoch 308/350, Training Loss: 0.0045, Validation Loss: 0.0073\n",
      "Epoch 309/350, Training Loss: 0.0040, Validation Loss: 0.0047\n",
      "Epoch 310/350, Training Loss: 0.0040, Validation Loss: 0.0047\n",
      "Epoch 311/350, Training Loss: 0.0041, Validation Loss: 0.0047\n",
      "Epoch 312/350, Training Loss: 0.0041, Validation Loss: 0.0049\n",
      "Epoch 313/350, Training Loss: 0.0041, Validation Loss: 0.0058\n",
      "Epoch 314/350, Training Loss: 0.0041, Validation Loss: 0.0053\n",
      "Epoch 315/350, Training Loss: 0.0040, Validation Loss: 0.0054\n",
      "Epoch 316/350, Training Loss: 0.0042, Validation Loss: 0.0062\n",
      "Epoch 317/350, Training Loss: 0.0040, Validation Loss: 0.0047\n",
      "Epoch 318/350, Training Loss: 0.0041, Validation Loss: 0.0047\n",
      "Epoch 319/350, Training Loss: 0.0041, Validation Loss: 0.0049\n",
      "Epoch 320/350, Training Loss: 0.0041, Validation Loss: 0.0059\n",
      "Epoch 321/350, Training Loss: 0.0041, Validation Loss: 0.0049\n",
      "Epoch 322/350, Training Loss: 0.0040, Validation Loss: 0.0047\n",
      "Epoch 323/350, Training Loss: 0.0041, Validation Loss: 0.0047\n",
      "Epoch 324/350, Training Loss: 0.0041, Validation Loss: 0.0048\n",
      "Epoch 325/350, Training Loss: 0.0040, Validation Loss: 0.0051\n",
      "Epoch 326/350, Training Loss: 0.0041, Validation Loss: 0.0057\n",
      "Epoch 327/350, Training Loss: 0.0040, Validation Loss: 0.0052\n",
      "Epoch 328/350, Training Loss: 0.0040, Validation Loss: 0.0061\n",
      "Epoch 329/350, Training Loss: 0.0043, Validation Loss: 0.0059\n",
      "Epoch 330/350, Training Loss: 0.0040, Validation Loss: 0.0048\n",
      "Epoch 331/350, Training Loss: 0.0040, Validation Loss: 0.0048\n",
      "Epoch 332/350, Training Loss: 0.0040, Validation Loss: 0.0051\n",
      "Epoch 333/350, Training Loss: 0.0040, Validation Loss: 0.0059\n",
      "Epoch 334/350, Training Loss: 0.0040, Validation Loss: 0.0049\n",
      "Epoch 335/350, Training Loss: 0.0040, Validation Loss: 0.0056\n",
      "Epoch 336/350, Training Loss: 0.0041, Validation Loss: 0.0049\n",
      "Epoch 337/350, Training Loss: 0.0040, Validation Loss: 0.0048\n",
      "Epoch 338/350, Training Loss: 0.0040, Validation Loss: 0.0052\n",
      "Epoch 339/350, Training Loss: 0.0040, Validation Loss: 0.0050\n",
      "Epoch 340/350, Training Loss: 0.0040, Validation Loss: 0.0047\n",
      "Epoch 341/350, Training Loss: 0.0041, Validation Loss: 0.0048\n",
      "Epoch 342/350, Training Loss: 0.0040, Validation Loss: 0.0049\n",
      "Epoch 343/350, Training Loss: 0.0040, Validation Loss: 0.0057\n",
      "Epoch 344/350, Training Loss: 0.0041, Validation Loss: 0.0053\n",
      "Epoch 345/350, Training Loss: 0.0040, Validation Loss: 0.0048\n",
      "Epoch 346/350, Training Loss: 0.0040, Validation Loss: 0.0048\n",
      "Epoch 347/350, Training Loss: 0.0040, Validation Loss: 0.0055\n",
      "Epoch 348/350, Training Loss: 0.0041, Validation Loss: 0.0051\n",
      "Epoch 349/350, Training Loss: 0.0040, Validation Loss: 0.0049\n",
      "Epoch 350/350, Training Loss: 0.0039, Validation Loss: 0.0048\n",
      "1.3241404739033307\n",
      "Epoch 1/350, Training Loss: 0.1962, Validation Loss: 0.0631\n",
      "Epoch 2/350, Training Loss: 0.0313, Validation Loss: 0.0209\n",
      "Epoch 3/350, Training Loss: 0.0233, Validation Loss: 0.0183\n",
      "Epoch 4/350, Training Loss: 0.0211, Validation Loss: 0.0165\n",
      "Epoch 5/350, Training Loss: 0.0192, Validation Loss: 0.0150\n",
      "Epoch 6/350, Training Loss: 0.0176, Validation Loss: 0.0138\n",
      "Epoch 7/350, Training Loss: 0.0162, Validation Loss: 0.0129\n",
      "Epoch 8/350, Training Loss: 0.0149, Validation Loss: 0.0124\n",
      "Epoch 9/350, Training Loss: 0.0139, Validation Loss: 0.0118\n",
      "Epoch 10/350, Training Loss: 0.0129, Validation Loss: 0.0109\n",
      "Epoch 11/350, Training Loss: 0.0120, Validation Loss: 0.0104\n",
      "Epoch 12/350, Training Loss: 0.0112, Validation Loss: 0.0100\n",
      "Epoch 13/350, Training Loss: 0.0104, Validation Loss: 0.0096\n",
      "Epoch 14/350, Training Loss: 0.0098, Validation Loss: 0.0091\n",
      "Epoch 15/350, Training Loss: 0.0092, Validation Loss: 0.0087\n",
      "Epoch 16/350, Training Loss: 0.0086, Validation Loss: 0.0082\n",
      "Epoch 17/350, Training Loss: 0.0083, Validation Loss: 0.0078\n",
      "Epoch 18/350, Training Loss: 0.0077, Validation Loss: 0.0077\n",
      "Epoch 19/350, Training Loss: 0.0075, Validation Loss: 0.0073\n",
      "Epoch 20/350, Training Loss: 0.0071, Validation Loss: 0.0071\n",
      "Epoch 21/350, Training Loss: 0.0068, Validation Loss: 0.0070\n",
      "Epoch 22/350, Training Loss: 0.0066, Validation Loss: 0.0069\n",
      "Epoch 23/350, Training Loss: 0.0064, Validation Loss: 0.0067\n",
      "Epoch 24/350, Training Loss: 0.0062, Validation Loss: 0.0066\n",
      "Epoch 25/350, Training Loss: 0.0060, Validation Loss: 0.0065\n",
      "Epoch 26/350, Training Loss: 0.0059, Validation Loss: 0.0064\n",
      "Epoch 27/350, Training Loss: 0.0058, Validation Loss: 0.0063\n",
      "Epoch 28/350, Training Loss: 0.0057, Validation Loss: 0.0062\n",
      "Epoch 29/350, Training Loss: 0.0056, Validation Loss: 0.0061\n",
      "Epoch 30/350, Training Loss: 0.0055, Validation Loss: 0.0060\n",
      "Epoch 31/350, Training Loss: 0.0055, Validation Loss: 0.0059\n",
      "Epoch 32/350, Training Loss: 0.0054, Validation Loss: 0.0058\n",
      "Epoch 33/350, Training Loss: 0.0053, Validation Loss: 0.0057\n",
      "Epoch 34/350, Training Loss: 0.0053, Validation Loss: 0.0057\n",
      "Epoch 35/350, Training Loss: 0.0052, Validation Loss: 0.0056\n",
      "Epoch 36/350, Training Loss: 0.0052, Validation Loss: 0.0056\n",
      "Epoch 37/350, Training Loss: 0.0051, Validation Loss: 0.0055\n",
      "Epoch 38/350, Training Loss: 0.0051, Validation Loss: 0.0055\n",
      "Epoch 39/350, Training Loss: 0.0050, Validation Loss: 0.0054\n",
      "Epoch 40/350, Training Loss: 0.0050, Validation Loss: 0.0054\n",
      "Epoch 41/350, Training Loss: 0.0049, Validation Loss: 0.0054\n",
      "Epoch 42/350, Training Loss: 0.0049, Validation Loss: 0.0053\n",
      "Epoch 43/350, Training Loss: 0.0048, Validation Loss: 0.0053\n",
      "Epoch 44/350, Training Loss: 0.0048, Validation Loss: 0.0053\n",
      "Epoch 45/350, Training Loss: 0.0048, Validation Loss: 0.0052\n",
      "Epoch 46/350, Training Loss: 0.0047, Validation Loss: 0.0052\n",
      "Epoch 47/350, Training Loss: 0.0047, Validation Loss: 0.0052\n",
      "Epoch 48/350, Training Loss: 0.0047, Validation Loss: 0.0052\n",
      "Epoch 49/350, Training Loss: 0.0046, Validation Loss: 0.0051\n",
      "Epoch 50/350, Training Loss: 0.0046, Validation Loss: 0.0051\n",
      "Epoch 51/350, Training Loss: 0.0046, Validation Loss: 0.0051\n",
      "Epoch 52/350, Training Loss: 0.0046, Validation Loss: 0.0051\n",
      "Epoch 53/350, Training Loss: 0.0045, Validation Loss: 0.0050\n",
      "Epoch 54/350, Training Loss: 0.0045, Validation Loss: 0.0050\n",
      "Epoch 55/350, Training Loss: 0.0045, Validation Loss: 0.0050\n",
      "Epoch 56/350, Training Loss: 0.0045, Validation Loss: 0.0050\n",
      "Epoch 57/350, Training Loss: 0.0044, Validation Loss: 0.0050\n",
      "Epoch 58/350, Training Loss: 0.0044, Validation Loss: 0.0049\n",
      "Epoch 59/350, Training Loss: 0.0044, Validation Loss: 0.0049\n",
      "Epoch 60/350, Training Loss: 0.0044, Validation Loss: 0.0049\n",
      "Epoch 61/350, Training Loss: 0.0043, Validation Loss: 0.0049\n",
      "Epoch 62/350, Training Loss: 0.0043, Validation Loss: 0.0048\n",
      "Epoch 63/350, Training Loss: 0.0043, Validation Loss: 0.0048\n",
      "Epoch 64/350, Training Loss: 0.0043, Validation Loss: 0.0048\n",
      "Epoch 65/350, Training Loss: 0.0043, Validation Loss: 0.0048\n",
      "Epoch 66/350, Training Loss: 0.0042, Validation Loss: 0.0047\n",
      "Epoch 67/350, Training Loss: 0.0042, Validation Loss: 0.0047\n",
      "Epoch 68/350, Training Loss: 0.0042, Validation Loss: 0.0047\n",
      "Epoch 69/350, Training Loss: 0.0042, Validation Loss: 0.0047\n",
      "Epoch 70/350, Training Loss: 0.0042, Validation Loss: 0.0047\n",
      "Epoch 71/350, Training Loss: 0.0042, Validation Loss: 0.0046\n",
      "Epoch 72/350, Training Loss: 0.0041, Validation Loss: 0.0046\n",
      "Epoch 73/350, Training Loss: 0.0041, Validation Loss: 0.0046\n",
      "Epoch 74/350, Training Loss: 0.0041, Validation Loss: 0.0046\n",
      "Epoch 75/350, Training Loss: 0.0041, Validation Loss: 0.0045\n",
      "Epoch 76/350, Training Loss: 0.0041, Validation Loss: 0.0045\n",
      "Epoch 77/350, Training Loss: 0.0041, Validation Loss: 0.0045\n",
      "Epoch 78/350, Training Loss: 0.0040, Validation Loss: 0.0045\n",
      "Epoch 79/350, Training Loss: 0.0040, Validation Loss: 0.0045\n",
      "Epoch 80/350, Training Loss: 0.0040, Validation Loss: 0.0045\n",
      "Epoch 81/350, Training Loss: 0.0040, Validation Loss: 0.0044\n",
      "Epoch 82/350, Training Loss: 0.0040, Validation Loss: 0.0044\n",
      "Epoch 83/350, Training Loss: 0.0040, Validation Loss: 0.0044\n",
      "Epoch 84/350, Training Loss: 0.0040, Validation Loss: 0.0044\n",
      "Epoch 85/350, Training Loss: 0.0040, Validation Loss: 0.0044\n",
      "Epoch 86/350, Training Loss: 0.0039, Validation Loss: 0.0044\n",
      "Epoch 87/350, Training Loss: 0.0039, Validation Loss: 0.0044\n",
      "Epoch 88/350, Training Loss: 0.0039, Validation Loss: 0.0043\n",
      "Epoch 89/350, Training Loss: 0.0039, Validation Loss: 0.0043\n",
      "Epoch 90/350, Training Loss: 0.0039, Validation Loss: 0.0043\n",
      "Epoch 91/350, Training Loss: 0.0039, Validation Loss: 0.0043\n",
      "Epoch 92/350, Training Loss: 0.0039, Validation Loss: 0.0043\n",
      "Epoch 93/350, Training Loss: 0.0039, Validation Loss: 0.0043\n",
      "Epoch 94/350, Training Loss: 0.0039, Validation Loss: 0.0043\n",
      "Epoch 95/350, Training Loss: 0.0039, Validation Loss: 0.0043\n",
      "Epoch 96/350, Training Loss: 0.0038, Validation Loss: 0.0043\n",
      "Epoch 97/350, Training Loss: 0.0038, Validation Loss: 0.0043\n",
      "Epoch 98/350, Training Loss: 0.0038, Validation Loss: 0.0042\n",
      "Epoch 99/350, Training Loss: 0.0038, Validation Loss: 0.0042\n",
      "Epoch 100/350, Training Loss: 0.0038, Validation Loss: 0.0042\n",
      "Epoch 101/350, Training Loss: 0.0038, Validation Loss: 0.0042\n",
      "Epoch 102/350, Training Loss: 0.0038, Validation Loss: 0.0042\n",
      "Epoch 103/350, Training Loss: 0.0038, Validation Loss: 0.0042\n",
      "Epoch 104/350, Training Loss: 0.0038, Validation Loss: 0.0042\n",
      "Epoch 105/350, Training Loss: 0.0038, Validation Loss: 0.0042\n",
      "Epoch 106/350, Training Loss: 0.0038, Validation Loss: 0.0042\n",
      "Epoch 107/350, Training Loss: 0.0038, Validation Loss: 0.0042\n",
      "Epoch 108/350, Training Loss: 0.0037, Validation Loss: 0.0042\n",
      "Epoch 109/350, Training Loss: 0.0037, Validation Loss: 0.0042\n",
      "Epoch 110/350, Training Loss: 0.0037, Validation Loss: 0.0042\n",
      "Epoch 111/350, Training Loss: 0.0037, Validation Loss: 0.0042\n",
      "Epoch 112/350, Training Loss: 0.0037, Validation Loss: 0.0042\n",
      "Epoch 113/350, Training Loss: 0.0037, Validation Loss: 0.0042\n",
      "Epoch 114/350, Training Loss: 0.0037, Validation Loss: 0.0042\n",
      "Epoch 115/350, Training Loss: 0.0037, Validation Loss: 0.0042\n",
      "Epoch 116/350, Training Loss: 0.0037, Validation Loss: 0.0041\n",
      "Epoch 117/350, Training Loss: 0.0037, Validation Loss: 0.0041\n",
      "Epoch 118/350, Training Loss: 0.0037, Validation Loss: 0.0041\n",
      "Epoch 119/350, Training Loss: 0.0037, Validation Loss: 0.0041\n",
      "Epoch 120/350, Training Loss: 0.0037, Validation Loss: 0.0041\n",
      "Epoch 121/350, Training Loss: 0.0037, Validation Loss: 0.0041\n",
      "Epoch 122/350, Training Loss: 0.0037, Validation Loss: 0.0041\n",
      "Epoch 123/350, Training Loss: 0.0037, Validation Loss: 0.0041\n",
      "Epoch 124/350, Training Loss: 0.0037, Validation Loss: 0.0041\n",
      "Epoch 125/350, Training Loss: 0.0037, Validation Loss: 0.0041\n",
      "Epoch 126/350, Training Loss: 0.0036, Validation Loss: 0.0041\n",
      "Epoch 127/350, Training Loss: 0.0036, Validation Loss: 0.0041\n",
      "Epoch 128/350, Training Loss: 0.0036, Validation Loss: 0.0041\n",
      "Epoch 129/350, Training Loss: 0.0036, Validation Loss: 0.0041\n",
      "Epoch 130/350, Training Loss: 0.0036, Validation Loss: 0.0041\n",
      "Epoch 131/350, Training Loss: 0.0036, Validation Loss: 0.0041\n",
      "Epoch 132/350, Training Loss: 0.0036, Validation Loss: 0.0041\n",
      "Epoch 133/350, Training Loss: 0.0036, Validation Loss: 0.0041\n",
      "Epoch 134/350, Training Loss: 0.0036, Validation Loss: 0.0041\n",
      "Epoch 135/350, Training Loss: 0.0036, Validation Loss: 0.0041\n",
      "Epoch 136/350, Training Loss: 0.0036, Validation Loss: 0.0041\n",
      "Epoch 137/350, Training Loss: 0.0036, Validation Loss: 0.0041\n",
      "Epoch 138/350, Training Loss: 0.0036, Validation Loss: 0.0041\n",
      "Epoch 139/350, Training Loss: 0.0036, Validation Loss: 0.0041\n",
      "Epoch 140/350, Training Loss: 0.0036, Validation Loss: 0.0041\n",
      "Epoch 141/350, Training Loss: 0.0036, Validation Loss: 0.0041\n",
      "Epoch 142/350, Training Loss: 0.0036, Validation Loss: 0.0041\n",
      "Epoch 143/350, Training Loss: 0.0036, Validation Loss: 0.0041\n",
      "Epoch 144/350, Training Loss: 0.0036, Validation Loss: 0.0041\n",
      "Epoch 145/350, Training Loss: 0.0036, Validation Loss: 0.0041\n",
      "Epoch 146/350, Training Loss: 0.0036, Validation Loss: 0.0041\n",
      "Epoch 147/350, Training Loss: 0.0036, Validation Loss: 0.0041\n",
      "Epoch 148/350, Training Loss: 0.0036, Validation Loss: 0.0041\n",
      "Epoch 149/350, Training Loss: 0.0036, Validation Loss: 0.0041\n",
      "Epoch 150/350, Training Loss: 0.0036, Validation Loss: 0.0041\n",
      "Epoch 151/350, Training Loss: 0.0036, Validation Loss: 0.0041\n",
      "Epoch 152/350, Training Loss: 0.0036, Validation Loss: 0.0041\n",
      "Epoch 153/350, Training Loss: 0.0036, Validation Loss: 0.0041\n",
      "Epoch 154/350, Training Loss: 0.0035, Validation Loss: 0.0041\n",
      "Epoch 155/350, Training Loss: 0.0035, Validation Loss: 0.0041\n",
      "Epoch 156/350, Training Loss: 0.0035, Validation Loss: 0.0041\n",
      "Epoch 157/350, Training Loss: 0.0035, Validation Loss: 0.0040\n",
      "Epoch 158/350, Training Loss: 0.0035, Validation Loss: 0.0040\n",
      "Epoch 159/350, Training Loss: 0.0035, Validation Loss: 0.0040\n",
      "Epoch 160/350, Training Loss: 0.0035, Validation Loss: 0.0040\n",
      "Epoch 161/350, Training Loss: 0.0035, Validation Loss: 0.0040\n",
      "Epoch 162/350, Training Loss: 0.0035, Validation Loss: 0.0040\n",
      "Epoch 163/350, Training Loss: 0.0035, Validation Loss: 0.0040\n",
      "Epoch 164/350, Training Loss: 0.0035, Validation Loss: 0.0040\n",
      "Epoch 165/350, Training Loss: 0.0035, Validation Loss: 0.0040\n",
      "Epoch 166/350, Training Loss: 0.0035, Validation Loss: 0.0040\n",
      "Epoch 167/350, Training Loss: 0.0035, Validation Loss: 0.0040\n",
      "Epoch 168/350, Training Loss: 0.0035, Validation Loss: 0.0040\n",
      "Epoch 169/350, Training Loss: 0.0035, Validation Loss: 0.0040\n",
      "Epoch 170/350, Training Loss: 0.0035, Validation Loss: 0.0040\n",
      "Epoch 171/350, Training Loss: 0.0035, Validation Loss: 0.0040\n",
      "Epoch 172/350, Training Loss: 0.0035, Validation Loss: 0.0040\n",
      "Epoch 173/350, Training Loss: 0.0035, Validation Loss: 0.0040\n",
      "Epoch 174/350, Training Loss: 0.0035, Validation Loss: 0.0040\n",
      "Epoch 175/350, Training Loss: 0.0035, Validation Loss: 0.0040\n",
      "Epoch 176/350, Training Loss: 0.0035, Validation Loss: 0.0040\n",
      "Epoch 177/350, Training Loss: 0.0035, Validation Loss: 0.0040\n",
      "Epoch 178/350, Training Loss: 0.0035, Validation Loss: 0.0040\n",
      "Epoch 179/350, Training Loss: 0.0035, Validation Loss: 0.0040\n",
      "Epoch 180/350, Training Loss: 0.0035, Validation Loss: 0.0040\n",
      "Epoch 181/350, Training Loss: 0.0035, Validation Loss: 0.0040\n",
      "Epoch 182/350, Training Loss: 0.0035, Validation Loss: 0.0040\n",
      "Epoch 183/350, Training Loss: 0.0035, Validation Loss: 0.0040\n",
      "Epoch 184/350, Training Loss: 0.0035, Validation Loss: 0.0040\n",
      "Epoch 185/350, Training Loss: 0.0035, Validation Loss: 0.0040\n",
      "Epoch 186/350, Training Loss: 0.0035, Validation Loss: 0.0040\n",
      "Epoch 187/350, Training Loss: 0.0035, Validation Loss: 0.0040\n",
      "Epoch 188/350, Training Loss: 0.0035, Validation Loss: 0.0040\n",
      "Epoch 189/350, Training Loss: 0.0035, Validation Loss: 0.0040\n",
      "Epoch 190/350, Training Loss: 0.0035, Validation Loss: 0.0040\n",
      "Epoch 191/350, Training Loss: 0.0035, Validation Loss: 0.0040\n",
      "Epoch 192/350, Training Loss: 0.0035, Validation Loss: 0.0040\n",
      "Epoch 193/350, Training Loss: 0.0034, Validation Loss: 0.0040\n",
      "Epoch 194/350, Training Loss: 0.0034, Validation Loss: 0.0040\n",
      "Epoch 195/350, Training Loss: 0.0034, Validation Loss: 0.0040\n",
      "Epoch 196/350, Training Loss: 0.0034, Validation Loss: 0.0040\n",
      "Epoch 197/350, Training Loss: 0.0034, Validation Loss: 0.0040\n",
      "Epoch 198/350, Training Loss: 0.0034, Validation Loss: 0.0040\n",
      "Epoch 199/350, Training Loss: 0.0034, Validation Loss: 0.0039\n",
      "Epoch 200/350, Training Loss: 0.0034, Validation Loss: 0.0039\n",
      "Epoch 201/350, Training Loss: 0.0034, Validation Loss: 0.0039\n",
      "Epoch 202/350, Training Loss: 0.0034, Validation Loss: 0.0039\n",
      "Epoch 203/350, Training Loss: 0.0034, Validation Loss: 0.0039\n",
      "Epoch 204/350, Training Loss: 0.0034, Validation Loss: 0.0039\n",
      "Epoch 205/350, Training Loss: 0.0034, Validation Loss: 0.0039\n",
      "Epoch 206/350, Training Loss: 0.0034, Validation Loss: 0.0039\n",
      "Epoch 207/350, Training Loss: 0.0034, Validation Loss: 0.0039\n",
      "Epoch 208/350, Training Loss: 0.0034, Validation Loss: 0.0039\n",
      "Epoch 209/350, Training Loss: 0.0034, Validation Loss: 0.0039\n",
      "Epoch 210/350, Training Loss: 0.0034, Validation Loss: 0.0039\n",
      "Epoch 211/350, Training Loss: 0.0034, Validation Loss: 0.0039\n",
      "Epoch 212/350, Training Loss: 0.0034, Validation Loss: 0.0039\n",
      "Epoch 213/350, Training Loss: 0.0034, Validation Loss: 0.0039\n",
      "Epoch 214/350, Training Loss: 0.0034, Validation Loss: 0.0039\n",
      "Epoch 215/350, Training Loss: 0.0034, Validation Loss: 0.0039\n",
      "Epoch 216/350, Training Loss: 0.0034, Validation Loss: 0.0039\n",
      "Epoch 217/350, Training Loss: 0.0034, Validation Loss: 0.0039\n",
      "Epoch 218/350, Training Loss: 0.0034, Validation Loss: 0.0039\n",
      "Epoch 219/350, Training Loss: 0.0034, Validation Loss: 0.0039\n",
      "Epoch 220/350, Training Loss: 0.0034, Validation Loss: 0.0039\n",
      "Epoch 221/350, Training Loss: 0.0034, Validation Loss: 0.0039\n",
      "Epoch 222/350, Training Loss: 0.0034, Validation Loss: 0.0039\n",
      "Epoch 223/350, Training Loss: 0.0034, Validation Loss: 0.0039\n",
      "Epoch 224/350, Training Loss: 0.0034, Validation Loss: 0.0039\n",
      "Epoch 225/350, Training Loss: 0.0034, Validation Loss: 0.0039\n",
      "Epoch 226/350, Training Loss: 0.0034, Validation Loss: 0.0039\n",
      "Epoch 227/350, Training Loss: 0.0034, Validation Loss: 0.0039\n",
      "Epoch 228/350, Training Loss: 0.0034, Validation Loss: 0.0039\n",
      "Epoch 229/350, Training Loss: 0.0034, Validation Loss: 0.0039\n",
      "Epoch 230/350, Training Loss: 0.0034, Validation Loss: 0.0039\n",
      "Epoch 231/350, Training Loss: 0.0034, Validation Loss: 0.0039\n",
      "Epoch 232/350, Training Loss: 0.0034, Validation Loss: 0.0039\n",
      "Epoch 233/350, Training Loss: 0.0034, Validation Loss: 0.0039\n",
      "Epoch 234/350, Training Loss: 0.0034, Validation Loss: 0.0039\n",
      "Epoch 235/350, Training Loss: 0.0034, Validation Loss: 0.0039\n",
      "Epoch 236/350, Training Loss: 0.0034, Validation Loss: 0.0039\n",
      "Epoch 237/350, Training Loss: 0.0034, Validation Loss: 0.0039\n",
      "Epoch 238/350, Training Loss: 0.0034, Validation Loss: 0.0039\n",
      "Epoch 239/350, Training Loss: 0.0034, Validation Loss: 0.0039\n",
      "Epoch 240/350, Training Loss: 0.0034, Validation Loss: 0.0039\n",
      "Epoch 241/350, Training Loss: 0.0034, Validation Loss: 0.0039\n",
      "Epoch 242/350, Training Loss: 0.0034, Validation Loss: 0.0039\n",
      "Epoch 243/350, Training Loss: 0.0034, Validation Loss: 0.0039\n",
      "Epoch 244/350, Training Loss: 0.0034, Validation Loss: 0.0039\n",
      "Epoch 245/350, Training Loss: 0.0034, Validation Loss: 0.0039\n",
      "Epoch 246/350, Training Loss: 0.0034, Validation Loss: 0.0039\n",
      "Epoch 247/350, Training Loss: 0.0034, Validation Loss: 0.0039\n",
      "Epoch 248/350, Training Loss: 0.0034, Validation Loss: 0.0039\n",
      "Epoch 249/350, Training Loss: 0.0033, Validation Loss: 0.0039\n",
      "Epoch 250/350, Training Loss: 0.0033, Validation Loss: 0.0039\n",
      "Epoch 251/350, Training Loss: 0.0033, Validation Loss: 0.0039\n",
      "Epoch 252/350, Training Loss: 0.0033, Validation Loss: 0.0039\n",
      "Epoch 253/350, Training Loss: 0.0033, Validation Loss: 0.0039\n",
      "Epoch 254/350, Training Loss: 0.0033, Validation Loss: 0.0039\n",
      "Epoch 255/350, Training Loss: 0.0033, Validation Loss: 0.0039\n",
      "Epoch 256/350, Training Loss: 0.0033, Validation Loss: 0.0039\n",
      "Epoch 257/350, Training Loss: 0.0033, Validation Loss: 0.0039\n",
      "Epoch 258/350, Training Loss: 0.0033, Validation Loss: 0.0039\n",
      "Epoch 259/350, Training Loss: 0.0033, Validation Loss: 0.0039\n",
      "Epoch 260/350, Training Loss: 0.0033, Validation Loss: 0.0039\n",
      "Epoch 261/350, Training Loss: 0.0033, Validation Loss: 0.0039\n",
      "Epoch 262/350, Training Loss: 0.0033, Validation Loss: 0.0039\n",
      "Epoch 263/350, Training Loss: 0.0033, Validation Loss: 0.0039\n",
      "Epoch 264/350, Training Loss: 0.0033, Validation Loss: 0.0039\n",
      "Epoch 265/350, Training Loss: 0.0033, Validation Loss: 0.0039\n",
      "Epoch 266/350, Training Loss: 0.0033, Validation Loss: 0.0039\n",
      "Epoch 267/350, Training Loss: 0.0033, Validation Loss: 0.0039\n",
      "Epoch 268/350, Training Loss: 0.0033, Validation Loss: 0.0039\n",
      "Epoch 269/350, Training Loss: 0.0033, Validation Loss: 0.0039\n",
      "Epoch 270/350, Training Loss: 0.0033, Validation Loss: 0.0039\n",
      "Epoch 271/350, Training Loss: 0.0033, Validation Loss: 0.0039\n",
      "Epoch 272/350, Training Loss: 0.0033, Validation Loss: 0.0039\n",
      "Epoch 273/350, Training Loss: 0.0033, Validation Loss: 0.0039\n",
      "Epoch 274/350, Training Loss: 0.0033, Validation Loss: 0.0039\n",
      "Epoch 275/350, Training Loss: 0.0033, Validation Loss: 0.0039\n",
      "Epoch 276/350, Training Loss: 0.0033, Validation Loss: 0.0039\n",
      "Epoch 277/350, Training Loss: 0.0033, Validation Loss: 0.0039\n",
      "Epoch 278/350, Training Loss: 0.0033, Validation Loss: 0.0039\n",
      "Epoch 279/350, Training Loss: 0.0033, Validation Loss: 0.0039\n",
      "Epoch 280/350, Training Loss: 0.0033, Validation Loss: 0.0039\n",
      "Epoch 281/350, Training Loss: 0.0033, Validation Loss: 0.0039\n",
      "Epoch 282/350, Training Loss: 0.0033, Validation Loss: 0.0039\n",
      "Epoch 283/350, Training Loss: 0.0033, Validation Loss: 0.0039\n",
      "Epoch 284/350, Training Loss: 0.0033, Validation Loss: 0.0039\n",
      "Epoch 285/350, Training Loss: 0.0033, Validation Loss: 0.0039\n",
      "Epoch 286/350, Training Loss: 0.0033, Validation Loss: 0.0039\n",
      "Epoch 287/350, Training Loss: 0.0033, Validation Loss: 0.0039\n",
      "Epoch 288/350, Training Loss: 0.0033, Validation Loss: 0.0039\n",
      "Epoch 289/350, Training Loss: 0.0033, Validation Loss: 0.0039\n",
      "Epoch 290/350, Training Loss: 0.0033, Validation Loss: 0.0039\n",
      "Epoch 291/350, Training Loss: 0.0033, Validation Loss: 0.0039\n",
      "Epoch 292/350, Training Loss: 0.0033, Validation Loss: 0.0039\n",
      "Epoch 293/350, Training Loss: 0.0033, Validation Loss: 0.0039\n",
      "Epoch 294/350, Training Loss: 0.0033, Validation Loss: 0.0039\n",
      "Epoch 295/350, Training Loss: 0.0033, Validation Loss: 0.0039\n",
      "Epoch 296/350, Training Loss: 0.0033, Validation Loss: 0.0039\n",
      "Epoch 297/350, Training Loss: 0.0033, Validation Loss: 0.0039\n",
      "Epoch 298/350, Training Loss: 0.0033, Validation Loss: 0.0039\n",
      "Epoch 299/350, Training Loss: 0.0033, Validation Loss: 0.0039\n",
      "Epoch 300/350, Training Loss: 0.0033, Validation Loss: 0.0039\n",
      "Epoch 301/350, Training Loss: 0.0033, Validation Loss: 0.0039\n",
      "Epoch 302/350, Training Loss: 0.0033, Validation Loss: 0.0039\n",
      "Epoch 303/350, Training Loss: 0.0033, Validation Loss: 0.0039\n",
      "Epoch 304/350, Training Loss: 0.0033, Validation Loss: 0.0039\n",
      "Epoch 305/350, Training Loss: 0.0033, Validation Loss: 0.0039\n",
      "Epoch 306/350, Training Loss: 0.0033, Validation Loss: 0.0039\n",
      "Epoch 307/350, Training Loss: 0.0033, Validation Loss: 0.0039\n",
      "Epoch 308/350, Training Loss: 0.0033, Validation Loss: 0.0039\n",
      "Epoch 309/350, Training Loss: 0.0033, Validation Loss: 0.0039\n",
      "Epoch 310/350, Training Loss: 0.0033, Validation Loss: 0.0039\n",
      "Epoch 311/350, Training Loss: 0.0033, Validation Loss: 0.0039\n",
      "Epoch 312/350, Training Loss: 0.0033, Validation Loss: 0.0039\n",
      "Epoch 313/350, Training Loss: 0.0033, Validation Loss: 0.0039\n",
      "Epoch 314/350, Training Loss: 0.0033, Validation Loss: 0.0039\n",
      "Epoch 315/350, Training Loss: 0.0033, Validation Loss: 0.0039\n",
      "Epoch 316/350, Training Loss: 0.0033, Validation Loss: 0.0039\n",
      "Epoch 317/350, Training Loss: 0.0033, Validation Loss: 0.0039\n",
      "Epoch 318/350, Training Loss: 0.0033, Validation Loss: 0.0039\n",
      "Epoch 319/350, Training Loss: 0.0033, Validation Loss: 0.0039\n",
      "Epoch 320/350, Training Loss: 0.0033, Validation Loss: 0.0039\n",
      "Epoch 321/350, Training Loss: 0.0033, Validation Loss: 0.0039\n",
      "Epoch 322/350, Training Loss: 0.0033, Validation Loss: 0.0039\n",
      "Epoch 323/350, Training Loss: 0.0033, Validation Loss: 0.0039\n",
      "Epoch 324/350, Training Loss: 0.0033, Validation Loss: 0.0039\n",
      "Epoch 325/350, Training Loss: 0.0033, Validation Loss: 0.0039\n",
      "Epoch 326/350, Training Loss: 0.0033, Validation Loss: 0.0039\n",
      "Epoch 327/350, Training Loss: 0.0033, Validation Loss: 0.0039\n",
      "Epoch 328/350, Training Loss: 0.0033, Validation Loss: 0.0039\n",
      "Epoch 329/350, Training Loss: 0.0033, Validation Loss: 0.0039\n",
      "Epoch 330/350, Training Loss: 0.0033, Validation Loss: 0.0039\n",
      "Epoch 331/350, Training Loss: 0.0033, Validation Loss: 0.0039\n",
      "Epoch 332/350, Training Loss: 0.0033, Validation Loss: 0.0039\n",
      "Epoch 333/350, Training Loss: 0.0033, Validation Loss: 0.0039\n",
      "Epoch 334/350, Training Loss: 0.0033, Validation Loss: 0.0039\n",
      "Epoch 335/350, Training Loss: 0.0033, Validation Loss: 0.0039\n",
      "Epoch 336/350, Training Loss: 0.0033, Validation Loss: 0.0039\n",
      "Epoch 337/350, Training Loss: 0.0033, Validation Loss: 0.0039\n",
      "Epoch 338/350, Training Loss: 0.0033, Validation Loss: 0.0039\n",
      "Epoch 339/350, Training Loss: 0.0033, Validation Loss: 0.0039\n",
      "Epoch 340/350, Training Loss: 0.0033, Validation Loss: 0.0039\n",
      "Epoch 341/350, Training Loss: 0.0033, Validation Loss: 0.0039\n",
      "Epoch 342/350, Training Loss: 0.0033, Validation Loss: 0.0039\n",
      "Epoch 343/350, Training Loss: 0.0033, Validation Loss: 0.0039\n",
      "Epoch 344/350, Training Loss: 0.0033, Validation Loss: 0.0039\n",
      "Epoch 345/350, Training Loss: 0.0033, Validation Loss: 0.0039\n",
      "Epoch 346/350, Training Loss: 0.0033, Validation Loss: 0.0039\n",
      "Epoch 347/350, Training Loss: 0.0033, Validation Loss: 0.0039\n",
      "Epoch 348/350, Training Loss: 0.0033, Validation Loss: 0.0039\n",
      "Epoch 349/350, Training Loss: 0.0033, Validation Loss: 0.0039\n",
      "Epoch 350/350, Training Loss: 0.0033, Validation Loss: 0.0039\n",
      "1.1350219468476588\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "_window_size = 7\n",
    "horizon = 1\n",
    "_stride = 1\n",
    "\n",
    "# TGCN\n",
    "num_epochs_TGCN = 350\n",
    "\n",
    "n_neighbors = 2\n",
    "df_PeMS_old, df_distance  = load_PeMS04_flow_data()\n",
    "df_PeMS, adjacency_matrix_PeMS, meanstd_dict = preprocess_PeMS_data(df_PeMS_old, df_distance, init_node=0, n_neighbors=n_neighbors, center_and_reduce=True)\n",
    "print(df_PeMS.columns)\n",
    "node_0, _ = mask_unknown_nodes(df_PeMS=df_PeMS, list_nodes_known=[117, 0, 92])\n",
    "node_1, mask_node_1 = mask_unknown_nodes(df_PeMS=df_PeMS, list_nodes_known=[117, 0])\n",
    "\n",
    "# TGCN Model\n",
    "model_TGCN = TGCN(adjacency_matrix_PeMS, hidden_dim=32, output_size=len(df_PeMS.columns))\n",
    "train_loader_TGCN, val_loader_TGCN, test_loader_TGCN, _ = createLoaders(node_0, window_size=_window_size, stride=_stride, prediction_horizon=horizon)\n",
    "model_path = \"./test3_3.pkl\"\n",
    "best_model , train_losses, valid_losses = train_model(model_TGCN, train_loader_TGCN, val_loader_TGCN, model_path=model_path, mask=None, num_epochs=num_epochs_TGCN, remove=False)\n",
    "y_true, y_pred = testmodel(best_model=best_model, test_loader=test_loader_TGCN, meanstd_dict=meanstd_dict, sensor_order_list=node_0.columns)\n",
    "rmse = 0\n",
    "for i in range(n_neighbors+1):\n",
    "    rmse = rmse + np.sqrt(mean_squared_error(y_true[:,:,i], y_pred[:,:,i]))\n",
    "print(rmse/n_neighbors+1)\n",
    "\n",
    "model_TGCN = TGCN(adjacency_matrix_PeMS, hidden_dim=32, output_size=len(df_PeMS.columns))\n",
    "train_loader_TGCN, val_loader_TGCN, test_loader_TGCN, _ = createLoaders(node_1, window_size=_window_size, stride=_stride, prediction_horizon=horizon)\n",
    "model_path = \"./test2_3.pkl\"\n",
    "best_model , train_losses, valid_losses = train_model(model_TGCN, train_loader_TGCN, val_loader_TGCN, model_path=model_path, mask=None, num_epochs=num_epochs_TGCN, remove=False)\n",
    "y_true, y_pred = testmodel(best_model=best_model, test_loader=test_loader_TGCN, meanstd_dict=meanstd_dict, sensor_order_list=node_1.columns)\n",
    "rmse = 0\n",
    "for i in range(n_neighbors+1):\n",
    "    rmse = rmse + np.sqrt(mean_squared_error(y_true[:,:,i], y_pred[:,:,i]))\n",
    "print(rmse/n_neighbors+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gcogoni\\Documents\\Stage\\Federated-Traffic-Flow-Forecasting\\src\\utils_graph.py:98: FutureWarning: adjacency_matrix will return a scipy.sparse array instead of a matrix in Networkx 3.0.\n",
      "  return nx.adjacency_matrix(graph, nodelist=nodes_order, weight=None).toarray()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Int64Index([243, 117, 28, 0, 173, 92], dtype='int64')\n",
      "Epoch 1/350, Training Loss: 0.2216, Validation Loss: 0.0742\n",
      "Epoch 2/350, Training Loss: 0.0879, Validation Loss: 0.0598\n",
      "Epoch 3/350, Training Loss: 0.0718, Validation Loss: 0.0481\n",
      "Epoch 4/350, Training Loss: 0.0633, Validation Loss: 0.0452\n",
      "Epoch 5/350, Training Loss: 0.0598, Validation Loss: 0.0434\n",
      "Epoch 6/350, Training Loss: 0.0564, Validation Loss: 0.0411\n",
      "Epoch 7/350, Training Loss: 0.0516, Validation Loss: 0.0378\n",
      "Epoch 8/350, Training Loss: 0.0448, Validation Loss: 0.0337\n",
      "Epoch 9/350, Training Loss: 0.0378, Validation Loss: 0.0305\n",
      "Epoch 10/350, Training Loss: 0.0331, Validation Loss: 0.0289\n",
      "Epoch 11/350, Training Loss: 0.0302, Validation Loss: 0.0264\n",
      "Epoch 12/350, Training Loss: 0.0285, Validation Loss: 0.0243\n",
      "Epoch 13/350, Training Loss: 0.0268, Validation Loss: 0.0237\n",
      "Epoch 14/350, Training Loss: 0.0255, Validation Loss: 0.0244\n",
      "Epoch 15/350, Training Loss: 0.0237, Validation Loss: 0.0216\n",
      "Epoch 16/350, Training Loss: 0.0229, Validation Loss: 0.0224\n",
      "Epoch 17/350, Training Loss: 0.0218, Validation Loss: 0.0202\n",
      "Epoch 18/350, Training Loss: 0.0213, Validation Loss: 0.0186\n",
      "Epoch 19/350, Training Loss: 0.0207, Validation Loss: 0.0179\n",
      "Epoch 20/350, Training Loss: 0.0202, Validation Loss: 0.0174\n",
      "Epoch 21/350, Training Loss: 0.0197, Validation Loss: 0.0170\n",
      "Epoch 22/350, Training Loss: 0.0192, Validation Loss: 0.0166\n",
      "Epoch 23/350, Training Loss: 0.0186, Validation Loss: 0.0168\n",
      "Epoch 24/350, Training Loss: 0.0186, Validation Loss: 0.0164\n",
      "Epoch 25/350, Training Loss: 0.0179, Validation Loss: 0.0164\n",
      "Epoch 26/350, Training Loss: 0.0178, Validation Loss: 0.0163\n",
      "Epoch 27/350, Training Loss: 0.0175, Validation Loss: 0.0164\n",
      "Epoch 28/350, Training Loss: 0.0172, Validation Loss: 0.0165\n",
      "Epoch 29/350, Training Loss: 0.0170, Validation Loss: 0.0167\n",
      "Epoch 30/350, Training Loss: 0.0168, Validation Loss: 0.0170\n",
      "Epoch 31/350, Training Loss: 0.0166, Validation Loss: 0.0173\n",
      "Epoch 32/350, Training Loss: 0.0164, Validation Loss: 0.0176\n",
      "Epoch 33/350, Training Loss: 0.0162, Validation Loss: 0.0178\n",
      "Epoch 34/350, Training Loss: 0.0160, Validation Loss: 0.0180\n",
      "Epoch 35/350, Training Loss: 0.0159, Validation Loss: 0.0182\n",
      "Epoch 36/350, Training Loss: 0.0157, Validation Loss: 0.0183\n",
      "Epoch 37/350, Training Loss: 0.0156, Validation Loss: 0.0183\n",
      "Epoch 38/350, Training Loss: 0.0154, Validation Loss: 0.0183\n",
      "Epoch 39/350, Training Loss: 0.0153, Validation Loss: 0.0183\n",
      "Epoch 40/350, Training Loss: 0.0152, Validation Loss: 0.0182\n",
      "Epoch 41/350, Training Loss: 0.0151, Validation Loss: 0.0182\n",
      "Epoch 42/350, Training Loss: 0.0150, Validation Loss: 0.0181\n",
      "Epoch 43/350, Training Loss: 0.0149, Validation Loss: 0.0180\n",
      "Epoch 44/350, Training Loss: 0.0148, Validation Loss: 0.0178\n",
      "Epoch 45/350, Training Loss: 0.0147, Validation Loss: 0.0177\n",
      "Epoch 46/350, Training Loss: 0.0146, Validation Loss: 0.0175\n",
      "Epoch 47/350, Training Loss: 0.0145, Validation Loss: 0.0174\n",
      "Epoch 48/350, Training Loss: 0.0144, Validation Loss: 0.0172\n",
      "Epoch 49/350, Training Loss: 0.0143, Validation Loss: 0.0170\n",
      "Epoch 50/350, Training Loss: 0.0143, Validation Loss: 0.0168\n",
      "Epoch 51/350, Training Loss: 0.0142, Validation Loss: 0.0167\n",
      "Epoch 52/350, Training Loss: 0.0141, Validation Loss: 0.0165\n",
      "Epoch 53/350, Training Loss: 0.0141, Validation Loss: 0.0163\n",
      "Epoch 54/350, Training Loss: 0.0140, Validation Loss: 0.0162\n",
      "Epoch 55/350, Training Loss: 0.0139, Validation Loss: 0.0160\n",
      "Epoch 56/350, Training Loss: 0.0139, Validation Loss: 0.0158\n",
      "Epoch 57/350, Training Loss: 0.0138, Validation Loss: 0.0157\n",
      "Epoch 58/350, Training Loss: 0.0138, Validation Loss: 0.0156\n",
      "Epoch 59/350, Training Loss: 0.0137, Validation Loss: 0.0154\n",
      "Epoch 60/350, Training Loss: 0.0136, Validation Loss: 0.0153\n",
      "Epoch 61/350, Training Loss: 0.0136, Validation Loss: 0.0152\n",
      "Epoch 62/350, Training Loss: 0.0135, Validation Loss: 0.0150\n",
      "Epoch 63/350, Training Loss: 0.0135, Validation Loss: 0.0149\n",
      "Epoch 64/350, Training Loss: 0.0134, Validation Loss: 0.0148\n",
      "Epoch 65/350, Training Loss: 0.0134, Validation Loss: 0.0147\n",
      "Epoch 66/350, Training Loss: 0.0133, Validation Loss: 0.0146\n",
      "Epoch 67/350, Training Loss: 0.0133, Validation Loss: 0.0145\n",
      "Epoch 68/350, Training Loss: 0.0132, Validation Loss: 0.0144\n",
      "Epoch 69/350, Training Loss: 0.0132, Validation Loss: 0.0144\n",
      "Epoch 70/350, Training Loss: 0.0131, Validation Loss: 0.0143\n",
      "Epoch 71/350, Training Loss: 0.0131, Validation Loss: 0.0142\n",
      "Epoch 72/350, Training Loss: 0.0130, Validation Loss: 0.0141\n",
      "Epoch 73/350, Training Loss: 0.0130, Validation Loss: 0.0140\n",
      "Epoch 74/350, Training Loss: 0.0129, Validation Loss: 0.0140\n",
      "Epoch 75/350, Training Loss: 0.0129, Validation Loss: 0.0139\n",
      "Epoch 76/350, Training Loss: 0.0128, Validation Loss: 0.0138\n",
      "Epoch 77/350, Training Loss: 0.0128, Validation Loss: 0.0138\n",
      "Epoch 78/350, Training Loss: 0.0127, Validation Loss: 0.0137\n",
      "Epoch 79/350, Training Loss: 0.0127, Validation Loss: 0.0137\n",
      "Epoch 80/350, Training Loss: 0.0127, Validation Loss: 0.0136\n",
      "Epoch 81/350, Training Loss: 0.0126, Validation Loss: 0.0136\n",
      "Epoch 82/350, Training Loss: 0.0126, Validation Loss: 0.0135\n",
      "Epoch 83/350, Training Loss: 0.0125, Validation Loss: 0.0134\n",
      "Epoch 84/350, Training Loss: 0.0125, Validation Loss: 0.0134\n",
      "Epoch 85/350, Training Loss: 0.0124, Validation Loss: 0.0133\n",
      "Epoch 86/350, Training Loss: 0.0124, Validation Loss: 0.0133\n",
      "Epoch 87/350, Training Loss: 0.0124, Validation Loss: 0.0132\n",
      "Epoch 88/350, Training Loss: 0.0123, Validation Loss: 0.0132\n",
      "Epoch 89/350, Training Loss: 0.0123, Validation Loss: 0.0131\n",
      "Epoch 90/350, Training Loss: 0.0122, Validation Loss: 0.0131\n",
      "Epoch 91/350, Training Loss: 0.0122, Validation Loss: 0.0130\n",
      "Epoch 92/350, Training Loss: 0.0122, Validation Loss: 0.0130\n",
      "Epoch 93/350, Training Loss: 0.0121, Validation Loss: 0.0129\n",
      "Epoch 94/350, Training Loss: 0.0121, Validation Loss: 0.0129\n",
      "Epoch 95/350, Training Loss: 0.0120, Validation Loss: 0.0129\n",
      "Epoch 96/350, Training Loss: 0.0120, Validation Loss: 0.0128\n",
      "Epoch 97/350, Training Loss: 0.0120, Validation Loss: 0.0128\n",
      "Epoch 98/350, Training Loss: 0.0119, Validation Loss: 0.0127\n",
      "Epoch 99/350, Training Loss: 0.0119, Validation Loss: 0.0127\n",
      "Epoch 100/350, Training Loss: 0.0118, Validation Loss: 0.0126\n",
      "Epoch 101/350, Training Loss: 0.0118, Validation Loss: 0.0126\n",
      "Epoch 102/350, Training Loss: 0.0118, Validation Loss: 0.0125\n",
      "Epoch 103/350, Training Loss: 0.0117, Validation Loss: 0.0125\n",
      "Epoch 104/350, Training Loss: 0.0117, Validation Loss: 0.0125\n",
      "Epoch 105/350, Training Loss: 0.0117, Validation Loss: 0.0124\n",
      "Epoch 106/350, Training Loss: 0.0116, Validation Loss: 0.0124\n",
      "Epoch 107/350, Training Loss: 0.0116, Validation Loss: 0.0123\n",
      "Epoch 108/350, Training Loss: 0.0116, Validation Loss: 0.0123\n",
      "Epoch 109/350, Training Loss: 0.0115, Validation Loss: 0.0123\n",
      "Epoch 110/350, Training Loss: 0.0115, Validation Loss: 0.0122\n",
      "Epoch 111/350, Training Loss: 0.0115, Validation Loss: 0.0122\n",
      "Epoch 112/350, Training Loss: 0.0114, Validation Loss: 0.0122\n",
      "Epoch 113/350, Training Loss: 0.0114, Validation Loss: 0.0121\n",
      "Epoch 114/350, Training Loss: 0.0114, Validation Loss: 0.0121\n",
      "Epoch 115/350, Training Loss: 0.0113, Validation Loss: 0.0121\n",
      "Epoch 116/350, Training Loss: 0.0113, Validation Loss: 0.0121\n",
      "Epoch 117/350, Training Loss: 0.0113, Validation Loss: 0.0120\n",
      "Epoch 118/350, Training Loss: 0.0112, Validation Loss: 0.0120\n",
      "Epoch 119/350, Training Loss: 0.0112, Validation Loss: 0.0120\n",
      "Epoch 120/350, Training Loss: 0.0112, Validation Loss: 0.0119\n",
      "Epoch 121/350, Training Loss: 0.0111, Validation Loss: 0.0119\n",
      "Epoch 122/350, Training Loss: 0.0111, Validation Loss: 0.0119\n",
      "Epoch 123/350, Training Loss: 0.0111, Validation Loss: 0.0119\n",
      "Epoch 124/350, Training Loss: 0.0111, Validation Loss: 0.0118\n",
      "Epoch 125/350, Training Loss: 0.0110, Validation Loss: 0.0118\n",
      "Epoch 126/350, Training Loss: 0.0110, Validation Loss: 0.0118\n",
      "Epoch 127/350, Training Loss: 0.0110, Validation Loss: 0.0118\n",
      "Epoch 128/350, Training Loss: 0.0109, Validation Loss: 0.0118\n",
      "Epoch 129/350, Training Loss: 0.0109, Validation Loss: 0.0117\n",
      "Epoch 130/350, Training Loss: 0.0109, Validation Loss: 0.0117\n",
      "Epoch 131/350, Training Loss: 0.0109, Validation Loss: 0.0117\n",
      "Epoch 132/350, Training Loss: 0.0108, Validation Loss: 0.0117\n",
      "Epoch 133/350, Training Loss: 0.0108, Validation Loss: 0.0116\n",
      "Epoch 134/350, Training Loss: 0.0108, Validation Loss: 0.0116\n",
      "Epoch 135/350, Training Loss: 0.0107, Validation Loss: 0.0116\n",
      "Epoch 136/350, Training Loss: 0.0107, Validation Loss: 0.0116\n",
      "Epoch 137/350, Training Loss: 0.0107, Validation Loss: 0.0116\n",
      "Epoch 138/350, Training Loss: 0.0107, Validation Loss: 0.0115\n",
      "Epoch 139/350, Training Loss: 0.0106, Validation Loss: 0.0115\n",
      "Epoch 140/350, Training Loss: 0.0106, Validation Loss: 0.0115\n",
      "Epoch 141/350, Training Loss: 0.0106, Validation Loss: 0.0115\n",
      "Epoch 142/350, Training Loss: 0.0106, Validation Loss: 0.0115\n",
      "Epoch 143/350, Training Loss: 0.0105, Validation Loss: 0.0114\n",
      "Epoch 144/350, Training Loss: 0.0105, Validation Loss: 0.0114\n",
      "Epoch 145/350, Training Loss: 0.0105, Validation Loss: 0.0114\n",
      "Epoch 146/350, Training Loss: 0.0105, Validation Loss: 0.0114\n",
      "Epoch 147/350, Training Loss: 0.0104, Validation Loss: 0.0113\n",
      "Epoch 148/350, Training Loss: 0.0104, Validation Loss: 0.0113\n",
      "Epoch 149/350, Training Loss: 0.0104, Validation Loss: 0.0113\n",
      "Epoch 150/350, Training Loss: 0.0104, Validation Loss: 0.0113\n",
      "Epoch 151/350, Training Loss: 0.0103, Validation Loss: 0.0113\n",
      "Epoch 152/350, Training Loss: 0.0103, Validation Loss: 0.0112\n",
      "Epoch 153/350, Training Loss: 0.0103, Validation Loss: 0.0112\n",
      "Epoch 154/350, Training Loss: 0.0103, Validation Loss: 0.0112\n",
      "Epoch 155/350, Training Loss: 0.0103, Validation Loss: 0.0112\n",
      "Epoch 156/350, Training Loss: 0.0102, Validation Loss: 0.0112\n",
      "Epoch 157/350, Training Loss: 0.0102, Validation Loss: 0.0111\n",
      "Epoch 158/350, Training Loss: 0.0102, Validation Loss: 0.0111\n",
      "Epoch 159/350, Training Loss: 0.0102, Validation Loss: 0.0111\n",
      "Epoch 160/350, Training Loss: 0.0101, Validation Loss: 0.0111\n",
      "Epoch 161/350, Training Loss: 0.0101, Validation Loss: 0.0110\n",
      "Epoch 162/350, Training Loss: 0.0101, Validation Loss: 0.0110\n",
      "Epoch 163/350, Training Loss: 0.0101, Validation Loss: 0.0110\n",
      "Epoch 164/350, Training Loss: 0.0100, Validation Loss: 0.0110\n",
      "Epoch 165/350, Training Loss: 0.0100, Validation Loss: 0.0110\n",
      "Epoch 166/350, Training Loss: 0.0100, Validation Loss: 0.0109\n",
      "Epoch 167/350, Training Loss: 0.0100, Validation Loss: 0.0109\n",
      "Epoch 168/350, Training Loss: 0.0100, Validation Loss: 0.0109\n",
      "Epoch 169/350, Training Loss: 0.0099, Validation Loss: 0.0109\n",
      "Epoch 170/350, Training Loss: 0.0099, Validation Loss: 0.0108\n",
      "Epoch 171/350, Training Loss: 0.0099, Validation Loss: 0.0108\n",
      "Epoch 172/350, Training Loss: 0.0099, Validation Loss: 0.0108\n",
      "Epoch 173/350, Training Loss: 0.0098, Validation Loss: 0.0108\n",
      "Epoch 174/350, Training Loss: 0.0098, Validation Loss: 0.0107\n",
      "Epoch 175/350, Training Loss: 0.0098, Validation Loss: 0.0107\n",
      "Epoch 176/350, Training Loss: 0.0098, Validation Loss: 0.0107\n",
      "Epoch 177/350, Training Loss: 0.0098, Validation Loss: 0.0107\n",
      "Epoch 178/350, Training Loss: 0.0097, Validation Loss: 0.0107\n",
      "Epoch 179/350, Training Loss: 0.0097, Validation Loss: 0.0106\n",
      "Epoch 180/350, Training Loss: 0.0097, Validation Loss: 0.0106\n",
      "Epoch 181/350, Training Loss: 0.0097, Validation Loss: 0.0106\n",
      "Epoch 182/350, Training Loss: 0.0096, Validation Loss: 0.0106\n",
      "Epoch 183/350, Training Loss: 0.0096, Validation Loss: 0.0105\n",
      "Epoch 184/350, Training Loss: 0.0096, Validation Loss: 0.0105\n",
      "Epoch 185/350, Training Loss: 0.0096, Validation Loss: 0.0105\n",
      "Epoch 186/350, Training Loss: 0.0096, Validation Loss: 0.0104\n",
      "Epoch 187/350, Training Loss: 0.0095, Validation Loss: 0.0104\n",
      "Epoch 188/350, Training Loss: 0.0095, Validation Loss: 0.0104\n",
      "Epoch 189/350, Training Loss: 0.0095, Validation Loss: 0.0104\n",
      "Epoch 190/350, Training Loss: 0.0095, Validation Loss: 0.0103\n",
      "Epoch 191/350, Training Loss: 0.0095, Validation Loss: 0.0103\n",
      "Epoch 192/350, Training Loss: 0.0094, Validation Loss: 0.0103\n",
      "Epoch 193/350, Training Loss: 0.0094, Validation Loss: 0.0103\n",
      "Epoch 194/350, Training Loss: 0.0094, Validation Loss: 0.0102\n",
      "Epoch 195/350, Training Loss: 0.0094, Validation Loss: 0.0102\n",
      "Epoch 196/350, Training Loss: 0.0094, Validation Loss: 0.0102\n",
      "Epoch 197/350, Training Loss: 0.0093, Validation Loss: 0.0102\n",
      "Epoch 198/350, Training Loss: 0.0093, Validation Loss: 0.0102\n",
      "Epoch 199/350, Training Loss: 0.0093, Validation Loss: 0.0101\n",
      "Epoch 200/350, Training Loss: 0.0093, Validation Loss: 0.0101\n",
      "Epoch 201/350, Training Loss: 0.0093, Validation Loss: 0.0101\n",
      "Epoch 202/350, Training Loss: 0.0092, Validation Loss: 0.0101\n",
      "Epoch 203/350, Training Loss: 0.0092, Validation Loss: 0.0100\n",
      "Epoch 204/350, Training Loss: 0.0092, Validation Loss: 0.0100\n",
      "Epoch 205/350, Training Loss: 0.0092, Validation Loss: 0.0100\n",
      "Epoch 206/350, Training Loss: 0.0092, Validation Loss: 0.0100\n",
      "Epoch 207/350, Training Loss: 0.0091, Validation Loss: 0.0100\n",
      "Epoch 208/350, Training Loss: 0.0091, Validation Loss: 0.0099\n",
      "Epoch 209/350, Training Loss: 0.0091, Validation Loss: 0.0099\n",
      "Epoch 210/350, Training Loss: 0.0091, Validation Loss: 0.0099\n",
      "Epoch 211/350, Training Loss: 0.0091, Validation Loss: 0.0099\n",
      "Epoch 212/350, Training Loss: 0.0090, Validation Loss: 0.0098\n",
      "Epoch 213/350, Training Loss: 0.0090, Validation Loss: 0.0098\n",
      "Epoch 214/350, Training Loss: 0.0090, Validation Loss: 0.0098\n",
      "Epoch 215/350, Training Loss: 0.0090, Validation Loss: 0.0098\n",
      "Epoch 216/350, Training Loss: 0.0090, Validation Loss: 0.0098\n",
      "Epoch 217/350, Training Loss: 0.0089, Validation Loss: 0.0098\n",
      "Epoch 218/350, Training Loss: 0.0089, Validation Loss: 0.0097\n",
      "Epoch 219/350, Training Loss: 0.0089, Validation Loss: 0.0097\n",
      "Epoch 220/350, Training Loss: 0.0089, Validation Loss: 0.0097\n",
      "Epoch 221/350, Training Loss: 0.0089, Validation Loss: 0.0097\n",
      "Epoch 222/350, Training Loss: 0.0088, Validation Loss: 0.0097\n",
      "Epoch 223/350, Training Loss: 0.0088, Validation Loss: 0.0096\n",
      "Epoch 224/350, Training Loss: 0.0088, Validation Loss: 0.0096\n",
      "Epoch 225/350, Training Loss: 0.0088, Validation Loss: 0.0096\n",
      "Epoch 226/350, Training Loss: 0.0088, Validation Loss: 0.0096\n",
      "Epoch 227/350, Training Loss: 0.0088, Validation Loss: 0.0096\n",
      "Epoch 228/350, Training Loss: 0.0087, Validation Loss: 0.0096\n",
      "Epoch 229/350, Training Loss: 0.0087, Validation Loss: 0.0095\n",
      "Epoch 230/350, Training Loss: 0.0087, Validation Loss: 0.0095\n",
      "Epoch 231/350, Training Loss: 0.0087, Validation Loss: 0.0095\n",
      "Epoch 232/350, Training Loss: 0.0087, Validation Loss: 0.0095\n",
      "Epoch 233/350, Training Loss: 0.0086, Validation Loss: 0.0095\n",
      "Epoch 234/350, Training Loss: 0.0086, Validation Loss: 0.0094\n",
      "Epoch 235/350, Training Loss: 0.0086, Validation Loss: 0.0094\n",
      "Epoch 236/350, Training Loss: 0.0086, Validation Loss: 0.0094\n",
      "Epoch 237/350, Training Loss: 0.0086, Validation Loss: 0.0094\n",
      "Epoch 238/350, Training Loss: 0.0086, Validation Loss: 0.0094\n",
      "Epoch 239/350, Training Loss: 0.0085, Validation Loss: 0.0094\n",
      "Epoch 240/350, Training Loss: 0.0085, Validation Loss: 0.0093\n",
      "Epoch 241/350, Training Loss: 0.0085, Validation Loss: 0.0093\n",
      "Epoch 242/350, Training Loss: 0.0085, Validation Loss: 0.0093\n",
      "Epoch 243/350, Training Loss: 0.0085, Validation Loss: 0.0093\n",
      "Epoch 244/350, Training Loss: 0.0085, Validation Loss: 0.0093\n",
      "Epoch 245/350, Training Loss: 0.0084, Validation Loss: 0.0093\n",
      "Epoch 246/350, Training Loss: 0.0084, Validation Loss: 0.0093\n",
      "Epoch 247/350, Training Loss: 0.0084, Validation Loss: 0.0092\n",
      "Epoch 248/350, Training Loss: 0.0084, Validation Loss: 0.0092\n",
      "Epoch 249/350, Training Loss: 0.0084, Validation Loss: 0.0092\n",
      "Epoch 250/350, Training Loss: 0.0084, Validation Loss: 0.0092\n",
      "Epoch 251/350, Training Loss: 0.0083, Validation Loss: 0.0092\n",
      "Epoch 252/350, Training Loss: 0.0083, Validation Loss: 0.0092\n",
      "Epoch 253/350, Training Loss: 0.0083, Validation Loss: 0.0092\n",
      "Epoch 254/350, Training Loss: 0.0083, Validation Loss: 0.0092\n",
      "Epoch 255/350, Training Loss: 0.0083, Validation Loss: 0.0091\n",
      "Epoch 256/350, Training Loss: 0.0083, Validation Loss: 0.0091\n",
      "Epoch 257/350, Training Loss: 0.0083, Validation Loss: 0.0091\n",
      "Epoch 258/350, Training Loss: 0.0082, Validation Loss: 0.0091\n",
      "Epoch 259/350, Training Loss: 0.0082, Validation Loss: 0.0091\n",
      "Epoch 260/350, Training Loss: 0.0082, Validation Loss: 0.0091\n",
      "Epoch 261/350, Training Loss: 0.0082, Validation Loss: 0.0091\n",
      "Epoch 262/350, Training Loss: 0.0082, Validation Loss: 0.0091\n",
      "Epoch 263/350, Training Loss: 0.0082, Validation Loss: 0.0091\n",
      "Epoch 264/350, Training Loss: 0.0082, Validation Loss: 0.0090\n",
      "Epoch 265/350, Training Loss: 0.0081, Validation Loss: 0.0090\n",
      "Epoch 266/350, Training Loss: 0.0081, Validation Loss: 0.0090\n",
      "Epoch 267/350, Training Loss: 0.0081, Validation Loss: 0.0090\n",
      "Epoch 268/350, Training Loss: 0.0081, Validation Loss: 0.0090\n",
      "Epoch 269/350, Training Loss: 0.0081, Validation Loss: 0.0090\n",
      "Epoch 270/350, Training Loss: 0.0081, Validation Loss: 0.0090\n",
      "Epoch 271/350, Training Loss: 0.0081, Validation Loss: 0.0090\n",
      "Epoch 272/350, Training Loss: 0.0080, Validation Loss: 0.0090\n",
      "Epoch 273/350, Training Loss: 0.0080, Validation Loss: 0.0090\n",
      "Epoch 274/350, Training Loss: 0.0080, Validation Loss: 0.0089\n",
      "Epoch 275/350, Training Loss: 0.0080, Validation Loss: 0.0089\n",
      "Epoch 276/350, Training Loss: 0.0080, Validation Loss: 0.0089\n",
      "Epoch 277/350, Training Loss: 0.0080, Validation Loss: 0.0089\n",
      "Epoch 278/350, Training Loss: 0.0080, Validation Loss: 0.0089\n",
      "Epoch 279/350, Training Loss: 0.0080, Validation Loss: 0.0089\n",
      "Epoch 280/350, Training Loss: 0.0079, Validation Loss: 0.0089\n",
      "Epoch 281/350, Training Loss: 0.0079, Validation Loss: 0.0089\n",
      "Epoch 282/350, Training Loss: 0.0079, Validation Loss: 0.0089\n",
      "Epoch 283/350, Training Loss: 0.0079, Validation Loss: 0.0089\n",
      "Epoch 284/350, Training Loss: 0.0079, Validation Loss: 0.0089\n",
      "Epoch 285/350, Training Loss: 0.0079, Validation Loss: 0.0088\n",
      "Epoch 286/350, Training Loss: 0.0079, Validation Loss: 0.0088\n",
      "Epoch 287/350, Training Loss: 0.0079, Validation Loss: 0.0088\n",
      "Epoch 288/350, Training Loss: 0.0078, Validation Loss: 0.0088\n",
      "Epoch 289/350, Training Loss: 0.0078, Validation Loss: 0.0088\n",
      "Epoch 290/350, Training Loss: 0.0078, Validation Loss: 0.0088\n",
      "Epoch 291/350, Training Loss: 0.0078, Validation Loss: 0.0088\n",
      "Epoch 292/350, Training Loss: 0.0078, Validation Loss: 0.0088\n",
      "Epoch 293/350, Training Loss: 0.0078, Validation Loss: 0.0088\n",
      "Epoch 294/350, Training Loss: 0.0078, Validation Loss: 0.0088\n",
      "Epoch 295/350, Training Loss: 0.0078, Validation Loss: 0.0087\n",
      "Epoch 296/350, Training Loss: 0.0078, Validation Loss: 0.0087\n",
      "Epoch 297/350, Training Loss: 0.0077, Validation Loss: 0.0087\n",
      "Epoch 298/350, Training Loss: 0.0077, Validation Loss: 0.0087\n",
      "Epoch 299/350, Training Loss: 0.0077, Validation Loss: 0.0087\n",
      "Epoch 300/350, Training Loss: 0.0077, Validation Loss: 0.0087\n",
      "Epoch 301/350, Training Loss: 0.0077, Validation Loss: 0.0087\n",
      "Epoch 302/350, Training Loss: 0.0077, Validation Loss: 0.0087\n",
      "Epoch 303/350, Training Loss: 0.0077, Validation Loss: 0.0087\n",
      "Epoch 304/350, Training Loss: 0.0077, Validation Loss: 0.0087\n",
      "Epoch 305/350, Training Loss: 0.0077, Validation Loss: 0.0086\n",
      "Epoch 306/350, Training Loss: 0.0076, Validation Loss: 0.0086\n",
      "Epoch 307/350, Training Loss: 0.0076, Validation Loss: 0.0086\n",
      "Epoch 308/350, Training Loss: 0.0076, Validation Loss: 0.0086\n",
      "Epoch 309/350, Training Loss: 0.0076, Validation Loss: 0.0086\n",
      "Epoch 310/350, Training Loss: 0.0076, Validation Loss: 0.0086\n",
      "Epoch 311/350, Training Loss: 0.0076, Validation Loss: 0.0086\n",
      "Epoch 312/350, Training Loss: 0.0076, Validation Loss: 0.0086\n",
      "Epoch 313/350, Training Loss: 0.0076, Validation Loss: 0.0086\n",
      "Epoch 314/350, Training Loss: 0.0076, Validation Loss: 0.0085\n",
      "Epoch 315/350, Training Loss: 0.0075, Validation Loss: 0.0085\n",
      "Epoch 316/350, Training Loss: 0.0075, Validation Loss: 0.0085\n",
      "Epoch 317/350, Training Loss: 0.0075, Validation Loss: 0.0085\n",
      "Epoch 318/350, Training Loss: 0.0075, Validation Loss: 0.0085\n",
      "Epoch 319/350, Training Loss: 0.0075, Validation Loss: 0.0085\n",
      "Epoch 320/350, Training Loss: 0.0075, Validation Loss: 0.0085\n",
      "Epoch 321/350, Training Loss: 0.0075, Validation Loss: 0.0085\n",
      "Epoch 322/350, Training Loss: 0.0075, Validation Loss: 0.0085\n",
      "Epoch 323/350, Training Loss: 0.0075, Validation Loss: 0.0084\n",
      "Epoch 324/350, Training Loss: 0.0075, Validation Loss: 0.0084\n",
      "Epoch 325/350, Training Loss: 0.0075, Validation Loss: 0.0084\n",
      "Epoch 326/350, Training Loss: 0.0074, Validation Loss: 0.0084\n",
      "Epoch 327/350, Training Loss: 0.0074, Validation Loss: 0.0084\n",
      "Epoch 328/350, Training Loss: 0.0074, Validation Loss: 0.0084\n",
      "Epoch 329/350, Training Loss: 0.0074, Validation Loss: 0.0084\n",
      "Epoch 330/350, Training Loss: 0.0074, Validation Loss: 0.0084\n",
      "Epoch 331/350, Training Loss: 0.0074, Validation Loss: 0.0083\n",
      "Epoch 332/350, Training Loss: 0.0074, Validation Loss: 0.0083\n",
      "Epoch 333/350, Training Loss: 0.0074, Validation Loss: 0.0083\n",
      "Epoch 334/350, Training Loss: 0.0074, Validation Loss: 0.0083\n",
      "Epoch 335/350, Training Loss: 0.0074, Validation Loss: 0.0083\n",
      "Epoch 336/350, Training Loss: 0.0074, Validation Loss: 0.0083\n",
      "Epoch 337/350, Training Loss: 0.0073, Validation Loss: 0.0083\n",
      "Epoch 338/350, Training Loss: 0.0073, Validation Loss: 0.0083\n",
      "Epoch 339/350, Training Loss: 0.0073, Validation Loss: 0.0083\n",
      "Epoch 340/350, Training Loss: 0.0073, Validation Loss: 0.0083\n",
      "Epoch 341/350, Training Loss: 0.0073, Validation Loss: 0.0082\n",
      "Epoch 342/350, Training Loss: 0.0073, Validation Loss: 0.0082\n",
      "Epoch 343/350, Training Loss: 0.0073, Validation Loss: 0.0082\n",
      "Epoch 344/350, Training Loss: 0.0073, Validation Loss: 0.0082\n",
      "Epoch 345/350, Training Loss: 0.0073, Validation Loss: 0.0082\n",
      "Epoch 346/350, Training Loss: 0.0073, Validation Loss: 0.0082\n",
      "Epoch 347/350, Training Loss: 0.0073, Validation Loss: 0.0082\n",
      "Epoch 348/350, Training Loss: 0.0073, Validation Loss: 0.0082\n",
      "Epoch 349/350, Training Loss: 0.0072, Validation Loss: 0.0082\n",
      "Epoch 350/350, Training Loss: 0.0072, Validation Loss: 0.0081\n",
      "1.2751356686451067\n",
      "Epoch 1/350, Training Loss: 0.2392, Validation Loss: 0.2052\n",
      "Epoch 2/350, Training Loss: 0.1705, Validation Loss: 0.1304\n",
      "Epoch 3/350, Training Loss: 0.0907, Validation Loss: 0.0466\n",
      "Epoch 4/350, Training Loss: 0.0290, Validation Loss: 0.0240\n",
      "Epoch 5/350, Training Loss: 0.0171, Validation Loss: 0.0161\n",
      "Epoch 6/350, Training Loss: 0.0147, Validation Loss: 0.0161\n",
      "Epoch 7/350, Training Loss: 0.0129, Validation Loss: 0.0157\n",
      "Epoch 8/350, Training Loss: 0.0119, Validation Loss: 0.0181\n",
      "Epoch 9/350, Training Loss: 0.0112, Validation Loss: 0.0158\n",
      "Epoch 10/350, Training Loss: 0.0105, Validation Loss: 0.0178\n",
      "Epoch 11/350, Training Loss: 0.0098, Validation Loss: 0.0150\n",
      "Epoch 12/350, Training Loss: 0.0094, Validation Loss: 0.0142\n",
      "Epoch 13/350, Training Loss: 0.0090, Validation Loss: 0.0133\n",
      "Epoch 14/350, Training Loss: 0.0086, Validation Loss: 0.0107\n",
      "Epoch 15/350, Training Loss: 0.0086, Validation Loss: 0.0147\n",
      "Epoch 16/350, Training Loss: 0.0079, Validation Loss: 0.0085\n",
      "Epoch 17/350, Training Loss: 0.0083, Validation Loss: 0.0111\n",
      "Epoch 18/350, Training Loss: 0.0073, Validation Loss: 0.0080\n",
      "Epoch 19/350, Training Loss: 0.0078, Validation Loss: 0.0157\n",
      "Epoch 20/350, Training Loss: 0.0071, Validation Loss: 0.0072\n",
      "Epoch 21/350, Training Loss: 0.0078, Validation Loss: 0.0084\n",
      "Epoch 22/350, Training Loss: 0.0064, Validation Loss: 0.0078\n",
      "Epoch 23/350, Training Loss: 0.0078, Validation Loss: 0.0120\n",
      "Epoch 24/350, Training Loss: 0.0064, Validation Loss: 0.0066\n",
      "Epoch 25/350, Training Loss: 0.0060, Validation Loss: 0.0066\n",
      "Epoch 26/350, Training Loss: 0.0064, Validation Loss: 0.0064\n",
      "Epoch 27/350, Training Loss: 0.0069, Validation Loss: 0.0071\n",
      "Epoch 28/350, Training Loss: 0.0055, Validation Loss: 0.0061\n",
      "Epoch 29/350, Training Loss: 0.0068, Validation Loss: 0.0084\n",
      "Epoch 30/350, Training Loss: 0.0054, Validation Loss: 0.0059\n",
      "Epoch 31/350, Training Loss: 0.0055, Validation Loss: 0.0119\n",
      "Epoch 32/350, Training Loss: 0.0055, Validation Loss: 0.0059\n",
      "Epoch 33/350, Training Loss: 0.0059, Validation Loss: 0.0098\n",
      "Epoch 34/350, Training Loss: 0.0051, Validation Loss: 0.0056\n",
      "Epoch 35/350, Training Loss: 0.0053, Validation Loss: 0.0146\n",
      "Epoch 36/350, Training Loss: 0.0052, Validation Loss: 0.0055\n",
      "Epoch 37/350, Training Loss: 0.0048, Validation Loss: 0.0071\n",
      "Epoch 38/350, Training Loss: 0.0051, Validation Loss: 0.0060\n",
      "Epoch 39/350, Training Loss: 0.0049, Validation Loss: 0.0063\n",
      "Epoch 40/350, Training Loss: 0.0048, Validation Loss: 0.0062\n",
      "Epoch 41/350, Training Loss: 0.0048, Validation Loss: 0.0065\n",
      "Epoch 42/350, Training Loss: 0.0047, Validation Loss: 0.0061\n",
      "Epoch 43/350, Training Loss: 0.0046, Validation Loss: 0.0066\n",
      "Epoch 44/350, Training Loss: 0.0045, Validation Loss: 0.0058\n",
      "Epoch 45/350, Training Loss: 0.0046, Validation Loss: 0.0068\n",
      "Epoch 46/350, Training Loss: 0.0044, Validation Loss: 0.0052\n",
      "Epoch 47/350, Training Loss: 0.0045, Validation Loss: 0.0078\n",
      "Epoch 48/350, Training Loss: 0.0042, Validation Loss: 0.0047\n",
      "Epoch 49/350, Training Loss: 0.0043, Validation Loss: 0.0051\n",
      "Epoch 50/350, Training Loss: 0.0043, Validation Loss: 0.0064\n",
      "Epoch 51/350, Training Loss: 0.0041, Validation Loss: 0.0047\n",
      "Epoch 52/350, Training Loss: 0.0044, Validation Loss: 0.0074\n",
      "Epoch 53/350, Training Loss: 0.0040, Validation Loss: 0.0045\n",
      "Epoch 54/350, Training Loss: 0.0041, Validation Loss: 0.0045\n",
      "Epoch 55/350, Training Loss: 0.0040, Validation Loss: 0.0047\n",
      "Epoch 56/350, Training Loss: 0.0042, Validation Loss: 0.0069\n",
      "Epoch 57/350, Training Loss: 0.0038, Validation Loss: 0.0044\n",
      "Epoch 58/350, Training Loss: 0.0040, Validation Loss: 0.0044\n",
      "Epoch 59/350, Training Loss: 0.0038, Validation Loss: 0.0043\n",
      "Epoch 60/350, Training Loss: 0.0041, Validation Loss: 0.0044\n",
      "Epoch 61/350, Training Loss: 0.0036, Validation Loss: 0.0043\n",
      "Epoch 62/350, Training Loss: 0.0042, Validation Loss: 0.0050\n",
      "Epoch 63/350, Training Loss: 0.0035, Validation Loss: 0.0042\n",
      "Epoch 64/350, Training Loss: 0.0040, Validation Loss: 0.0043\n",
      "Epoch 65/350, Training Loss: 0.0035, Validation Loss: 0.0042\n",
      "Epoch 66/350, Training Loss: 0.0040, Validation Loss: 0.0041\n",
      "Epoch 67/350, Training Loss: 0.0035, Validation Loss: 0.0042\n",
      "Epoch 68/350, Training Loss: 0.0039, Validation Loss: 0.0041\n",
      "Epoch 69/350, Training Loss: 0.0034, Validation Loss: 0.0042\n",
      "Epoch 70/350, Training Loss: 0.0039, Validation Loss: 0.0043\n",
      "Epoch 71/350, Training Loss: 0.0034, Validation Loss: 0.0043\n",
      "Epoch 72/350, Training Loss: 0.0040, Validation Loss: 0.0059\n",
      "Epoch 73/350, Training Loss: 0.0033, Validation Loss: 0.0041\n",
      "Epoch 74/350, Training Loss: 0.0037, Validation Loss: 0.0046\n",
      "Epoch 75/350, Training Loss: 0.0034, Validation Loss: 0.0039\n",
      "Epoch 76/350, Training Loss: 0.0037, Validation Loss: 0.0048\n",
      "Epoch 77/350, Training Loss: 0.0033, Validation Loss: 0.0039\n",
      "Epoch 78/350, Training Loss: 0.0037, Validation Loss: 0.0043\n",
      "Epoch 79/350, Training Loss: 0.0033, Validation Loss: 0.0039\n",
      "Epoch 80/350, Training Loss: 0.0037, Validation Loss: 0.0042\n",
      "Epoch 81/350, Training Loss: 0.0032, Validation Loss: 0.0038\n",
      "Epoch 82/350, Training Loss: 0.0037, Validation Loss: 0.0040\n",
      "Epoch 83/350, Training Loss: 0.0032, Validation Loss: 0.0038\n",
      "Epoch 84/350, Training Loss: 0.0036, Validation Loss: 0.0039\n",
      "Epoch 85/350, Training Loss: 0.0032, Validation Loss: 0.0038\n",
      "Epoch 86/350, Training Loss: 0.0036, Validation Loss: 0.0039\n",
      "Epoch 87/350, Training Loss: 0.0031, Validation Loss: 0.0037\n",
      "Epoch 88/350, Training Loss: 0.0036, Validation Loss: 0.0038\n",
      "Epoch 89/350, Training Loss: 0.0031, Validation Loss: 0.0037\n",
      "Epoch 90/350, Training Loss: 0.0035, Validation Loss: 0.0038\n",
      "Epoch 91/350, Training Loss: 0.0031, Validation Loss: 0.0037\n",
      "Epoch 92/350, Training Loss: 0.0035, Validation Loss: 0.0037\n",
      "Epoch 93/350, Training Loss: 0.0031, Validation Loss: 0.0036\n",
      "Epoch 94/350, Training Loss: 0.0034, Validation Loss: 0.0037\n",
      "Epoch 95/350, Training Loss: 0.0030, Validation Loss: 0.0036\n",
      "Epoch 96/350, Training Loss: 0.0034, Validation Loss: 0.0036\n",
      "Epoch 97/350, Training Loss: 0.0030, Validation Loss: 0.0036\n",
      "Epoch 98/350, Training Loss: 0.0034, Validation Loss: 0.0036\n",
      "Epoch 99/350, Training Loss: 0.0030, Validation Loss: 0.0036\n",
      "Epoch 100/350, Training Loss: 0.0033, Validation Loss: 0.0035\n",
      "Epoch 101/350, Training Loss: 0.0030, Validation Loss: 0.0035\n",
      "Epoch 102/350, Training Loss: 0.0033, Validation Loss: 0.0035\n",
      "Epoch 103/350, Training Loss: 0.0029, Validation Loss: 0.0035\n",
      "Epoch 104/350, Training Loss: 0.0033, Validation Loss: 0.0034\n",
      "Epoch 105/350, Training Loss: 0.0029, Validation Loss: 0.0035\n",
      "Epoch 106/350, Training Loss: 0.0032, Validation Loss: 0.0034\n",
      "Epoch 107/350, Training Loss: 0.0029, Validation Loss: 0.0035\n",
      "Epoch 108/350, Training Loss: 0.0032, Validation Loss: 0.0034\n",
      "Epoch 109/350, Training Loss: 0.0029, Validation Loss: 0.0034\n",
      "Epoch 110/350, Training Loss: 0.0031, Validation Loss: 0.0034\n",
      "Epoch 111/350, Training Loss: 0.0029, Validation Loss: 0.0034\n",
      "Epoch 112/350, Training Loss: 0.0031, Validation Loss: 0.0034\n",
      "Epoch 113/350, Training Loss: 0.0029, Validation Loss: 0.0034\n",
      "Epoch 114/350, Training Loss: 0.0031, Validation Loss: 0.0034\n",
      "Epoch 115/350, Training Loss: 0.0029, Validation Loss: 0.0034\n",
      "Epoch 116/350, Training Loss: 0.0030, Validation Loss: 0.0033\n",
      "Epoch 117/350, Training Loss: 0.0029, Validation Loss: 0.0033\n",
      "Epoch 118/350, Training Loss: 0.0030, Validation Loss: 0.0033\n",
      "Epoch 119/350, Training Loss: 0.0028, Validation Loss: 0.0033\n",
      "Epoch 120/350, Training Loss: 0.0030, Validation Loss: 0.0033\n",
      "Epoch 121/350, Training Loss: 0.0028, Validation Loss: 0.0033\n",
      "Epoch 122/350, Training Loss: 0.0030, Validation Loss: 0.0033\n",
      "Epoch 123/350, Training Loss: 0.0028, Validation Loss: 0.0033\n",
      "Epoch 124/350, Training Loss: 0.0029, Validation Loss: 0.0033\n",
      "Epoch 125/350, Training Loss: 0.0028, Validation Loss: 0.0033\n",
      "Epoch 126/350, Training Loss: 0.0029, Validation Loss: 0.0033\n",
      "Epoch 127/350, Training Loss: 0.0028, Validation Loss: 0.0033\n",
      "Epoch 128/350, Training Loss: 0.0029, Validation Loss: 0.0033\n",
      "Epoch 129/350, Training Loss: 0.0028, Validation Loss: 0.0033\n",
      "Epoch 130/350, Training Loss: 0.0029, Validation Loss: 0.0033\n",
      "Epoch 131/350, Training Loss: 0.0028, Validation Loss: 0.0033\n",
      "Epoch 132/350, Training Loss: 0.0029, Validation Loss: 0.0033\n",
      "Epoch 133/350, Training Loss: 0.0027, Validation Loss: 0.0033\n",
      "Epoch 134/350, Training Loss: 0.0028, Validation Loss: 0.0033\n",
      "Epoch 135/350, Training Loss: 0.0027, Validation Loss: 0.0032\n",
      "Epoch 136/350, Training Loss: 0.0028, Validation Loss: 0.0033\n",
      "Epoch 137/350, Training Loss: 0.0027, Validation Loss: 0.0032\n",
      "Epoch 138/350, Training Loss: 0.0028, Validation Loss: 0.0033\n",
      "Epoch 139/350, Training Loss: 0.0027, Validation Loss: 0.0032\n",
      "Epoch 140/350, Training Loss: 0.0028, Validation Loss: 0.0032\n",
      "Epoch 141/350, Training Loss: 0.0027, Validation Loss: 0.0032\n",
      "Epoch 142/350, Training Loss: 0.0028, Validation Loss: 0.0032\n",
      "Epoch 143/350, Training Loss: 0.0027, Validation Loss: 0.0032\n",
      "Epoch 144/350, Training Loss: 0.0028, Validation Loss: 0.0032\n",
      "Epoch 145/350, Training Loss: 0.0026, Validation Loss: 0.0032\n",
      "Epoch 146/350, Training Loss: 0.0027, Validation Loss: 0.0032\n",
      "Epoch 147/350, Training Loss: 0.0026, Validation Loss: 0.0031\n",
      "Epoch 148/350, Training Loss: 0.0027, Validation Loss: 0.0032\n",
      "Epoch 149/350, Training Loss: 0.0026, Validation Loss: 0.0031\n",
      "Epoch 150/350, Training Loss: 0.0027, Validation Loss: 0.0031\n",
      "Epoch 151/350, Training Loss: 0.0026, Validation Loss: 0.0031\n",
      "Epoch 152/350, Training Loss: 0.0027, Validation Loss: 0.0031\n",
      "Epoch 153/350, Training Loss: 0.0026, Validation Loss: 0.0031\n",
      "Epoch 154/350, Training Loss: 0.0031, Validation Loss: 0.0049\n",
      "Epoch 155/350, Training Loss: 0.0025, Validation Loss: 0.0030\n",
      "Epoch 156/350, Training Loss: 0.0025, Validation Loss: 0.0036\n",
      "Epoch 157/350, Training Loss: 0.0026, Validation Loss: 0.0033\n",
      "Epoch 158/350, Training Loss: 0.0026, Validation Loss: 0.0031\n",
      "Epoch 159/350, Training Loss: 0.0027, Validation Loss: 0.0034\n",
      "Epoch 160/350, Training Loss: 0.0025, Validation Loss: 0.0031\n",
      "Epoch 161/350, Training Loss: 0.0027, Validation Loss: 0.0034\n",
      "Epoch 162/350, Training Loss: 0.0025, Validation Loss: 0.0031\n",
      "Epoch 163/350, Training Loss: 0.0027, Validation Loss: 0.0033\n",
      "Epoch 164/350, Training Loss: 0.0025, Validation Loss: 0.0031\n",
      "Epoch 165/350, Training Loss: 0.0027, Validation Loss: 0.0031\n",
      "Epoch 166/350, Training Loss: 0.0025, Validation Loss: 0.0031\n",
      "Epoch 167/350, Training Loss: 0.0027, Validation Loss: 0.0031\n",
      "Epoch 168/350, Training Loss: 0.0024, Validation Loss: 0.0031\n",
      "Epoch 169/350, Training Loss: 0.0027, Validation Loss: 0.0030\n",
      "Epoch 170/350, Training Loss: 0.0026, Validation Loss: 0.0049\n",
      "Epoch 171/350, Training Loss: 0.0027, Validation Loss: 0.0034\n",
      "Epoch 172/350, Training Loss: 0.0024, Validation Loss: 0.0030\n",
      "Epoch 173/350, Training Loss: 0.0026, Validation Loss: 0.0031\n",
      "Epoch 174/350, Training Loss: 0.0025, Validation Loss: 0.0029\n",
      "Epoch 175/350, Training Loss: 0.0026, Validation Loss: 0.0032\n",
      "Epoch 176/350, Training Loss: 0.0024, Validation Loss: 0.0030\n",
      "Epoch 177/350, Training Loss: 0.0026, Validation Loss: 0.0029\n",
      "Epoch 178/350, Training Loss: 0.0025, Validation Loss: 0.0038\n",
      "Epoch 179/350, Training Loss: 0.0025, Validation Loss: 0.0031\n",
      "Epoch 180/350, Training Loss: 0.0024, Validation Loss: 0.0029\n",
      "Epoch 181/350, Training Loss: 0.0026, Validation Loss: 0.0030\n",
      "Epoch 182/350, Training Loss: 0.0024, Validation Loss: 0.0030\n",
      "Epoch 183/350, Training Loss: 0.0029, Validation Loss: 0.0037\n",
      "Epoch 184/350, Training Loss: 0.0024, Validation Loss: 0.0029\n",
      "Epoch 185/350, Training Loss: 0.0024, Validation Loss: 0.0033\n",
      "Epoch 186/350, Training Loss: 0.0025, Validation Loss: 0.0029\n",
      "Epoch 187/350, Training Loss: 0.0024, Validation Loss: 0.0033\n",
      "Epoch 188/350, Training Loss: 0.0025, Validation Loss: 0.0031\n",
      "Epoch 189/350, Training Loss: 0.0024, Validation Loss: 0.0029\n",
      "Epoch 190/350, Training Loss: 0.0025, Validation Loss: 0.0030\n",
      "Epoch 191/350, Training Loss: 0.0024, Validation Loss: 0.0029\n",
      "Epoch 192/350, Training Loss: 0.0027, Validation Loss: 0.0037\n",
      "Epoch 193/350, Training Loss: 0.0023, Validation Loss: 0.0029\n",
      "Epoch 194/350, Training Loss: 0.0024, Validation Loss: 0.0029\n",
      "Epoch 195/350, Training Loss: 0.0025, Validation Loss: 0.0044\n",
      "Epoch 196/350, Training Loss: 0.0026, Validation Loss: 0.0028\n",
      "Epoch 197/350, Training Loss: 0.0023, Validation Loss: 0.0029\n",
      "Epoch 198/350, Training Loss: 0.0024, Validation Loss: 0.0028\n",
      "Epoch 199/350, Training Loss: 0.0025, Validation Loss: 0.0029\n",
      "Epoch 200/350, Training Loss: 0.0025, Validation Loss: 0.0057\n",
      "Epoch 201/350, Training Loss: 0.0024, Validation Loss: 0.0028\n",
      "Epoch 202/350, Training Loss: 0.0024, Validation Loss: 0.0033\n",
      "Epoch 203/350, Training Loss: 0.0025, Validation Loss: 0.0031\n",
      "Epoch 204/350, Training Loss: 0.0023, Validation Loss: 0.0028\n",
      "Epoch 205/350, Training Loss: 0.0025, Validation Loss: 0.0031\n",
      "Epoch 206/350, Training Loss: 0.0023, Validation Loss: 0.0029\n",
      "Epoch 207/350, Training Loss: 0.0027, Validation Loss: 0.0057\n",
      "Epoch 208/350, Training Loss: 0.0023, Validation Loss: 0.0027\n",
      "Epoch 209/350, Training Loss: 0.0022, Validation Loss: 0.0029\n",
      "Epoch 210/350, Training Loss: 0.0025, Validation Loss: 0.0029\n",
      "Epoch 211/350, Training Loss: 0.0023, Validation Loss: 0.0030\n",
      "Epoch 212/350, Training Loss: 0.0024, Validation Loss: 0.0028\n",
      "Epoch 213/350, Training Loss: 0.0024, Validation Loss: 0.0027\n",
      "Epoch 214/350, Training Loss: 0.0025, Validation Loss: 0.0042\n",
      "Epoch 215/350, Training Loss: 0.0023, Validation Loss: 0.0029\n",
      "Epoch 216/350, Training Loss: 0.0023, Validation Loss: 0.0029\n",
      "Epoch 217/350, Training Loss: 0.0025, Validation Loss: 0.0030\n",
      "Epoch 218/350, Training Loss: 0.0022, Validation Loss: 0.0028\n",
      "Epoch 219/350, Training Loss: 0.0025, Validation Loss: 0.0032\n",
      "Epoch 220/350, Training Loss: 0.0023, Validation Loss: 0.0028\n",
      "Epoch 221/350, Training Loss: 0.0024, Validation Loss: 0.0028\n",
      "Epoch 222/350, Training Loss: 0.0024, Validation Loss: 0.0030\n",
      "Epoch 223/350, Training Loss: 0.0023, Validation Loss: 0.0028\n",
      "Epoch 224/350, Training Loss: 0.0023, Validation Loss: 0.0027\n",
      "Epoch 225/350, Training Loss: 0.0026, Validation Loss: 0.0056\n",
      "Epoch 226/350, Training Loss: 0.0023, Validation Loss: 0.0027\n",
      "Epoch 227/350, Training Loss: 0.0022, Validation Loss: 0.0028\n",
      "Epoch 228/350, Training Loss: 0.0024, Validation Loss: 0.0027\n",
      "Epoch 229/350, Training Loss: 0.0024, Validation Loss: 0.0031\n",
      "Epoch 230/350, Training Loss: 0.0023, Validation Loss: 0.0028\n",
      "Epoch 231/350, Training Loss: 0.0023, Validation Loss: 0.0028\n",
      "Epoch 232/350, Training Loss: 0.0024, Validation Loss: 0.0029\n",
      "Epoch 233/350, Training Loss: 0.0023, Validation Loss: 0.0028\n",
      "Epoch 234/350, Training Loss: 0.0024, Validation Loss: 0.0031\n",
      "Epoch 235/350, Training Loss: 0.0022, Validation Loss: 0.0028\n",
      "Epoch 236/350, Training Loss: 0.0023, Validation Loss: 0.0027\n",
      "Epoch 237/350, Training Loss: 0.0023, Validation Loss: 0.0031\n",
      "Epoch 238/350, Training Loss: 0.0025, Validation Loss: 0.0026\n",
      "Epoch 239/350, Training Loss: 0.0022, Validation Loss: 0.0028\n",
      "Epoch 240/350, Training Loss: 0.0023, Validation Loss: 0.0028\n",
      "Epoch 241/350, Training Loss: 0.0024, Validation Loss: 0.0030\n",
      "Epoch 242/350, Training Loss: 0.0022, Validation Loss: 0.0027\n",
      "Epoch 243/350, Training Loss: 0.0024, Validation Loss: 0.0028\n",
      "Epoch 244/350, Training Loss: 0.0022, Validation Loss: 0.0028\n",
      "Epoch 245/350, Training Loss: 0.0024, Validation Loss: 0.0029\n",
      "Epoch 246/350, Training Loss: 0.0022, Validation Loss: 0.0027\n",
      "Epoch 247/350, Training Loss: 0.0023, Validation Loss: 0.0027\n",
      "Epoch 248/350, Training Loss: 0.0022, Validation Loss: 0.0028\n",
      "Epoch 249/350, Training Loss: 0.0023, Validation Loss: 0.0028\n",
      "Epoch 250/350, Training Loss: 0.0022, Validation Loss: 0.0027\n",
      "Epoch 251/350, Training Loss: 0.0023, Validation Loss: 0.0026\n",
      "Epoch 252/350, Training Loss: 0.0025, Validation Loss: 0.0055\n",
      "Epoch 253/350, Training Loss: 0.0023, Validation Loss: 0.0026\n",
      "Epoch 254/350, Training Loss: 0.0022, Validation Loss: 0.0027\n",
      "Epoch 255/350, Training Loss: 0.0023, Validation Loss: 0.0026\n",
      "Epoch 256/350, Training Loss: 0.0023, Validation Loss: 0.0034\n",
      "Epoch 257/350, Training Loss: 0.0023, Validation Loss: 0.0028\n",
      "Epoch 258/350, Training Loss: 0.0022, Validation Loss: 0.0027\n",
      "Epoch 259/350, Training Loss: 0.0023, Validation Loss: 0.0027\n",
      "Epoch 260/350, Training Loss: 0.0022, Validation Loss: 0.0027\n",
      "Epoch 261/350, Training Loss: 0.0024, Validation Loss: 0.0029\n",
      "Epoch 262/350, Training Loss: 0.0022, Validation Loss: 0.0027\n",
      "Epoch 263/350, Training Loss: 0.0023, Validation Loss: 0.0026\n",
      "Epoch 264/350, Training Loss: 0.0023, Validation Loss: 0.0031\n",
      "Epoch 265/350, Training Loss: 0.0024, Validation Loss: 0.0026\n",
      "Epoch 266/350, Training Loss: 0.0022, Validation Loss: 0.0027\n",
      "Epoch 267/350, Training Loss: 0.0022, Validation Loss: 0.0027\n",
      "Epoch 268/350, Training Loss: 0.0025, Validation Loss: 0.0048\n",
      "Epoch 269/350, Training Loss: 0.0022, Validation Loss: 0.0026\n",
      "Epoch 270/350, Training Loss: 0.0021, Validation Loss: 0.0027\n",
      "Epoch 271/350, Training Loss: 0.0023, Validation Loss: 0.0028\n",
      "Epoch 272/350, Training Loss: 0.0022, Validation Loss: 0.0027\n",
      "Epoch 273/350, Training Loss: 0.0022, Validation Loss: 0.0026\n",
      "Epoch 274/350, Training Loss: 0.0023, Validation Loss: 0.0026\n",
      "Epoch 275/350, Training Loss: 0.0023, Validation Loss: 0.0030\n",
      "Epoch 276/350, Training Loss: 0.0024, Validation Loss: 0.0026\n",
      "Epoch 277/350, Training Loss: 0.0021, Validation Loss: 0.0027\n",
      "Epoch 278/350, Training Loss: 0.0022, Validation Loss: 0.0027\n",
      "Epoch 279/350, Training Loss: 0.0023, Validation Loss: 0.0026\n",
      "Epoch 280/350, Training Loss: 0.0021, Validation Loss: 0.0028\n",
      "Epoch 281/350, Training Loss: 0.0023, Validation Loss: 0.0026\n",
      "Epoch 282/350, Training Loss: 0.0022, Validation Loss: 0.0027\n",
      "Epoch 283/350, Training Loss: 0.0023, Validation Loss: 0.0028\n",
      "Epoch 284/350, Training Loss: 0.0021, Validation Loss: 0.0026\n",
      "Epoch 285/350, Training Loss: 0.0022, Validation Loss: 0.0026\n",
      "Epoch 286/350, Training Loss: 0.0022, Validation Loss: 0.0028\n",
      "Epoch 287/350, Training Loss: 0.0024, Validation Loss: 0.0026\n",
      "Epoch 288/350, Training Loss: 0.0021, Validation Loss: 0.0028\n",
      "Epoch 289/350, Training Loss: 0.0022, Validation Loss: 0.0026\n",
      "Epoch 290/350, Training Loss: 0.0023, Validation Loss: 0.0027\n",
      "Epoch 291/350, Training Loss: 0.0021, Validation Loss: 0.0027\n",
      "Epoch 292/350, Training Loss: 0.0023, Validation Loss: 0.0026\n",
      "Epoch 293/350, Training Loss: 0.0021, Validation Loss: 0.0027\n",
      "Epoch 294/350, Training Loss: 0.0023, Validation Loss: 0.0026\n",
      "Epoch 295/350, Training Loss: 0.0021, Validation Loss: 0.0028\n",
      "Epoch 296/350, Training Loss: 0.0023, Validation Loss: 0.0027\n",
      "Epoch 297/350, Training Loss: 0.0021, Validation Loss: 0.0026\n",
      "Epoch 298/350, Training Loss: 0.0022, Validation Loss: 0.0026\n",
      "Epoch 299/350, Training Loss: 0.0022, Validation Loss: 0.0027\n",
      "Epoch 300/350, Training Loss: 0.0023, Validation Loss: 0.0028\n",
      "Epoch 301/350, Training Loss: 0.0022, Validation Loss: 0.0027\n",
      "Epoch 302/350, Training Loss: 0.0022, Validation Loss: 0.0026\n",
      "Epoch 303/350, Training Loss: 0.0022, Validation Loss: 0.0026\n",
      "Epoch 304/350, Training Loss: 0.0021, Validation Loss: 0.0027\n",
      "Epoch 305/350, Training Loss: 0.0023, Validation Loss: 0.0026\n",
      "Epoch 306/350, Training Loss: 0.0021, Validation Loss: 0.0026\n",
      "Epoch 307/350, Training Loss: 0.0022, Validation Loss: 0.0027\n",
      "Epoch 308/350, Training Loss: 0.0021, Validation Loss: 0.0026\n",
      "Epoch 309/350, Training Loss: 0.0022, Validation Loss: 0.0026\n",
      "Epoch 310/350, Training Loss: 0.0021, Validation Loss: 0.0027\n",
      "Epoch 311/350, Training Loss: 0.0025, Validation Loss: 0.0051\n",
      "Epoch 312/350, Training Loss: 0.0022, Validation Loss: 0.0025\n",
      "Epoch 313/350, Training Loss: 0.0021, Validation Loss: 0.0027\n",
      "Epoch 314/350, Training Loss: 0.0021, Validation Loss: 0.0026\n",
      "Epoch 315/350, Training Loss: 0.0022, Validation Loss: 0.0026\n",
      "Epoch 316/350, Training Loss: 0.0021, Validation Loss: 0.0028\n",
      "Epoch 317/350, Training Loss: 0.0023, Validation Loss: 0.0030\n",
      "Epoch 318/350, Training Loss: 0.0022, Validation Loss: 0.0028\n",
      "Epoch 319/350, Training Loss: 0.0022, Validation Loss: 0.0026\n",
      "Epoch 320/350, Training Loss: 0.0021, Validation Loss: 0.0026\n",
      "Epoch 321/350, Training Loss: 0.0022, Validation Loss: 0.0029\n",
      "Epoch 322/350, Training Loss: 0.0022, Validation Loss: 0.0027\n",
      "Epoch 323/350, Training Loss: 0.0022, Validation Loss: 0.0027\n",
      "Epoch 324/350, Training Loss: 0.0022, Validation Loss: 0.0026\n",
      "Epoch 325/350, Training Loss: 0.0021, Validation Loss: 0.0026\n",
      "Epoch 326/350, Training Loss: 0.0022, Validation Loss: 0.0026\n",
      "Epoch 327/350, Training Loss: 0.0021, Validation Loss: 0.0027\n",
      "Epoch 328/350, Training Loss: 0.0023, Validation Loss: 0.0028\n",
      "Epoch 329/350, Training Loss: 0.0021, Validation Loss: 0.0026\n",
      "Epoch 330/350, Training Loss: 0.0021, Validation Loss: 0.0026\n",
      "Epoch 331/350, Training Loss: 0.0021, Validation Loss: 0.0026\n",
      "Epoch 332/350, Training Loss: 0.0022, Validation Loss: 0.0027\n",
      "Epoch 333/350, Training Loss: 0.0022, Validation Loss: 0.0028\n",
      "Epoch 334/350, Training Loss: 0.0022, Validation Loss: 0.0028\n",
      "Epoch 335/350, Training Loss: 0.0022, Validation Loss: 0.0027\n",
      "Epoch 336/350, Training Loss: 0.0022, Validation Loss: 0.0027\n",
      "Epoch 337/350, Training Loss: 0.0021, Validation Loss: 0.0026\n",
      "Epoch 338/350, Training Loss: 0.0021, Validation Loss: 0.0026\n",
      "Epoch 339/350, Training Loss: 0.0022, Validation Loss: 0.0026\n",
      "Epoch 340/350, Training Loss: 0.0021, Validation Loss: 0.0027\n",
      "Epoch 341/350, Training Loss: 0.0022, Validation Loss: 0.0026\n",
      "Epoch 342/350, Training Loss: 0.0021, Validation Loss: 0.0026\n",
      "Epoch 343/350, Training Loss: 0.0022, Validation Loss: 0.0027\n",
      "Epoch 344/350, Training Loss: 0.0021, Validation Loss: 0.0027\n",
      "Epoch 345/350, Training Loss: 0.0022, Validation Loss: 0.0028\n",
      "Epoch 346/350, Training Loss: 0.0022, Validation Loss: 0.0028\n",
      "Epoch 347/350, Training Loss: 0.0022, Validation Loss: 0.0029\n",
      "Epoch 348/350, Training Loss: 0.0022, Validation Loss: 0.0028\n",
      "Epoch 349/350, Training Loss: 0.0021, Validation Loss: 0.0028\n",
      "Epoch 350/350, Training Loss: 0.0021, Validation Loss: 0.0027\n",
      "1.1420189718523672\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "\n",
    "_window_size = 7\n",
    "horizon = 1\n",
    "_stride = 1\n",
    "\n",
    "# TGCN\n",
    "num_epochs_TGCN = 350\n",
    "\n",
    "n_neighbors = 5\n",
    "df_PeMS_old, df_distance  = load_PeMS04_flow_data()\n",
    "df_PeMS, adjacency_matrix_PeMS, meanstd_dict = preprocess_PeMS_data(df_PeMS_old, df_distance, init_node=0, n_neighbors=n_neighbors, center_and_reduce=True)\n",
    "print(df_PeMS.columns)\n",
    "node_0, _ = mask_unknown_nodes(df_PeMS=df_PeMS, list_nodes_known=[243, 117, 28, 0, 173, 92])\n",
    "node_1, mask_node_1 = mask_unknown_nodes(df_PeMS=df_PeMS, list_nodes_known=[243, 117, 0])\n",
    "\n",
    "# TGCN Model\n",
    "model_TGCN = TGCN(adjacency_matrix_PeMS, hidden_dim=32, output_size=len(df_PeMS.columns))\n",
    "train_loader_TGCN, val_loader_TGCN, test_loader_TGCN, _ = createLoaders(node_0, window_size=_window_size, stride=_stride, prediction_horizon=horizon)\n",
    "model_path = \"./test5_5.pkl\"\n",
    "best_model , train_losses, valid_losses = train_model(model_TGCN, train_loader_TGCN, val_loader_TGCN, model_path=model_path, mask=None, num_epochs=num_epochs_TGCN, remove=False)\n",
    "y_true, y_pred = testmodel(best_model=best_model, test_loader=test_loader_TGCN, meanstd_dict=meanstd_dict, sensor_order_list=node_0.columns)\n",
    "rmse = 0\n",
    "for i in range(n_neighbors+1):\n",
    "    rmse = rmse + np.sqrt(mean_squared_error(y_true[:,:,i], y_pred[:,:,i]))\n",
    "print(rmse/n_neighbors+1)\n",
    "\n",
    "model_TGCN = TGCN(adjacency_matrix_PeMS, hidden_dim=32, output_size=len(df_PeMS.columns))\n",
    "train_loader_TGCN, val_loader_TGCN, test_loader_TGCN, _ = createLoaders(node_1, window_size=_window_size, stride=_stride, prediction_horizon=horizon)\n",
    "model_path = \"./test3_5.pkl\"\n",
    "best_model , train_losses, valid_losses = train_model(model_TGCN, train_loader_TGCN, val_loader_TGCN, model_path=model_path, mask=None, num_epochs=num_epochs_TGCN, remove=False)\n",
    "y_true, y_pred = testmodel(best_model=best_model, test_loader=test_loader_TGCN, meanstd_dict=meanstd_dict, sensor_order_list=node_1.columns)\n",
    "rmse = 0\n",
    "for i in range(n_neighbors+1):\n",
    "    rmse = rmse + np.sqrt(mean_squared_error(y_true[:,:,i], y_pred[:,:,i]))\n",
    "print(rmse/n_neighbors+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gcogoni\\Documents\\Stage\\Federated-Traffic-Flow-Forecasting\\src\\utils_graph.py:98: FutureWarning: adjacency_matrix will return a scipy.sparse array instead of a matrix in Networkx 3.0.\n",
      "  return nx.adjacency_matrix(graph, nodelist=nodes_order, weight=None).toarray()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Int64Index([68, 62, 242, 243, 117, 28, 170, 0, 173, 271, 92, 57, 97], dtype='int64')\n",
      "Epoch 1/350, Training Loss: 0.1854, Validation Loss: 0.0955\n",
      "Epoch 2/350, Training Loss: 0.1014, Validation Loss: 0.0828\n",
      "Epoch 3/350, Training Loss: 0.0874, Validation Loss: 0.0708\n",
      "Epoch 4/350, Training Loss: 0.0790, Validation Loss: 0.0662\n",
      "Epoch 5/350, Training Loss: 0.0746, Validation Loss: 0.0625\n",
      "Epoch 6/350, Training Loss: 0.0703, Validation Loss: 0.0581\n",
      "Epoch 7/350, Training Loss: 0.0629, Validation Loss: 0.0507\n",
      "Epoch 8/350, Training Loss: 0.0530, Validation Loss: 0.0428\n",
      "Epoch 9/350, Training Loss: 0.0451, Validation Loss: 0.0382\n",
      "Epoch 10/350, Training Loss: 0.0413, Validation Loss: 0.0360\n",
      "Epoch 11/350, Training Loss: 0.0391, Validation Loss: 0.0338\n",
      "Epoch 12/350, Training Loss: 0.0365, Validation Loss: 0.0348\n",
      "Epoch 13/350, Training Loss: 0.0348, Validation Loss: 0.0316\n",
      "Epoch 14/350, Training Loss: 0.0334, Validation Loss: 0.0300\n",
      "Epoch 15/350, Training Loss: 0.0323, Validation Loss: 0.0290\n",
      "Epoch 16/350, Training Loss: 0.0313, Validation Loss: 0.0283\n",
      "Epoch 17/350, Training Loss: 0.0304, Validation Loss: 0.0275\n",
      "Epoch 18/350, Training Loss: 0.0297, Validation Loss: 0.0269\n",
      "Epoch 19/350, Training Loss: 0.0290, Validation Loss: 0.0262\n",
      "Epoch 20/350, Training Loss: 0.0283, Validation Loss: 0.0256\n",
      "Epoch 21/350, Training Loss: 0.0277, Validation Loss: 0.0250\n",
      "Epoch 22/350, Training Loss: 0.0271, Validation Loss: 0.0245\n",
      "Epoch 23/350, Training Loss: 0.0266, Validation Loss: 0.0240\n",
      "Epoch 24/350, Training Loss: 0.0261, Validation Loss: 0.0236\n",
      "Epoch 25/350, Training Loss: 0.0256, Validation Loss: 0.0231\n",
      "Epoch 26/350, Training Loss: 0.0251, Validation Loss: 0.0228\n",
      "Epoch 27/350, Training Loss: 0.0247, Validation Loss: 0.0224\n",
      "Epoch 28/350, Training Loss: 0.0243, Validation Loss: 0.0221\n",
      "Epoch 29/350, Training Loss: 0.0239, Validation Loss: 0.0217\n",
      "Epoch 30/350, Training Loss: 0.0236, Validation Loss: 0.0214\n",
      "Epoch 31/350, Training Loss: 0.0233, Validation Loss: 0.0211\n",
      "Epoch 32/350, Training Loss: 0.0230, Validation Loss: 0.0209\n",
      "Epoch 33/350, Training Loss: 0.0227, Validation Loss: 0.0206\n",
      "Epoch 34/350, Training Loss: 0.0224, Validation Loss: 0.0204\n",
      "Epoch 35/350, Training Loss: 0.0222, Validation Loss: 0.0203\n",
      "Epoch 36/350, Training Loss: 0.0219, Validation Loss: 0.0201\n",
      "Epoch 37/350, Training Loss: 0.0217, Validation Loss: 0.0200\n",
      "Epoch 38/350, Training Loss: 0.0215, Validation Loss: 0.0198\n",
      "Epoch 39/350, Training Loss: 0.0213, Validation Loss: 0.0197\n",
      "Epoch 40/350, Training Loss: 0.0211, Validation Loss: 0.0196\n",
      "Epoch 41/350, Training Loss: 0.0209, Validation Loss: 0.0196\n",
      "Epoch 42/350, Training Loss: 0.0207, Validation Loss: 0.0195\n",
      "Epoch 43/350, Training Loss: 0.0205, Validation Loss: 0.0194\n",
      "Epoch 44/350, Training Loss: 0.0204, Validation Loss: 0.0193\n",
      "Epoch 45/350, Training Loss: 0.0202, Validation Loss: 0.0192\n",
      "Epoch 46/350, Training Loss: 0.0201, Validation Loss: 0.0192\n",
      "Epoch 47/350, Training Loss: 0.0199, Validation Loss: 0.0191\n",
      "Epoch 48/350, Training Loss: 0.0198, Validation Loss: 0.0190\n",
      "Epoch 49/350, Training Loss: 0.0196, Validation Loss: 0.0189\n",
      "Epoch 50/350, Training Loss: 0.0195, Validation Loss: 0.0188\n",
      "Epoch 51/350, Training Loss: 0.0194, Validation Loss: 0.0187\n",
      "Epoch 52/350, Training Loss: 0.0192, Validation Loss: 0.0186\n",
      "Epoch 53/350, Training Loss: 0.0191, Validation Loss: 0.0184\n",
      "Epoch 54/350, Training Loss: 0.0190, Validation Loss: 0.0183\n",
      "Epoch 55/350, Training Loss: 0.0189, Validation Loss: 0.0182\n",
      "Epoch 56/350, Training Loss: 0.0188, Validation Loss: 0.0181\n",
      "Epoch 57/350, Training Loss: 0.0186, Validation Loss: 0.0180\n",
      "Epoch 58/350, Training Loss: 0.0185, Validation Loss: 0.0179\n",
      "Epoch 59/350, Training Loss: 0.0184, Validation Loss: 0.0178\n",
      "Epoch 60/350, Training Loss: 0.0183, Validation Loss: 0.0177\n",
      "Epoch 61/350, Training Loss: 0.0182, Validation Loss: 0.0176\n",
      "Epoch 62/350, Training Loss: 0.0181, Validation Loss: 0.0175\n",
      "Epoch 63/350, Training Loss: 0.0181, Validation Loss: 0.0175\n",
      "Epoch 64/350, Training Loss: 0.0180, Validation Loss: 0.0173\n",
      "Epoch 65/350, Training Loss: 0.0179, Validation Loss: 0.0172\n",
      "Epoch 66/350, Training Loss: 0.0177, Validation Loss: 0.0170\n",
      "Epoch 67/350, Training Loss: 0.0178, Validation Loss: 0.0169\n",
      "Epoch 68/350, Training Loss: 0.0175, Validation Loss: 0.0168\n",
      "Epoch 69/350, Training Loss: 0.0176, Validation Loss: 0.0168\n",
      "Epoch 70/350, Training Loss: 0.0174, Validation Loss: 0.0167\n",
      "Epoch 71/350, Training Loss: 0.0174, Validation Loss: 0.0167\n",
      "Epoch 72/350, Training Loss: 0.0173, Validation Loss: 0.0167\n",
      "Epoch 73/350, Training Loss: 0.0172, Validation Loss: 0.0166\n",
      "Epoch 74/350, Training Loss: 0.0172, Validation Loss: 0.0166\n",
      "Epoch 75/350, Training Loss: 0.0171, Validation Loss: 0.0165\n",
      "Epoch 76/350, Training Loss: 0.0170, Validation Loss: 0.0165\n",
      "Epoch 77/350, Training Loss: 0.0170, Validation Loss: 0.0165\n",
      "Epoch 78/350, Training Loss: 0.0169, Validation Loss: 0.0164\n",
      "Epoch 79/350, Training Loss: 0.0168, Validation Loss: 0.0164\n",
      "Epoch 80/350, Training Loss: 0.0168, Validation Loss: 0.0164\n",
      "Epoch 81/350, Training Loss: 0.0167, Validation Loss: 0.0163\n",
      "Epoch 82/350, Training Loss: 0.0167, Validation Loss: 0.0163\n",
      "Epoch 83/350, Training Loss: 0.0166, Validation Loss: 0.0163\n",
      "Epoch 84/350, Training Loss: 0.0165, Validation Loss: 0.0163\n",
      "Epoch 85/350, Training Loss: 0.0165, Validation Loss: 0.0162\n",
      "Epoch 86/350, Training Loss: 0.0164, Validation Loss: 0.0162\n",
      "Epoch 87/350, Training Loss: 0.0164, Validation Loss: 0.0162\n",
      "Epoch 88/350, Training Loss: 0.0163, Validation Loss: 0.0161\n",
      "Epoch 89/350, Training Loss: 0.0163, Validation Loss: 0.0161\n",
      "Epoch 90/350, Training Loss: 0.0162, Validation Loss: 0.0161\n",
      "Epoch 91/350, Training Loss: 0.0162, Validation Loss: 0.0161\n",
      "Epoch 92/350, Training Loss: 0.0161, Validation Loss: 0.0160\n",
      "Epoch 93/350, Training Loss: 0.0160, Validation Loss: 0.0160\n",
      "Epoch 94/350, Training Loss: 0.0160, Validation Loss: 0.0160\n",
      "Epoch 95/350, Training Loss: 0.0159, Validation Loss: 0.0160\n",
      "Epoch 96/350, Training Loss: 0.0159, Validation Loss: 0.0159\n",
      "Epoch 97/350, Training Loss: 0.0158, Validation Loss: 0.0159\n",
      "Epoch 98/350, Training Loss: 0.0158, Validation Loss: 0.0159\n",
      "Epoch 99/350, Training Loss: 0.0157, Validation Loss: 0.0159\n",
      "Epoch 100/350, Training Loss: 0.0157, Validation Loss: 0.0158\n",
      "Epoch 101/350, Training Loss: 0.0156, Validation Loss: 0.0158\n",
      "Epoch 102/350, Training Loss: 0.0156, Validation Loss: 0.0158\n",
      "Epoch 103/350, Training Loss: 0.0155, Validation Loss: 0.0157\n",
      "Epoch 104/350, Training Loss: 0.0155, Validation Loss: 0.0157\n",
      "Epoch 105/350, Training Loss: 0.0155, Validation Loss: 0.0157\n",
      "Epoch 106/350, Training Loss: 0.0154, Validation Loss: 0.0157\n",
      "Epoch 107/350, Training Loss: 0.0154, Validation Loss: 0.0156\n",
      "Epoch 108/350, Training Loss: 0.0153, Validation Loss: 0.0156\n",
      "Epoch 109/350, Training Loss: 0.0153, Validation Loss: 0.0156\n",
      "Epoch 110/350, Training Loss: 0.0152, Validation Loss: 0.0156\n",
      "Epoch 111/350, Training Loss: 0.0152, Validation Loss: 0.0155\n",
      "Epoch 112/350, Training Loss: 0.0151, Validation Loss: 0.0155\n",
      "Epoch 113/350, Training Loss: 0.0151, Validation Loss: 0.0155\n",
      "Epoch 114/350, Training Loss: 0.0150, Validation Loss: 0.0154\n",
      "Epoch 115/350, Training Loss: 0.0150, Validation Loss: 0.0154\n",
      "Epoch 116/350, Training Loss: 0.0150, Validation Loss: 0.0154\n",
      "Epoch 117/350, Training Loss: 0.0149, Validation Loss: 0.0154\n",
      "Epoch 118/350, Training Loss: 0.0149, Validation Loss: 0.0153\n",
      "Epoch 119/350, Training Loss: 0.0148, Validation Loss: 0.0153\n",
      "Epoch 120/350, Training Loss: 0.0148, Validation Loss: 0.0153\n",
      "Epoch 121/350, Training Loss: 0.0148, Validation Loss: 0.0153\n",
      "Epoch 122/350, Training Loss: 0.0147, Validation Loss: 0.0152\n",
      "Epoch 123/350, Training Loss: 0.0147, Validation Loss: 0.0152\n",
      "Epoch 124/350, Training Loss: 0.0146, Validation Loss: 0.0152\n",
      "Epoch 125/350, Training Loss: 0.0146, Validation Loss: 0.0152\n",
      "Epoch 126/350, Training Loss: 0.0146, Validation Loss: 0.0152\n",
      "Epoch 127/350, Training Loss: 0.0145, Validation Loss: 0.0151\n",
      "Epoch 128/350, Training Loss: 0.0145, Validation Loss: 0.0151\n",
      "Epoch 129/350, Training Loss: 0.0144, Validation Loss: 0.0151\n",
      "Epoch 130/350, Training Loss: 0.0144, Validation Loss: 0.0151\n",
      "Epoch 131/350, Training Loss: 0.0144, Validation Loss: 0.0151\n",
      "Epoch 132/350, Training Loss: 0.0143, Validation Loss: 0.0150\n",
      "Epoch 133/350, Training Loss: 0.0143, Validation Loss: 0.0150\n",
      "Epoch 134/350, Training Loss: 0.0143, Validation Loss: 0.0150\n",
      "Epoch 135/350, Training Loss: 0.0142, Validation Loss: 0.0150\n",
      "Epoch 136/350, Training Loss: 0.0142, Validation Loss: 0.0150\n",
      "Epoch 137/350, Training Loss: 0.0142, Validation Loss: 0.0149\n",
      "Epoch 138/350, Training Loss: 0.0141, Validation Loss: 0.0149\n",
      "Epoch 139/350, Training Loss: 0.0141, Validation Loss: 0.0149\n",
      "Epoch 140/350, Training Loss: 0.0141, Validation Loss: 0.0149\n",
      "Epoch 141/350, Training Loss: 0.0140, Validation Loss: 0.0149\n",
      "Epoch 142/350, Training Loss: 0.0140, Validation Loss: 0.0149\n",
      "Epoch 143/350, Training Loss: 0.0140, Validation Loss: 0.0148\n",
      "Epoch 144/350, Training Loss: 0.0139, Validation Loss: 0.0148\n",
      "Epoch 145/350, Training Loss: 0.0139, Validation Loss: 0.0148\n",
      "Epoch 146/350, Training Loss: 0.0139, Validation Loss: 0.0148\n",
      "Epoch 147/350, Training Loss: 0.0138, Validation Loss: 0.0148\n",
      "Epoch 148/350, Training Loss: 0.0138, Validation Loss: 0.0148\n",
      "Epoch 149/350, Training Loss: 0.0138, Validation Loss: 0.0147\n",
      "Epoch 150/350, Training Loss: 0.0138, Validation Loss: 0.0147\n",
      "Epoch 151/350, Training Loss: 0.0137, Validation Loss: 0.0147\n",
      "Epoch 152/350, Training Loss: 0.0137, Validation Loss: 0.0147\n",
      "Epoch 153/350, Training Loss: 0.0137, Validation Loss: 0.0147\n",
      "Epoch 154/350, Training Loss: 0.0136, Validation Loss: 0.0147\n",
      "Epoch 155/350, Training Loss: 0.0136, Validation Loss: 0.0146\n",
      "Epoch 156/350, Training Loss: 0.0136, Validation Loss: 0.0146\n",
      "Epoch 157/350, Training Loss: 0.0136, Validation Loss: 0.0146\n",
      "Epoch 158/350, Training Loss: 0.0135, Validation Loss: 0.0146\n",
      "Epoch 159/350, Training Loss: 0.0135, Validation Loss: 0.0146\n",
      "Epoch 160/350, Training Loss: 0.0135, Validation Loss: 0.0146\n",
      "Epoch 161/350, Training Loss: 0.0134, Validation Loss: 0.0145\n",
      "Epoch 162/350, Training Loss: 0.0134, Validation Loss: 0.0145\n",
      "Epoch 163/350, Training Loss: 0.0134, Validation Loss: 0.0145\n",
      "Epoch 164/350, Training Loss: 0.0134, Validation Loss: 0.0145\n",
      "Epoch 165/350, Training Loss: 0.0133, Validation Loss: 0.0145\n",
      "Epoch 166/350, Training Loss: 0.0133, Validation Loss: 0.0145\n",
      "Epoch 167/350, Training Loss: 0.0133, Validation Loss: 0.0145\n",
      "Epoch 168/350, Training Loss: 0.0133, Validation Loss: 0.0144\n",
      "Epoch 169/350, Training Loss: 0.0132, Validation Loss: 0.0144\n",
      "Epoch 170/350, Training Loss: 0.0132, Validation Loss: 0.0144\n",
      "Epoch 171/350, Training Loss: 0.0132, Validation Loss: 0.0144\n",
      "Epoch 172/350, Training Loss: 0.0132, Validation Loss: 0.0144\n",
      "Epoch 173/350, Training Loss: 0.0131, Validation Loss: 0.0144\n",
      "Epoch 174/350, Training Loss: 0.0131, Validation Loss: 0.0143\n",
      "Epoch 175/350, Training Loss: 0.0131, Validation Loss: 0.0143\n",
      "Epoch 176/350, Training Loss: 0.0131, Validation Loss: 0.0143\n",
      "Epoch 177/350, Training Loss: 0.0130, Validation Loss: 0.0143\n",
      "Epoch 178/350, Training Loss: 0.0130, Validation Loss: 0.0143\n",
      "Epoch 179/350, Training Loss: 0.0130, Validation Loss: 0.0143\n",
      "Epoch 180/350, Training Loss: 0.0130, Validation Loss: 0.0143\n",
      "Epoch 181/350, Training Loss: 0.0129, Validation Loss: 0.0142\n",
      "Epoch 182/350, Training Loss: 0.0129, Validation Loss: 0.0142\n",
      "Epoch 183/350, Training Loss: 0.0129, Validation Loss: 0.0142\n",
      "Epoch 184/350, Training Loss: 0.0129, Validation Loss: 0.0142\n",
      "Epoch 185/350, Training Loss: 0.0128, Validation Loss: 0.0142\n",
      "Epoch 186/350, Training Loss: 0.0128, Validation Loss: 0.0142\n",
      "Epoch 187/350, Training Loss: 0.0128, Validation Loss: 0.0142\n",
      "Epoch 188/350, Training Loss: 0.0128, Validation Loss: 0.0141\n",
      "Epoch 189/350, Training Loss: 0.0128, Validation Loss: 0.0141\n",
      "Epoch 190/350, Training Loss: 0.0127, Validation Loss: 0.0141\n",
      "Epoch 191/350, Training Loss: 0.0127, Validation Loss: 0.0141\n",
      "Epoch 192/350, Training Loss: 0.0127, Validation Loss: 0.0141\n",
      "Epoch 193/350, Training Loss: 0.0127, Validation Loss: 0.0141\n",
      "Epoch 194/350, Training Loss: 0.0127, Validation Loss: 0.0141\n",
      "Epoch 195/350, Training Loss: 0.0126, Validation Loss: 0.0140\n",
      "Epoch 196/350, Training Loss: 0.0126, Validation Loss: 0.0140\n",
      "Epoch 197/350, Training Loss: 0.0126, Validation Loss: 0.0140\n",
      "Epoch 198/350, Training Loss: 0.0126, Validation Loss: 0.0140\n",
      "Epoch 199/350, Training Loss: 0.0126, Validation Loss: 0.0140\n",
      "Epoch 200/350, Training Loss: 0.0125, Validation Loss: 0.0140\n",
      "Epoch 201/350, Training Loss: 0.0125, Validation Loss: 0.0140\n",
      "Epoch 202/350, Training Loss: 0.0125, Validation Loss: 0.0139\n",
      "Epoch 203/350, Training Loss: 0.0125, Validation Loss: 0.0139\n",
      "Epoch 204/350, Training Loss: 0.0125, Validation Loss: 0.0139\n",
      "Epoch 205/350, Training Loss: 0.0124, Validation Loss: 0.0139\n",
      "Epoch 206/350, Training Loss: 0.0124, Validation Loss: 0.0139\n",
      "Epoch 207/350, Training Loss: 0.0124, Validation Loss: 0.0139\n",
      "Epoch 208/350, Training Loss: 0.0124, Validation Loss: 0.0139\n",
      "Epoch 209/350, Training Loss: 0.0124, Validation Loss: 0.0138\n",
      "Epoch 210/350, Training Loss: 0.0123, Validation Loss: 0.0138\n",
      "Epoch 211/350, Training Loss: 0.0123, Validation Loss: 0.0138\n",
      "Epoch 212/350, Training Loss: 0.0123, Validation Loss: 0.0138\n",
      "Epoch 213/350, Training Loss: 0.0123, Validation Loss: 0.0138\n",
      "Epoch 214/350, Training Loss: 0.0123, Validation Loss: 0.0138\n",
      "Epoch 215/350, Training Loss: 0.0123, Validation Loss: 0.0138\n",
      "Epoch 216/350, Training Loss: 0.0122, Validation Loss: 0.0138\n",
      "Epoch 217/350, Training Loss: 0.0122, Validation Loss: 0.0137\n",
      "Epoch 218/350, Training Loss: 0.0122, Validation Loss: 0.0137\n",
      "Epoch 219/350, Training Loss: 0.0122, Validation Loss: 0.0137\n",
      "Epoch 220/350, Training Loss: 0.0122, Validation Loss: 0.0137\n",
      "Epoch 221/350, Training Loss: 0.0122, Validation Loss: 0.0137\n",
      "Epoch 222/350, Training Loss: 0.0121, Validation Loss: 0.0137\n",
      "Epoch 223/350, Training Loss: 0.0121, Validation Loss: 0.0137\n",
      "Epoch 224/350, Training Loss: 0.0121, Validation Loss: 0.0136\n",
      "Epoch 225/350, Training Loss: 0.0121, Validation Loss: 0.0136\n",
      "Epoch 226/350, Training Loss: 0.0121, Validation Loss: 0.0136\n",
      "Epoch 227/350, Training Loss: 0.0121, Validation Loss: 0.0136\n",
      "Epoch 228/350, Training Loss: 0.0120, Validation Loss: 0.0136\n",
      "Epoch 229/350, Training Loss: 0.0120, Validation Loss: 0.0136\n",
      "Epoch 230/350, Training Loss: 0.0120, Validation Loss: 0.0136\n",
      "Epoch 231/350, Training Loss: 0.0120, Validation Loss: 0.0135\n",
      "Epoch 232/350, Training Loss: 0.0120, Validation Loss: 0.0135\n",
      "Epoch 233/350, Training Loss: 0.0120, Validation Loss: 0.0135\n",
      "Epoch 234/350, Training Loss: 0.0120, Validation Loss: 0.0135\n",
      "Epoch 235/350, Training Loss: 0.0119, Validation Loss: 0.0135\n",
      "Epoch 236/350, Training Loss: 0.0119, Validation Loss: 0.0135\n",
      "Epoch 237/350, Training Loss: 0.0119, Validation Loss: 0.0135\n",
      "Epoch 238/350, Training Loss: 0.0119, Validation Loss: 0.0135\n",
      "Epoch 239/350, Training Loss: 0.0119, Validation Loss: 0.0134\n",
      "Epoch 240/350, Training Loss: 0.0119, Validation Loss: 0.0134\n",
      "Epoch 241/350, Training Loss: 0.0118, Validation Loss: 0.0134\n",
      "Epoch 242/350, Training Loss: 0.0118, Validation Loss: 0.0134\n",
      "Epoch 243/350, Training Loss: 0.0118, Validation Loss: 0.0134\n",
      "Epoch 244/350, Training Loss: 0.0118, Validation Loss: 0.0134\n",
      "Epoch 245/350, Training Loss: 0.0118, Validation Loss: 0.0134\n",
      "Epoch 246/350, Training Loss: 0.0118, Validation Loss: 0.0133\n",
      "Epoch 247/350, Training Loss: 0.0118, Validation Loss: 0.0133\n",
      "Epoch 248/350, Training Loss: 0.0118, Validation Loss: 0.0133\n",
      "Epoch 249/350, Training Loss: 0.0117, Validation Loss: 0.0133\n",
      "Epoch 250/350, Training Loss: 0.0117, Validation Loss: 0.0133\n",
      "Epoch 251/350, Training Loss: 0.0117, Validation Loss: 0.0133\n",
      "Epoch 252/350, Training Loss: 0.0117, Validation Loss: 0.0133\n",
      "Epoch 253/350, Training Loss: 0.0117, Validation Loss: 0.0133\n",
      "Epoch 254/350, Training Loss: 0.0117, Validation Loss: 0.0132\n",
      "Epoch 255/350, Training Loss: 0.0117, Validation Loss: 0.0132\n",
      "Epoch 256/350, Training Loss: 0.0116, Validation Loss: 0.0132\n",
      "Epoch 257/350, Training Loss: 0.0116, Validation Loss: 0.0132\n",
      "Epoch 258/350, Training Loss: 0.0116, Validation Loss: 0.0132\n",
      "Epoch 259/350, Training Loss: 0.0116, Validation Loss: 0.0132\n",
      "Epoch 260/350, Training Loss: 0.0116, Validation Loss: 0.0132\n",
      "Epoch 261/350, Training Loss: 0.0116, Validation Loss: 0.0132\n",
      "Epoch 262/350, Training Loss: 0.0116, Validation Loss: 0.0131\n",
      "Epoch 263/350, Training Loss: 0.0116, Validation Loss: 0.0131\n",
      "Epoch 264/350, Training Loss: 0.0115, Validation Loss: 0.0131\n",
      "Epoch 265/350, Training Loss: 0.0115, Validation Loss: 0.0131\n",
      "Epoch 266/350, Training Loss: 0.0115, Validation Loss: 0.0131\n",
      "Epoch 267/350, Training Loss: 0.0115, Validation Loss: 0.0131\n",
      "Epoch 268/350, Training Loss: 0.0115, Validation Loss: 0.0131\n",
      "Epoch 269/350, Training Loss: 0.0115, Validation Loss: 0.0131\n",
      "Epoch 270/350, Training Loss: 0.0115, Validation Loss: 0.0130\n",
      "Epoch 271/350, Training Loss: 0.0115, Validation Loss: 0.0130\n",
      "Epoch 272/350, Training Loss: 0.0114, Validation Loss: 0.0130\n",
      "Epoch 273/350, Training Loss: 0.0114, Validation Loss: 0.0130\n",
      "Epoch 274/350, Training Loss: 0.0114, Validation Loss: 0.0130\n",
      "Epoch 275/350, Training Loss: 0.0114, Validation Loss: 0.0130\n",
      "Epoch 276/350, Training Loss: 0.0114, Validation Loss: 0.0130\n",
      "Epoch 277/350, Training Loss: 0.0114, Validation Loss: 0.0130\n",
      "Epoch 278/350, Training Loss: 0.0114, Validation Loss: 0.0130\n",
      "Epoch 279/350, Training Loss: 0.0114, Validation Loss: 0.0130\n",
      "Epoch 280/350, Training Loss: 0.0114, Validation Loss: 0.0129\n",
      "Epoch 281/350, Training Loss: 0.0113, Validation Loss: 0.0129\n",
      "Epoch 282/350, Training Loss: 0.0113, Validation Loss: 0.0129\n",
      "Epoch 283/350, Training Loss: 0.0113, Validation Loss: 0.0129\n",
      "Epoch 284/350, Training Loss: 0.0113, Validation Loss: 0.0129\n",
      "Epoch 285/350, Training Loss: 0.0113, Validation Loss: 0.0129\n",
      "Epoch 286/350, Training Loss: 0.0113, Validation Loss: 0.0129\n",
      "Epoch 287/350, Training Loss: 0.0113, Validation Loss: 0.0129\n",
      "Epoch 288/350, Training Loss: 0.0113, Validation Loss: 0.0129\n",
      "Epoch 289/350, Training Loss: 0.0112, Validation Loss: 0.0129\n",
      "Epoch 290/350, Training Loss: 0.0112, Validation Loss: 0.0128\n",
      "Epoch 291/350, Training Loss: 0.0112, Validation Loss: 0.0128\n",
      "Epoch 292/350, Training Loss: 0.0112, Validation Loss: 0.0128\n",
      "Epoch 293/350, Training Loss: 0.0112, Validation Loss: 0.0128\n",
      "Epoch 294/350, Training Loss: 0.0112, Validation Loss: 0.0128\n",
      "Epoch 295/350, Training Loss: 0.0112, Validation Loss: 0.0128\n",
      "Epoch 296/350, Training Loss: 0.0112, Validation Loss: 0.0128\n",
      "Epoch 297/350, Training Loss: 0.0112, Validation Loss: 0.0128\n",
      "Epoch 298/350, Training Loss: 0.0112, Validation Loss: 0.0128\n",
      "Epoch 299/350, Training Loss: 0.0111, Validation Loss: 0.0128\n",
      "Epoch 300/350, Training Loss: 0.0111, Validation Loss: 0.0128\n",
      "Epoch 301/350, Training Loss: 0.0111, Validation Loss: 0.0128\n",
      "Epoch 302/350, Training Loss: 0.0111, Validation Loss: 0.0127\n",
      "Epoch 303/350, Training Loss: 0.0111, Validation Loss: 0.0127\n",
      "Epoch 304/350, Training Loss: 0.0111, Validation Loss: 0.0127\n",
      "Epoch 305/350, Training Loss: 0.0111, Validation Loss: 0.0127\n",
      "Epoch 306/350, Training Loss: 0.0111, Validation Loss: 0.0127\n",
      "Epoch 307/350, Training Loss: 0.0111, Validation Loss: 0.0127\n",
      "Epoch 308/350, Training Loss: 0.0111, Validation Loss: 0.0127\n",
      "Epoch 309/350, Training Loss: 0.0110, Validation Loss: 0.0127\n",
      "Epoch 310/350, Training Loss: 0.0110, Validation Loss: 0.0127\n",
      "Epoch 311/350, Training Loss: 0.0110, Validation Loss: 0.0127\n",
      "Epoch 312/350, Training Loss: 0.0110, Validation Loss: 0.0127\n",
      "Epoch 313/350, Training Loss: 0.0110, Validation Loss: 0.0127\n",
      "Epoch 314/350, Training Loss: 0.0110, Validation Loss: 0.0127\n",
      "Epoch 315/350, Training Loss: 0.0110, Validation Loss: 0.0127\n",
      "Epoch 316/350, Training Loss: 0.0110, Validation Loss: 0.0126\n",
      "Epoch 317/350, Training Loss: 0.0110, Validation Loss: 0.0126\n",
      "Epoch 318/350, Training Loss: 0.0110, Validation Loss: 0.0126\n",
      "Epoch 319/350, Training Loss: 0.0109, Validation Loss: 0.0126\n",
      "Epoch 320/350, Training Loss: 0.0109, Validation Loss: 0.0126\n",
      "Epoch 321/350, Training Loss: 0.0109, Validation Loss: 0.0126\n",
      "Epoch 322/350, Training Loss: 0.0109, Validation Loss: 0.0126\n",
      "Epoch 323/350, Training Loss: 0.0109, Validation Loss: 0.0126\n",
      "Epoch 324/350, Training Loss: 0.0109, Validation Loss: 0.0126\n",
      "Epoch 325/350, Training Loss: 0.0109, Validation Loss: 0.0126\n",
      "Epoch 326/350, Training Loss: 0.0109, Validation Loss: 0.0126\n",
      "Epoch 327/350, Training Loss: 0.0109, Validation Loss: 0.0126\n",
      "Epoch 328/350, Training Loss: 0.0109, Validation Loss: 0.0126\n",
      "Epoch 329/350, Training Loss: 0.0109, Validation Loss: 0.0125\n",
      "Epoch 330/350, Training Loss: 0.0108, Validation Loss: 0.0125\n",
      "Epoch 331/350, Training Loss: 0.0108, Validation Loss: 0.0125\n",
      "Epoch 332/350, Training Loss: 0.0108, Validation Loss: 0.0125\n",
      "Epoch 333/350, Training Loss: 0.0108, Validation Loss: 0.0125\n",
      "Epoch 334/350, Training Loss: 0.0108, Validation Loss: 0.0125\n",
      "Epoch 335/350, Training Loss: 0.0108, Validation Loss: 0.0125\n",
      "Epoch 336/350, Training Loss: 0.0108, Validation Loss: 0.0125\n",
      "Epoch 337/350, Training Loss: 0.0108, Validation Loss: 0.0125\n",
      "Epoch 338/350, Training Loss: 0.0108, Validation Loss: 0.0125\n",
      "Epoch 339/350, Training Loss: 0.0108, Validation Loss: 0.0125\n",
      "Epoch 340/350, Training Loss: 0.0108, Validation Loss: 0.0125\n",
      "Epoch 341/350, Training Loss: 0.0107, Validation Loss: 0.0125\n",
      "Epoch 342/350, Training Loss: 0.0107, Validation Loss: 0.0125\n",
      "Epoch 343/350, Training Loss: 0.0107, Validation Loss: 0.0124\n",
      "Epoch 344/350, Training Loss: 0.0107, Validation Loss: 0.0124\n",
      "Epoch 345/350, Training Loss: 0.0107, Validation Loss: 0.0124\n",
      "Epoch 346/350, Training Loss: 0.0107, Validation Loss: 0.0124\n",
      "Epoch 347/350, Training Loss: 0.0107, Validation Loss: 0.0124\n",
      "Epoch 348/350, Training Loss: 0.0107, Validation Loss: 0.0124\n",
      "Epoch 349/350, Training Loss: 0.0107, Validation Loss: 0.0124\n",
      "Epoch 350/350, Training Loss: 0.0107, Validation Loss: 0.0124\n",
      "1.6803940344894994\n",
      "Epoch 1/350, Training Loss: 0.2435, Validation Loss: 0.1678\n",
      "Epoch 2/350, Training Loss: 0.0844, Validation Loss: 0.0556\n",
      "Epoch 3/350, Training Loss: 0.0488, Validation Loss: 0.0445\n",
      "Epoch 4/350, Training Loss: 0.0403, Validation Loss: 0.0372\n",
      "Epoch 5/350, Training Loss: 0.0351, Validation Loss: 0.0324\n",
      "Epoch 6/350, Training Loss: 0.0315, Validation Loss: 0.0288\n",
      "Epoch 7/350, Training Loss: 0.0283, Validation Loss: 0.0256\n",
      "Epoch 8/350, Training Loss: 0.0257, Validation Loss: 0.0231\n",
      "Epoch 9/350, Training Loss: 0.0233, Validation Loss: 0.0211\n",
      "Epoch 10/350, Training Loss: 0.0213, Validation Loss: 0.0197\n",
      "Epoch 11/350, Training Loss: 0.0197, Validation Loss: 0.0187\n",
      "Epoch 12/350, Training Loss: 0.0185, Validation Loss: 0.0179\n",
      "Epoch 13/350, Training Loss: 0.0176, Validation Loss: 0.0173\n",
      "Epoch 14/350, Training Loss: 0.0168, Validation Loss: 0.0168\n",
      "Epoch 15/350, Training Loss: 0.0162, Validation Loss: 0.0164\n",
      "Epoch 16/350, Training Loss: 0.0156, Validation Loss: 0.0160\n",
      "Epoch 17/350, Training Loss: 0.0151, Validation Loss: 0.0156\n",
      "Epoch 18/350, Training Loss: 0.0147, Validation Loss: 0.0153\n",
      "Epoch 19/350, Training Loss: 0.0143, Validation Loss: 0.0150\n",
      "Epoch 20/350, Training Loss: 0.0139, Validation Loss: 0.0146\n",
      "Epoch 21/350, Training Loss: 0.0135, Validation Loss: 0.0142\n",
      "Epoch 22/350, Training Loss: 0.0132, Validation Loss: 0.0139\n",
      "Epoch 23/350, Training Loss: 0.0129, Validation Loss: 0.0134\n",
      "Epoch 24/350, Training Loss: 0.0126, Validation Loss: 0.0130\n",
      "Epoch 25/350, Training Loss: 0.0123, Validation Loss: 0.0126\n",
      "Epoch 26/350, Training Loss: 0.0120, Validation Loss: 0.0122\n",
      "Epoch 27/350, Training Loss: 0.0117, Validation Loss: 0.0119\n",
      "Epoch 28/350, Training Loss: 0.0115, Validation Loss: 0.0116\n",
      "Epoch 29/350, Training Loss: 0.0113, Validation Loss: 0.0113\n",
      "Epoch 30/350, Training Loss: 0.0111, Validation Loss: 0.0110\n",
      "Epoch 31/350, Training Loss: 0.0109, Validation Loss: 0.0106\n",
      "Epoch 32/350, Training Loss: 0.0107, Validation Loss: 0.0103\n",
      "Epoch 33/350, Training Loss: 0.0105, Validation Loss: 0.0102\n",
      "Epoch 34/350, Training Loss: 0.0103, Validation Loss: 0.0101\n",
      "Epoch 35/350, Training Loss: 0.0102, Validation Loss: 0.0099\n",
      "Epoch 36/350, Training Loss: 0.0102, Validation Loss: 0.0105\n",
      "Epoch 37/350, Training Loss: 0.0097, Validation Loss: 0.0096\n",
      "Epoch 38/350, Training Loss: 0.0098, Validation Loss: 0.0100\n",
      "Epoch 39/350, Training Loss: 0.0100, Validation Loss: 0.0099\n",
      "Epoch 40/350, Training Loss: 0.0092, Validation Loss: 0.0092\n",
      "Epoch 41/350, Training Loss: 0.0097, Validation Loss: 0.0093\n",
      "Epoch 42/350, Training Loss: 0.0090, Validation Loss: 0.0090\n",
      "Epoch 43/350, Training Loss: 0.0093, Validation Loss: 0.0090\n",
      "Epoch 44/350, Training Loss: 0.0088, Validation Loss: 0.0089\n",
      "Epoch 45/350, Training Loss: 0.0095, Validation Loss: 0.0088\n",
      "Epoch 46/350, Training Loss: 0.0086, Validation Loss: 0.0087\n",
      "Epoch 47/350, Training Loss: 0.0090, Validation Loss: 0.0087\n",
      "Epoch 48/350, Training Loss: 0.0084, Validation Loss: 0.0086\n",
      "Epoch 49/350, Training Loss: 0.0089, Validation Loss: 0.0084\n",
      "Epoch 50/350, Training Loss: 0.0084, Validation Loss: 0.0087\n",
      "Epoch 51/350, Training Loss: 0.0088, Validation Loss: 0.0088\n",
      "Epoch 52/350, Training Loss: 0.0081, Validation Loss: 0.0083\n",
      "Epoch 53/350, Training Loss: 0.0081, Validation Loss: 0.0085\n",
      "Epoch 54/350, Training Loss: 0.0084, Validation Loss: 0.0081\n",
      "Epoch 55/350, Training Loss: 0.0080, Validation Loss: 0.0085\n",
      "Epoch 56/350, Training Loss: 0.0080, Validation Loss: 0.0080\n",
      "Epoch 57/350, Training Loss: 0.0082, Validation Loss: 0.0079\n",
      "Epoch 58/350, Training Loss: 0.0077, Validation Loss: 0.0078\n",
      "Epoch 59/350, Training Loss: 0.0077, Validation Loss: 0.0080\n",
      "Epoch 60/350, Training Loss: 0.0078, Validation Loss: 0.0077\n",
      "Epoch 61/350, Training Loss: 0.0077, Validation Loss: 0.0078\n",
      "Epoch 62/350, Training Loss: 0.0076, Validation Loss: 0.0075\n",
      "Epoch 63/350, Training Loss: 0.0076, Validation Loss: 0.0077\n",
      "Epoch 64/350, Training Loss: 0.0074, Validation Loss: 0.0074\n",
      "Epoch 65/350, Training Loss: 0.0075, Validation Loss: 0.0075\n",
      "Epoch 66/350, Training Loss: 0.0073, Validation Loss: 0.0073\n",
      "Epoch 67/350, Training Loss: 0.0071, Validation Loss: 0.0073\n",
      "Epoch 68/350, Training Loss: 0.0073, Validation Loss: 0.0072\n",
      "Epoch 69/350, Training Loss: 0.0072, Validation Loss: 0.0072\n",
      "Epoch 70/350, Training Loss: 0.0072, Validation Loss: 0.0071\n",
      "Epoch 71/350, Training Loss: 0.0069, Validation Loss: 0.0071\n",
      "Epoch 72/350, Training Loss: 0.0071, Validation Loss: 0.0070\n",
      "Epoch 73/350, Training Loss: 0.0069, Validation Loss: 0.0070\n",
      "Epoch 74/350, Training Loss: 0.0070, Validation Loss: 0.0069\n",
      "Epoch 75/350, Training Loss: 0.0068, Validation Loss: 0.0069\n",
      "Epoch 76/350, Training Loss: 0.0069, Validation Loss: 0.0068\n",
      "Epoch 77/350, Training Loss: 0.0067, Validation Loss: 0.0069\n",
      "Epoch 78/350, Training Loss: 0.0068, Validation Loss: 0.0067\n",
      "Epoch 79/350, Training Loss: 0.0066, Validation Loss: 0.0068\n",
      "Epoch 80/350, Training Loss: 0.0067, Validation Loss: 0.0066\n",
      "Epoch 81/350, Training Loss: 0.0065, Validation Loss: 0.0067\n",
      "Epoch 82/350, Training Loss: 0.0066, Validation Loss: 0.0066\n",
      "Epoch 83/350, Training Loss: 0.0064, Validation Loss: 0.0066\n",
      "Epoch 84/350, Training Loss: 0.0065, Validation Loss: 0.0065\n",
      "Epoch 85/350, Training Loss: 0.0063, Validation Loss: 0.0065\n",
      "Epoch 86/350, Training Loss: 0.0064, Validation Loss: 0.0064\n",
      "Epoch 87/350, Training Loss: 0.0063, Validation Loss: 0.0064\n",
      "Epoch 88/350, Training Loss: 0.0063, Validation Loss: 0.0064\n",
      "Epoch 89/350, Training Loss: 0.0062, Validation Loss: 0.0064\n",
      "Epoch 90/350, Training Loss: 0.0062, Validation Loss: 0.0063\n",
      "Epoch 91/350, Training Loss: 0.0061, Validation Loss: 0.0063\n",
      "Epoch 92/350, Training Loss: 0.0062, Validation Loss: 0.0062\n",
      "Epoch 93/350, Training Loss: 0.0061, Validation Loss: 0.0062\n",
      "Epoch 94/350, Training Loss: 0.0061, Validation Loss: 0.0061\n",
      "Epoch 95/350, Training Loss: 0.0060, Validation Loss: 0.0061\n",
      "Epoch 96/350, Training Loss: 0.0060, Validation Loss: 0.0061\n",
      "Epoch 97/350, Training Loss: 0.0060, Validation Loss: 0.0061\n",
      "Epoch 98/350, Training Loss: 0.0060, Validation Loss: 0.0060\n",
      "Epoch 99/350, Training Loss: 0.0059, Validation Loss: 0.0060\n",
      "Epoch 100/350, Training Loss: 0.0059, Validation Loss: 0.0059\n",
      "Epoch 101/350, Training Loss: 0.0059, Validation Loss: 0.0059\n",
      "Epoch 102/350, Training Loss: 0.0058, Validation Loss: 0.0059\n",
      "Epoch 103/350, Training Loss: 0.0058, Validation Loss: 0.0059\n",
      "Epoch 104/350, Training Loss: 0.0058, Validation Loss: 0.0058\n",
      "Epoch 105/350, Training Loss: 0.0057, Validation Loss: 0.0058\n",
      "Epoch 106/350, Training Loss: 0.0057, Validation Loss: 0.0058\n",
      "Epoch 107/350, Training Loss: 0.0057, Validation Loss: 0.0058\n",
      "Epoch 108/350, Training Loss: 0.0057, Validation Loss: 0.0057\n",
      "Epoch 109/350, Training Loss: 0.0057, Validation Loss: 0.0057\n",
      "Epoch 110/350, Training Loss: 0.0056, Validation Loss: 0.0057\n",
      "Epoch 111/350, Training Loss: 0.0056, Validation Loss: 0.0057\n",
      "Epoch 112/350, Training Loss: 0.0056, Validation Loss: 0.0056\n",
      "Epoch 113/350, Training Loss: 0.0056, Validation Loss: 0.0056\n",
      "Epoch 114/350, Training Loss: 0.0055, Validation Loss: 0.0056\n",
      "Epoch 115/350, Training Loss: 0.0055, Validation Loss: 0.0056\n",
      "Epoch 116/350, Training Loss: 0.0055, Validation Loss: 0.0055\n",
      "Epoch 117/350, Training Loss: 0.0055, Validation Loss: 0.0055\n",
      "Epoch 118/350, Training Loss: 0.0054, Validation Loss: 0.0055\n",
      "Epoch 119/350, Training Loss: 0.0054, Validation Loss: 0.0055\n",
      "Epoch 120/350, Training Loss: 0.0054, Validation Loss: 0.0054\n",
      "Epoch 121/350, Training Loss: 0.0054, Validation Loss: 0.0054\n",
      "Epoch 122/350, Training Loss: 0.0053, Validation Loss: 0.0054\n",
      "Epoch 123/350, Training Loss: 0.0053, Validation Loss: 0.0054\n",
      "Epoch 124/350, Training Loss: 0.0053, Validation Loss: 0.0053\n",
      "Epoch 125/350, Training Loss: 0.0053, Validation Loss: 0.0053\n",
      "Epoch 126/350, Training Loss: 0.0052, Validation Loss: 0.0053\n",
      "Epoch 127/350, Training Loss: 0.0052, Validation Loss: 0.0053\n",
      "Epoch 128/350, Training Loss: 0.0052, Validation Loss: 0.0053\n",
      "Epoch 129/350, Training Loss: 0.0052, Validation Loss: 0.0052\n",
      "Epoch 130/350, Training Loss: 0.0052, Validation Loss: 0.0052\n",
      "Epoch 131/350, Training Loss: 0.0051, Validation Loss: 0.0052\n",
      "Epoch 132/350, Training Loss: 0.0051, Validation Loss: 0.0052\n",
      "Epoch 133/350, Training Loss: 0.0051, Validation Loss: 0.0052\n",
      "Epoch 134/350, Training Loss: 0.0051, Validation Loss: 0.0052\n",
      "Epoch 135/350, Training Loss: 0.0051, Validation Loss: 0.0051\n",
      "Epoch 136/350, Training Loss: 0.0050, Validation Loss: 0.0051\n",
      "Epoch 137/350, Training Loss: 0.0050, Validation Loss: 0.0051\n",
      "Epoch 138/350, Training Loss: 0.0050, Validation Loss: 0.0051\n",
      "Epoch 139/350, Training Loss: 0.0050, Validation Loss: 0.0051\n",
      "Epoch 140/350, Training Loss: 0.0050, Validation Loss: 0.0050\n",
      "Epoch 141/350, Training Loss: 0.0049, Validation Loss: 0.0050\n",
      "Epoch 142/350, Training Loss: 0.0049, Validation Loss: 0.0050\n",
      "Epoch 143/350, Training Loss: 0.0049, Validation Loss: 0.0050\n",
      "Epoch 144/350, Training Loss: 0.0049, Validation Loss: 0.0050\n",
      "Epoch 145/350, Training Loss: 0.0049, Validation Loss: 0.0050\n",
      "Epoch 146/350, Training Loss: 0.0049, Validation Loss: 0.0049\n",
      "Epoch 147/350, Training Loss: 0.0048, Validation Loss: 0.0049\n",
      "Epoch 148/350, Training Loss: 0.0048, Validation Loss: 0.0049\n",
      "Epoch 149/350, Training Loss: 0.0048, Validation Loss: 0.0049\n",
      "Epoch 150/350, Training Loss: 0.0048, Validation Loss: 0.0049\n",
      "Epoch 151/350, Training Loss: 0.0048, Validation Loss: 0.0049\n",
      "Epoch 152/350, Training Loss: 0.0048, Validation Loss: 0.0048\n",
      "Epoch 153/350, Training Loss: 0.0047, Validation Loss: 0.0048\n",
      "Epoch 154/350, Training Loss: 0.0047, Validation Loss: 0.0048\n",
      "Epoch 155/350, Training Loss: 0.0047, Validation Loss: 0.0048\n",
      "Epoch 156/350, Training Loss: 0.0047, Validation Loss: 0.0048\n",
      "Epoch 157/350, Training Loss: 0.0047, Validation Loss: 0.0048\n",
      "Epoch 158/350, Training Loss: 0.0047, Validation Loss: 0.0048\n",
      "Epoch 159/350, Training Loss: 0.0046, Validation Loss: 0.0047\n",
      "Epoch 160/350, Training Loss: 0.0046, Validation Loss: 0.0047\n",
      "Epoch 161/350, Training Loss: 0.0046, Validation Loss: 0.0047\n",
      "Epoch 162/350, Training Loss: 0.0046, Validation Loss: 0.0047\n",
      "Epoch 163/350, Training Loss: 0.0046, Validation Loss: 0.0047\n",
      "Epoch 164/350, Training Loss: 0.0046, Validation Loss: 0.0047\n",
      "Epoch 165/350, Training Loss: 0.0046, Validation Loss: 0.0047\n",
      "Epoch 166/350, Training Loss: 0.0045, Validation Loss: 0.0047\n",
      "Epoch 167/350, Training Loss: 0.0045, Validation Loss: 0.0046\n",
      "Epoch 168/350, Training Loss: 0.0045, Validation Loss: 0.0046\n",
      "Epoch 169/350, Training Loss: 0.0045, Validation Loss: 0.0046\n",
      "Epoch 170/350, Training Loss: 0.0045, Validation Loss: 0.0046\n",
      "Epoch 171/350, Training Loss: 0.0045, Validation Loss: 0.0046\n",
      "Epoch 172/350, Training Loss: 0.0045, Validation Loss: 0.0046\n",
      "Epoch 173/350, Training Loss: 0.0045, Validation Loss: 0.0046\n",
      "Epoch 174/350, Training Loss: 0.0044, Validation Loss: 0.0046\n",
      "Epoch 175/350, Training Loss: 0.0044, Validation Loss: 0.0046\n",
      "Epoch 176/350, Training Loss: 0.0044, Validation Loss: 0.0045\n",
      "Epoch 177/350, Training Loss: 0.0044, Validation Loss: 0.0045\n",
      "Epoch 178/350, Training Loss: 0.0044, Validation Loss: 0.0045\n",
      "Epoch 179/350, Training Loss: 0.0044, Validation Loss: 0.0045\n",
      "Epoch 180/350, Training Loss: 0.0044, Validation Loss: 0.0045\n",
      "Epoch 181/350, Training Loss: 0.0044, Validation Loss: 0.0045\n",
      "Epoch 182/350, Training Loss: 0.0043, Validation Loss: 0.0045\n",
      "Epoch 183/350, Training Loss: 0.0043, Validation Loss: 0.0045\n",
      "Epoch 184/350, Training Loss: 0.0043, Validation Loss: 0.0045\n",
      "Epoch 185/350, Training Loss: 0.0043, Validation Loss: 0.0044\n",
      "Epoch 186/350, Training Loss: 0.0043, Validation Loss: 0.0044\n",
      "Epoch 187/350, Training Loss: 0.0043, Validation Loss: 0.0044\n",
      "Epoch 188/350, Training Loss: 0.0043, Validation Loss: 0.0044\n",
      "Epoch 189/350, Training Loss: 0.0043, Validation Loss: 0.0044\n",
      "Epoch 190/350, Training Loss: 0.0042, Validation Loss: 0.0044\n",
      "Epoch 191/350, Training Loss: 0.0042, Validation Loss: 0.0044\n",
      "Epoch 192/350, Training Loss: 0.0042, Validation Loss: 0.0043\n",
      "Epoch 193/350, Training Loss: 0.0042, Validation Loss: 0.0043\n",
      "Epoch 194/350, Training Loss: 0.0042, Validation Loss: 0.0043\n",
      "Epoch 195/350, Training Loss: 0.0042, Validation Loss: 0.0043\n",
      "Epoch 196/350, Training Loss: 0.0042, Validation Loss: 0.0044\n",
      "Epoch 197/350, Training Loss: 0.0042, Validation Loss: 0.0044\n",
      "Epoch 198/350, Training Loss: 0.0042, Validation Loss: 0.0043\n",
      "Epoch 199/350, Training Loss: 0.0042, Validation Loss: 0.0043\n",
      "Epoch 200/350, Training Loss: 0.0042, Validation Loss: 0.0043\n",
      "Epoch 201/350, Training Loss: 0.0041, Validation Loss: 0.0043\n",
      "Epoch 202/350, Training Loss: 0.0041, Validation Loss: 0.0043\n",
      "Epoch 203/350, Training Loss: 0.0042, Validation Loss: 0.0043\n",
      "Epoch 204/350, Training Loss: 0.0042, Validation Loss: 0.0045\n",
      "Epoch 205/350, Training Loss: 0.0042, Validation Loss: 0.0043\n",
      "Epoch 206/350, Training Loss: 0.0040, Validation Loss: 0.0043\n",
      "Epoch 207/350, Training Loss: 0.0041, Validation Loss: 0.0042\n",
      "Epoch 208/350, Training Loss: 0.0040, Validation Loss: 0.0042\n",
      "Epoch 209/350, Training Loss: 0.0042, Validation Loss: 0.0042\n",
      "Epoch 210/350, Training Loss: 0.0040, Validation Loss: 0.0045\n",
      "Epoch 211/350, Training Loss: 0.0041, Validation Loss: 0.0042\n",
      "Epoch 212/350, Training Loss: 0.0040, Validation Loss: 0.0043\n",
      "Epoch 213/350, Training Loss: 0.0041, Validation Loss: 0.0042\n",
      "Epoch 214/350, Training Loss: 0.0040, Validation Loss: 0.0046\n",
      "Epoch 215/350, Training Loss: 0.0041, Validation Loss: 0.0042\n",
      "Epoch 216/350, Training Loss: 0.0040, Validation Loss: 0.0042\n",
      "Epoch 217/350, Training Loss: 0.0041, Validation Loss: 0.0041\n",
      "Epoch 218/350, Training Loss: 0.0040, Validation Loss: 0.0045\n",
      "Epoch 219/350, Training Loss: 0.0041, Validation Loss: 0.0042\n",
      "Epoch 220/350, Training Loss: 0.0039, Validation Loss: 0.0043\n",
      "Epoch 221/350, Training Loss: 0.0040, Validation Loss: 0.0041\n",
      "Epoch 222/350, Training Loss: 0.0040, Validation Loss: 0.0045\n",
      "Epoch 223/350, Training Loss: 0.0040, Validation Loss: 0.0041\n",
      "Epoch 224/350, Training Loss: 0.0039, Validation Loss: 0.0044\n",
      "Epoch 225/350, Training Loss: 0.0040, Validation Loss: 0.0041\n",
      "Epoch 226/350, Training Loss: 0.0039, Validation Loss: 0.0043\n",
      "Epoch 227/350, Training Loss: 0.0040, Validation Loss: 0.0041\n",
      "Epoch 228/350, Training Loss: 0.0039, Validation Loss: 0.0044\n",
      "Epoch 229/350, Training Loss: 0.0040, Validation Loss: 0.0041\n",
      "Epoch 230/350, Training Loss: 0.0039, Validation Loss: 0.0044\n",
      "Epoch 231/350, Training Loss: 0.0040, Validation Loss: 0.0041\n",
      "Epoch 232/350, Training Loss: 0.0039, Validation Loss: 0.0044\n",
      "Epoch 233/350, Training Loss: 0.0040, Validation Loss: 0.0041\n",
      "Epoch 234/350, Training Loss: 0.0039, Validation Loss: 0.0043\n",
      "Epoch 235/350, Training Loss: 0.0039, Validation Loss: 0.0041\n",
      "Epoch 236/350, Training Loss: 0.0039, Validation Loss: 0.0043\n",
      "Epoch 237/350, Training Loss: 0.0039, Validation Loss: 0.0041\n",
      "Epoch 238/350, Training Loss: 0.0038, Validation Loss: 0.0043\n",
      "Epoch 239/350, Training Loss: 0.0039, Validation Loss: 0.0041\n",
      "Epoch 240/350, Training Loss: 0.0038, Validation Loss: 0.0043\n",
      "Epoch 241/350, Training Loss: 0.0039, Validation Loss: 0.0040\n",
      "Epoch 242/350, Training Loss: 0.0038, Validation Loss: 0.0043\n",
      "Epoch 243/350, Training Loss: 0.0039, Validation Loss: 0.0040\n",
      "Epoch 244/350, Training Loss: 0.0038, Validation Loss: 0.0043\n",
      "Epoch 245/350, Training Loss: 0.0039, Validation Loss: 0.0040\n",
      "Epoch 246/350, Training Loss: 0.0038, Validation Loss: 0.0042\n",
      "Epoch 247/350, Training Loss: 0.0039, Validation Loss: 0.0040\n",
      "Epoch 248/350, Training Loss: 0.0038, Validation Loss: 0.0042\n",
      "Epoch 249/350, Training Loss: 0.0039, Validation Loss: 0.0040\n",
      "Epoch 250/350, Training Loss: 0.0038, Validation Loss: 0.0042\n",
      "Epoch 251/350, Training Loss: 0.0038, Validation Loss: 0.0040\n",
      "Epoch 252/350, Training Loss: 0.0038, Validation Loss: 0.0042\n",
      "Epoch 253/350, Training Loss: 0.0038, Validation Loss: 0.0040\n",
      "Epoch 254/350, Training Loss: 0.0038, Validation Loss: 0.0041\n",
      "Epoch 255/350, Training Loss: 0.0038, Validation Loss: 0.0040\n",
      "Epoch 256/350, Training Loss: 0.0037, Validation Loss: 0.0041\n",
      "Epoch 257/350, Training Loss: 0.0038, Validation Loss: 0.0040\n",
      "Epoch 258/350, Training Loss: 0.0037, Validation Loss: 0.0041\n",
      "Epoch 259/350, Training Loss: 0.0038, Validation Loss: 0.0040\n",
      "Epoch 260/350, Training Loss: 0.0037, Validation Loss: 0.0041\n",
      "Epoch 261/350, Training Loss: 0.0038, Validation Loss: 0.0040\n",
      "Epoch 262/350, Training Loss: 0.0037, Validation Loss: 0.0041\n",
      "Epoch 263/350, Training Loss: 0.0038, Validation Loss: 0.0040\n",
      "Epoch 264/350, Training Loss: 0.0037, Validation Loss: 0.0040\n",
      "Epoch 265/350, Training Loss: 0.0038, Validation Loss: 0.0040\n",
      "Epoch 266/350, Training Loss: 0.0037, Validation Loss: 0.0040\n",
      "Epoch 267/350, Training Loss: 0.0038, Validation Loss: 0.0040\n",
      "Epoch 268/350, Training Loss: 0.0037, Validation Loss: 0.0040\n",
      "Epoch 269/350, Training Loss: 0.0038, Validation Loss: 0.0039\n",
      "Epoch 270/350, Training Loss: 0.0037, Validation Loss: 0.0040\n",
      "Epoch 271/350, Training Loss: 0.0038, Validation Loss: 0.0039\n",
      "Epoch 272/350, Training Loss: 0.0037, Validation Loss: 0.0040\n",
      "Epoch 273/350, Training Loss: 0.0038, Validation Loss: 0.0039\n",
      "Epoch 274/350, Training Loss: 0.0036, Validation Loss: 0.0040\n",
      "Epoch 275/350, Training Loss: 0.0038, Validation Loss: 0.0039\n",
      "Epoch 276/350, Training Loss: 0.0036, Validation Loss: 0.0040\n",
      "Epoch 277/350, Training Loss: 0.0037, Validation Loss: 0.0039\n",
      "Epoch 278/350, Training Loss: 0.0036, Validation Loss: 0.0039\n",
      "Epoch 279/350, Training Loss: 0.0037, Validation Loss: 0.0039\n",
      "Epoch 280/350, Training Loss: 0.0036, Validation Loss: 0.0039\n",
      "Epoch 281/350, Training Loss: 0.0037, Validation Loss: 0.0039\n",
      "Epoch 282/350, Training Loss: 0.0036, Validation Loss: 0.0039\n",
      "Epoch 283/350, Training Loss: 0.0037, Validation Loss: 0.0039\n",
      "Epoch 284/350, Training Loss: 0.0036, Validation Loss: 0.0039\n",
      "Epoch 285/350, Training Loss: 0.0037, Validation Loss: 0.0039\n",
      "Epoch 286/350, Training Loss: 0.0036, Validation Loss: 0.0039\n",
      "Epoch 287/350, Training Loss: 0.0037, Validation Loss: 0.0039\n",
      "Epoch 288/350, Training Loss: 0.0036, Validation Loss: 0.0039\n",
      "Epoch 289/350, Training Loss: 0.0037, Validation Loss: 0.0039\n",
      "Epoch 290/350, Training Loss: 0.0036, Validation Loss: 0.0039\n",
      "Epoch 291/350, Training Loss: 0.0037, Validation Loss: 0.0039\n",
      "Epoch 292/350, Training Loss: 0.0036, Validation Loss: 0.0039\n",
      "Epoch 293/350, Training Loss: 0.0037, Validation Loss: 0.0039\n",
      "Epoch 294/350, Training Loss: 0.0036, Validation Loss: 0.0039\n",
      "Epoch 295/350, Training Loss: 0.0037, Validation Loss: 0.0038\n",
      "Epoch 296/350, Training Loss: 0.0036, Validation Loss: 0.0038\n",
      "Epoch 297/350, Training Loss: 0.0037, Validation Loss: 0.0038\n",
      "Epoch 298/350, Training Loss: 0.0036, Validation Loss: 0.0038\n",
      "Epoch 299/350, Training Loss: 0.0036, Validation Loss: 0.0038\n",
      "Epoch 300/350, Training Loss: 0.0036, Validation Loss: 0.0038\n",
      "Epoch 301/350, Training Loss: 0.0036, Validation Loss: 0.0038\n",
      "Epoch 302/350, Training Loss: 0.0035, Validation Loss: 0.0038\n",
      "Epoch 303/350, Training Loss: 0.0036, Validation Loss: 0.0038\n",
      "Epoch 304/350, Training Loss: 0.0035, Validation Loss: 0.0038\n",
      "Epoch 305/350, Training Loss: 0.0036, Validation Loss: 0.0038\n",
      "Epoch 306/350, Training Loss: 0.0035, Validation Loss: 0.0038\n",
      "Epoch 307/350, Training Loss: 0.0036, Validation Loss: 0.0038\n",
      "Epoch 308/350, Training Loss: 0.0035, Validation Loss: 0.0038\n",
      "Epoch 309/350, Training Loss: 0.0036, Validation Loss: 0.0038\n",
      "Epoch 310/350, Training Loss: 0.0035, Validation Loss: 0.0038\n",
      "Epoch 311/350, Training Loss: 0.0036, Validation Loss: 0.0038\n",
      "Epoch 312/350, Training Loss: 0.0035, Validation Loss: 0.0038\n",
      "Epoch 313/350, Training Loss: 0.0036, Validation Loss: 0.0038\n",
      "Epoch 314/350, Training Loss: 0.0035, Validation Loss: 0.0038\n",
      "Epoch 315/350, Training Loss: 0.0036, Validation Loss: 0.0038\n",
      "Epoch 316/350, Training Loss: 0.0035, Validation Loss: 0.0038\n",
      "Epoch 317/350, Training Loss: 0.0036, Validation Loss: 0.0038\n",
      "Epoch 318/350, Training Loss: 0.0035, Validation Loss: 0.0038\n",
      "Epoch 319/350, Training Loss: 0.0036, Validation Loss: 0.0038\n",
      "Epoch 320/350, Training Loss: 0.0035, Validation Loss: 0.0038\n",
      "Epoch 321/350, Training Loss: 0.0036, Validation Loss: 0.0038\n",
      "Epoch 322/350, Training Loss: 0.0035, Validation Loss: 0.0038\n",
      "Epoch 323/350, Training Loss: 0.0035, Validation Loss: 0.0038\n",
      "Epoch 324/350, Training Loss: 0.0035, Validation Loss: 0.0037\n",
      "Epoch 325/350, Training Loss: 0.0035, Validation Loss: 0.0038\n",
      "Epoch 326/350, Training Loss: 0.0035, Validation Loss: 0.0037\n",
      "Epoch 327/350, Training Loss: 0.0035, Validation Loss: 0.0038\n",
      "Epoch 328/350, Training Loss: 0.0035, Validation Loss: 0.0037\n",
      "Epoch 329/350, Training Loss: 0.0035, Validation Loss: 0.0038\n",
      "Epoch 330/350, Training Loss: 0.0035, Validation Loss: 0.0037\n",
      "Epoch 331/350, Training Loss: 0.0035, Validation Loss: 0.0038\n",
      "Epoch 332/350, Training Loss: 0.0035, Validation Loss: 0.0037\n",
      "Epoch 333/350, Training Loss: 0.0035, Validation Loss: 0.0038\n",
      "Epoch 334/350, Training Loss: 0.0035, Validation Loss: 0.0037\n",
      "Epoch 335/350, Training Loss: 0.0035, Validation Loss: 0.0038\n",
      "Epoch 336/350, Training Loss: 0.0034, Validation Loss: 0.0037\n",
      "Epoch 337/350, Training Loss: 0.0035, Validation Loss: 0.0037\n",
      "Epoch 338/350, Training Loss: 0.0034, Validation Loss: 0.0037\n",
      "Epoch 339/350, Training Loss: 0.0035, Validation Loss: 0.0037\n",
      "Epoch 340/350, Training Loss: 0.0034, Validation Loss: 0.0037\n",
      "Epoch 341/350, Training Loss: 0.0035, Validation Loss: 0.0037\n",
      "Epoch 342/350, Training Loss: 0.0034, Validation Loss: 0.0037\n",
      "Epoch 343/350, Training Loss: 0.0035, Validation Loss: 0.0037\n",
      "Epoch 344/350, Training Loss: 0.0034, Validation Loss: 0.0037\n",
      "Epoch 345/350, Training Loss: 0.0035, Validation Loss: 0.0037\n",
      "Epoch 346/350, Training Loss: 0.0034, Validation Loss: 0.0037\n",
      "Epoch 347/350, Training Loss: 0.0035, Validation Loss: 0.0037\n",
      "Epoch 348/350, Training Loss: 0.0034, Validation Loss: 0.0037\n",
      "Epoch 349/350, Training Loss: 0.0035, Validation Loss: 0.0037\n",
      "Epoch 350/350, Training Loss: 0.0034, Validation Loss: 0.0037\n",
      "1.288724230882164\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, mean_absolute_percentage_error\n",
    "\n",
    "\n",
    "_window_size = 7\n",
    "horizon = 1\n",
    "_stride = 1\n",
    "\n",
    "# TGCN\n",
    "num_epochs_TGCN = 350\n",
    "\n",
    "n_neighbors = 12\n",
    "df_PeMS_old, df_distance  = load_PeMS04_flow_data()\n",
    "df_PeMS, adjacency_matrix_PeMS, meanstd_dict = preprocess_PeMS_data(df_PeMS_old, df_distance, init_node=0, n_neighbors=n_neighbors, center_and_reduce=True)\n",
    "print(df_PeMS.columns)\n",
    "node_0, _ = mask_unknown_nodes(df_PeMS=df_PeMS, list_nodes_known=[68, 62, 242, 243, 117, 28, 170, 0, 173, 271, 92, 57, 97])\n",
    "node_1, mask_node_1 = mask_unknown_nodes(df_PeMS=df_PeMS, list_nodes_known=[68, 62, 242, 243, 117, 28, 170])\n",
    "\n",
    "# TGCN Model\n",
    "model_TGCN = TGCN(adjacency_matrix_PeMS, hidden_dim=32, output_size=len(df_PeMS.columns))\n",
    "train_loader_TGCN, val_loader_TGCN, test_loader_TGCN, _ = createLoaders(node_0, window_size=_window_size, stride=_stride, prediction_horizon=horizon)\n",
    "model_path = \"./test13_13.pkl\"\n",
    "\n",
    "best_model , train_losses, valid_losses = train_model(model_TGCN, train_loader_TGCN, val_loader_TGCN, model_path=model_path, mask=None, num_epochs=num_epochs_TGCN, remove=False)\n",
    "y_true, y_pred = testmodel(best_model=best_model, test_loader=test_loader_TGCN, meanstd_dict=meanstd_dict, sensor_order_list=node_0.columns)\n",
    "rmse = 0\n",
    "for i in range(n_neighbors+1):\n",
    "    rmse = rmse + np.sqrt(mean_squared_error(y_true[:,:,i], y_pred[:,:,i]))\n",
    "print(rmse/n_neighbors+1)\n",
    "\n",
    "model_TGCN = TGCN(adjacency_matrix_PeMS, hidden_dim=32, output_size=len(df_PeMS.columns))\n",
    "train_loader_TGCN, val_loader_TGCN, test_loader_TGCN, _ = createLoaders(node_1, window_size=_window_size, stride=_stride, prediction_horizon=horizon)\n",
    "model_path = \"./test7_13.pkl\"\n",
    "best_model , train_losses, valid_losses = train_model(model_TGCN, train_loader_TGCN, val_loader_TGCN, model_path=model_path, mask=None, num_epochs=num_epochs_TGCN, remove=False)\n",
    "y_true, y_pred = testmodel(best_model=best_model, test_loader=test_loader_TGCN, meanstd_dict=meanstd_dict, sensor_order_list=node_1.columns)\n",
    "rmse = 0\n",
    "for i in range(n_neighbors+1):\n",
    "    rmse = rmse + np.sqrt(mean_squared_error(y_true[:,:,i], y_pred[:,:,i]))\n",
    "print(rmse/n_neighbors+1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (All variables) vs (MASK + Dummy Variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gcogoni\\Documents\\Stage\\Federated-Traffic-Flow-Forecasting\\src\\utils_graph.py:98: FutureWarning: adjacency_matrix will return a scipy.sparse array instead of a matrix in Networkx 3.0.\n",
      "  return nx.adjacency_matrix(graph, nodelist=nodes_order, weight=None).toarray()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Int64Index([117, 0, 92], dtype='int64')\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'_IncompatibleKeys' object has no attribute 'double'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 22\u001b[0m\n\u001b[0;32m     20\u001b[0m model_path \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m./test3_3.pkl\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     21\u001b[0m best_model \u001b[39m=\u001b[39m TGCN(adjacency_matrix_PeMS, hidden_dim\u001b[39m=\u001b[39m\u001b[39m32\u001b[39m, output_size\u001b[39m=\u001b[39m\u001b[39mlen\u001b[39m(df_PeMS\u001b[39m.\u001b[39mcolumns))\u001b[39m.\u001b[39mload_state_dict(torch\u001b[39m.\u001b[39mload(model_path))\n\u001b[1;32m---> 22\u001b[0m y_true, y_pred \u001b[39m=\u001b[39m testmodel(best_model\u001b[39m=\u001b[39;49mbest_model, test_loader\u001b[39m=\u001b[39;49mtest_loader_TGCN, meanstd_dict\u001b[39m=\u001b[39;49mmeanstd_dict, sensor_order_list\u001b[39m=\u001b[39;49mnode_0\u001b[39m.\u001b[39;49mcolumns)\n\u001b[0;32m     23\u001b[0m rmse \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m     24\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(n_neighbors\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\gcogoni\\Documents\\Stage\\Federated-Traffic-Flow-Forecasting\\src\\utils_training.py:207\u001b[0m, in \u001b[0;36mtestmodel\u001b[1;34m(best_model, test_loader, path, meanstd_dict, sensor_order_list, maximum, mask)\u001b[0m\n\u001b[0;32m    205\u001b[0m \u001b[39mif\u001b[39;00m path :\n\u001b[0;32m    206\u001b[0m     best_model\u001b[39m.\u001b[39mload_state_dict(torch\u001b[39m.\u001b[39mload(path))\n\u001b[1;32m--> 207\u001b[0m best_model\u001b[39m.\u001b[39mdouble()\n\u001b[0;32m    208\u001b[0m best_model\u001b[39m.\u001b[39meval()\n\u001b[0;32m    209\u001b[0m device \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mdevice(\u001b[39m\"\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mis_available() \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: '_IncompatibleKeys' object has no attribute 'double'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, mean_absolute_percentage_error\n",
    "\n",
    "\n",
    "_window_size = 7\n",
    "horizon = 1\n",
    "_stride = 1\n",
    "\n",
    "# TGCN\n",
    "num_epochs_TGCN = 400\n",
    "\n",
    "n_neighbors = 2\n",
    "df_PeMS_old, df_distance  = load_PeMS04_flow_data()\n",
    "df_PeMS, adjacency_matrix_PeMS, meanstd_dict = preprocess_PeMS_data(df_PeMS_old, df_distance, init_node=0, n_neighbors=n_neighbors, center_and_reduce=True)\n",
    "print(df_PeMS.columns)\n",
    "node_0, _ = mask_unknown_nodes(df_PeMS=df_PeMS, list_nodes_known=[117, 0, 92])\n",
    "node_1, mask_node_1 = mask_unknown_nodes(df_PeMS=df_PeMS, list_nodes_known=[117, 0])\n",
    "\n",
    "# TGCN Model\n",
    "model_path = \"./test3_3.pkl\"\n",
    "best_model = TGCN(adjacency_matrix_PeMS, hidden_dim=32, output_size=len(df_PeMS.columns)).load_state_dict(torch.load(model_path))\n",
    "y_true, y_pred = testmodel(best_model=best_model, test_loader=test_loader_TGCN, meanstd_dict=meanstd_dict, sensor_order_list=node_0.columns)\n",
    "rmse = 0\n",
    "for i in range(n_neighbors+1):\n",
    "    rmse = rmse + np.sqrt(mean_squared_error(y_true[:,:,i], y_pred[:,:,i]))\n",
    "print(rmse/n_neighbors+1)\n",
    "\n",
    "model_TGCN = TGCN(adjacency_matrix_PeMS, hidden_dim=32, output_size=len(df_PeMS.columns))\n",
    "train_loader_TGCN, val_loader_TGCN, test_loader_TGCN, _ = createLoaders(node_1, window_size=_window_size, stride=_stride, prediction_horizon=horizon)\n",
    "model_path = \"./test2_3_mask.pkl\"\n",
    "best_model , train_losses, valid_losses = train_model(model_TGCN, train_loader_TGCN, val_loader_TGCN, model_path=model_path, mask=mask_node_1, num_epochs=num_epochs_TGCN, remove=False)\n",
    "y_true, y_pred = testmodel(best_model=best_model, test_loader=test_loader_TGCN, meanstd_dict=meanstd_dict, sensor_order_list=node_1.columns)\n",
    "rmse = 0\n",
    "for i in range(n_neighbors+1):\n",
    "    rmse = rmse + np.sqrt(mean_squared_error(y_true[:,:,i], y_pred[:,:,i]))\n",
    "print(rmse/n_neighbors+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, mean_absolute_percentage_error\n",
    "\n",
    "\n",
    "_window_size = 7\n",
    "horizon = 1\n",
    "_stride = 1\n",
    "\n",
    "# TGCN\n",
    "num_epochs_TGCN = 400\n",
    "\n",
    "n_neighbors = 5\n",
    "df_PeMS_old, df_distance  = load_PeMS04_flow_data()\n",
    "df_PeMS, adjacency_matrix_PeMS, meanstd_dict = preprocess_PeMS_data(df_PeMS_old, df_distance, init_node=0, n_neighbors=n_neighbors, center_and_reduce=True)\n",
    "print(df_PeMS.columns)\n",
    "node_0, _ = mask_unknown_nodes(df_PeMS=df_PeMS, list_nodes_known=[243, 117, 28, 0, 173, 92])\n",
    "node_1, mask_node_1 = mask_unknown_nodes(df_PeMS=df_PeMS, list_nodes_known=[243, 117, 0])\n",
    "\n",
    "# TGCN Model\n",
    "model_path = \"./test5_5.pkl\"\n",
    "best_model = TGCN(adjacency_matrix_PeMS, hidden_dim=32, output_size=len(df_PeMS.columns)).load_state_dict(torch.load(model_path))\n",
    "y_true, y_pred = testmodel(best_model=best_model, test_loader=test_loader_TGCN, meanstd_dict=meanstd_dict, sensor_order_list=node_0.columns)\n",
    "\n",
    "rmse = 0\n",
    "for i in range(n_neighbors+1):\n",
    "    rmse = rmse + np.sqrt(mean_squared_error(y_true[:,:,i], y_pred[:,:,i]))\n",
    "print(rmse/n_neighbors+1)\n",
    "\n",
    "model_TGCN = TGCN(adjacency_matrix_PeMS, hidden_dim=32, output_size=len(df_PeMS.columns))\n",
    "train_loader_TGCN, val_loader_TGCN, test_loader_TGCN, _ = createLoaders(node_1, window_size=_window_size, stride=_stride, prediction_horizon=horizon)\n",
    "model_path = \"./test2_3_mask.pkl\"\n",
    "best_model , train_losses, valid_losses = train_model(model_TGCN, train_loader_TGCN, val_loader_TGCN, model_path=model_path, mask=mask_node_1, num_epochs=num_epochs_TGCN, remove=False)\n",
    "y_true, y_pred = testmodel(best_model=best_model, test_loader=test_loader_TGCN, meanstd_dict=meanstd_dict, sensor_order_list=node_1.columns)\n",
    "rmse = 0\n",
    "for i in range(n_neighbors+1):\n",
    "    rmse = rmse + np.sqrt(mean_squared_error(y_true[:,:,i], y_pred[:,:,i]))\n",
    "print(rmse/n_neighbors+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, mean_absolute_percentage_error\n",
    "\n",
    "\n",
    "_window_size = 7\n",
    "horizon = 1\n",
    "_stride = 1\n",
    "\n",
    "# TGCN\n",
    "num_epochs_TGCN = 400\n",
    "\n",
    "n_neighbors = 12\n",
    "df_PeMS_old, df_distance  = load_PeMS04_flow_data()\n",
    "df_PeMS, adjacency_matrix_PeMS, meanstd_dict = preprocess_PeMS_data(df_PeMS_old, df_distance, init_node=0, n_neighbors=n_neighbors, center_and_reduce=True)\n",
    "print(df_PeMS.columns)\n",
    "node_0, _ = mask_unknown_nodes(df_PeMS=df_PeMS, list_nodes_known=[68, 62, 242, 243, 117, 28, 170, 0, 173, 271, 92, 57, 97])\n",
    "node_1, mask_node_1 = mask_unknown_nodes(df_PeMS=df_PeMS, list_nodes_known=[68, 62, 242, 243, 117, 28, 170])\n",
    "\n",
    "# TGCN Model\n",
    "model_path = \"./test13_13.pkl\"\n",
    "best_model = TGCN(adjacency_matrix_PeMS, hidden_dim=32, output_size=len(df_PeMS.columns)).load_state_dict(torch.load(model_path))\n",
    "y_true, y_pred = testmodel(best_model=best_model, test_loader=test_loader_TGCN, meanstd_dict=meanstd_dict, sensor_order_list=node_0.columns)\n",
    "rmse = 0\n",
    "for i in range(n_neighbors+1):\n",
    "    rmse = rmse + np.sqrt(mean_squared_error(y_true[:,:,i], y_pred[:,:,i]))\n",
    "print(rmse/n_neighbors+1)\n",
    "\n",
    "model_TGCN = TGCN(adjacency_matrix_PeMS, hidden_dim=32, output_size=len(df_PeMS.columns))\n",
    "train_loader_TGCN, val_loader_TGCN, test_loader_TGCN, _ = createLoaders(node_1, window_size=_window_size, stride=_stride, prediction_horizon=horizon)\n",
    "model_path = \"./test7_13_mask.pkl\"\n",
    "best_model , train_losses, valid_losses = train_model(model_TGCN, train_loader_TGCN, val_loader_TGCN, model_path=model_path, mask=mask_node_1, num_epochs=num_epochs_TGCN, remove=False)\n",
    "y_true, y_pred = testmodel(best_model=best_model, test_loader=test_loader_TGCN, meanstd_dict=meanstd_dict, sensor_order_list=node_1.columns)\n",
    "rmse = 0\n",
    "for i in range(n_neighbors+1):\n",
    "    rmse = rmse + np.sqrt(mean_squared_error(y_true[:,:,i], y_pred[:,:,i]))\n",
    "print(rmse/n_neighbors+1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  (Dummy Variables) VS (MASK + Dummy Variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, mean_absolute_percentage_error\n",
    "\n",
    "\n",
    "_window_size = 7\n",
    "horizon = 1\n",
    "_stride = 1\n",
    "\n",
    "# TGCN\n",
    "num_epochs_TGCN = 400\n",
    "\n",
    "n_neighbors = 2\n",
    "df_PeMS_old, df_distance  = load_PeMS04_flow_data()\n",
    "df_PeMS, adjacency_matrix_PeMS, meanstd_dict = preprocess_PeMS_data(df_PeMS_old, df_distance, init_node=0, n_neighbors=n_neighbors, center_and_reduce=True)\n",
    "print(df_PeMS.columns)\n",
    "node_0, mask_node_0 = mask_unknown_nodes(df_PeMS=df_PeMS, list_nodes_known=[117, 0])\n",
    "node_1, mask_node_1 = mask_unknown_nodes(df_PeMS=df_PeMS, list_nodes_known=[117, 0])\n",
    "\n",
    "# TGCN Model\n",
    "model_TGCN = TGCN(adjacency_matrix_PeMS, hidden_dim=32, output_size=len(df_PeMS.columns))\n",
    "train_loader_TGCN, val_loader_TGCN, test_loader_TGCN, _ = createLoaders(node_0, window_size=_window_size, stride=_stride, prediction_horizon=horizon)\n",
    "model_path = \"./test2_2_without_mask.pkl\"\n",
    "\n",
    "best_model , train_losses, valid_losses = train_model(model_TGCN, train_loader_TGCN, val_loader_TGCN, model_path=model_path, mask=None, num_epochs=num_epochs_TGCN, remove=False)\n",
    "y_true, y_pred = testmodel(best_model=best_model, test_loader=test_loader_TGCN, meanstd_dict=meanstd_dict, sensor_order_list=node_0.columns)\n",
    "rmse = 0\n",
    "for i in range(n_neighbors+1):\n",
    "    rmse = rmse + np.sqrt(mean_squared_error(y_true[:,:,i], y_pred[:,:,i]))\n",
    "print(rmse/n_neighbors+1)\n",
    "\n",
    "model_TGCN = TGCN(adjacency_matrix_PeMS, hidden_dim=32, output_size=len(df_PeMS.columns))\n",
    "train_loader_TGCN, val_loader_TGCN, test_loader_TGCN, _ = createLoaders(node_1, window_size=_window_size, stride=_stride, prediction_horizon=horizon)\n",
    "model_path = \"./test2_2_mask.pkl\"\n",
    "best_model , train_losses, valid_losses = train_model(model_TGCN, train_loader_TGCN, val_loader_TGCN, model_path=model_path, mask=mask_node_1, num_epochs=num_epochs_TGCN, remove=False)\n",
    "y_true, y_pred = testmodel(best_model=best_model, test_loader=test_loader_TGCN, meanstd_dict=meanstd_dict, sensor_order_list=node_1.columns)\n",
    "rmse = 0\n",
    "for i in range(n_neighbors+1):\n",
    "    rmse = rmse + np.sqrt(mean_squared_error(y_true[:,:,i], y_pred[:,:,i]))\n",
    "print(rmse/n_neighbors+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, mean_absolute_percentage_error\n",
    "\n",
    "\n",
    "_window_size = 7\n",
    "horizon = 1\n",
    "_stride = 1\n",
    "\n",
    "# TGCN\n",
    "num_epochs_TGCN = 400\n",
    "\n",
    "n_neighbors = 5\n",
    "df_PeMS_old, df_distance  = load_PeMS04_flow_data()\n",
    "df_PeMS, adjacency_matrix_PeMS, meanstd_dict = preprocess_PeMS_data(df_PeMS_old, df_distance, init_node=0, n_neighbors=n_neighbors, center_and_reduce=True)\n",
    "print(df_PeMS.columns)\n",
    "node_0, _ = mask_unknown_nodes(df_PeMS=df_PeMS, list_nodes_known=[243, 117, 28])\n",
    "node_1, mask_node_1 = mask_unknown_nodes(df_PeMS=df_PeMS, list_nodes_known=[243, 117, 0])\n",
    "\n",
    "# TGCN Model\n",
    "model_TGCN = TGCN(adjacency_matrix_PeMS, hidden_dim=32, output_size=len(df_PeMS.columns))\n",
    "train_loader_TGCN, val_loader_TGCN, test_loader_TGCN, _ = createLoaders(node_0, window_size=_window_size, stride=_stride, prediction_horizon=horizon)\n",
    "model_path = \"./test3_3_without_mask.pkl\"\n",
    "\n",
    "best_model , train_losses, valid_losses = train_model(model_TGCN, train_loader_TGCN, val_loader_TGCN, model_path=model_path, mask=None, num_epochs=num_epochs_TGCN, remove=False)\n",
    "y_true, y_pred = testmodel(best_model=best_model, test_loader=test_loader_TGCN, meanstd_dict=meanstd_dict, sensor_order_list=node_0.columns)\n",
    "rmse = 0\n",
    "for i in range(n_neighbors+1):\n",
    "    rmse = rmse + np.sqrt(mean_squared_error(y_true[:,:,i], y_pred[:,:,i]))\n",
    "print(rmse/n_neighbors+1)\n",
    "\n",
    "model_TGCN = TGCN(adjacency_matrix_PeMS, hidden_dim=32, output_size=len(df_PeMS.columns))\n",
    "train_loader_TGCN, val_loader_TGCN, test_loader_TGCN, _ = createLoaders(node_1, window_size=_window_size, stride=_stride, prediction_horizon=horizon)\n",
    "model_path = \"./test3_3_mask.pkl\"\n",
    "best_model , train_losses, valid_losses = train_model(model_TGCN, train_loader_TGCN, val_loader_TGCN, model_path=model_path, mask=mask_node_1, num_epochs=num_epochs_TGCN, remove=False)\n",
    "y_true, y_pred = testmodel(best_model=best_model, test_loader=test_loader_TGCN, meanstd_dict=meanstd_dict, sensor_order_list=node_1.columns)\n",
    "rmse = 0\n",
    "for i in range(n_neighbors+1):\n",
    "    rmse = rmse + np.sqrt(mean_squared_error(y_true[:,:,i], y_pred[:,:,i]))\n",
    "print(rmse/n_neighbors+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, mean_absolute_percentage_error\n",
    "\n",
    "\n",
    "_window_size = 7\n",
    "horizon = 1\n",
    "_stride = 1\n",
    "\n",
    "# TGCN\n",
    "num_epochs_TGCN = 400\n",
    "\n",
    "n_neighbors = 12\n",
    "df_PeMS_old, df_distance  = load_PeMS04_flow_data()\n",
    "df_PeMS, adjacency_matrix_PeMS, meanstd_dict = preprocess_PeMS_data(df_PeMS_old, df_distance, init_node=0, n_neighbors=n_neighbors, center_and_reduce=True)\n",
    "print(df_PeMS.columns)\n",
    "node_0, _ = mask_unknown_nodes(df_PeMS=df_PeMS, list_nodes_known=[68, 62, 242, 243, 117, 28, 170])\n",
    "node_1, mask_node_1 = mask_unknown_nodes(df_PeMS=df_PeMS, list_nodes_known=[68, 62, 242, 243, 117, 28, 170])\n",
    "\n",
    "# TGCN Model\n",
    "model_TGCN = TGCN(adjacency_matrix_PeMS, hidden_dim=32, output_size=len(df_PeMS.columns))\n",
    "train_loader_TGCN, val_loader_TGCN, test_loader_TGCN, _ = createLoaders(node_0, window_size=_window_size, stride=_stride, prediction_horizon=horizon)\n",
    "model_path = \"./test7_13_without_mask.pkl\"\n",
    "\n",
    "best_model , train_losses, valid_losses = train_model(model_TGCN, train_loader_TGCN, val_loader_TGCN, model_path=model_path, mask=None, num_epochs=num_epochs_TGCN, remove=False)\n",
    "y_true, y_pred = testmodel(best_model=best_model, test_loader=test_loader_TGCN, meanstd_dict=meanstd_dict, sensor_order_list=node_0.columns)\n",
    "rmse = 0\n",
    "for i in range(n_neighbors+1):\n",
    "    rmse = rmse + np.sqrt(mean_squared_error(y_true[:,:,i], y_pred[:,:,i]))\n",
    "print(rmse/n_neighbors+1)\n",
    "\n",
    "model_TGCN = TGCN(adjacency_matrix_PeMS, hidden_dim=32, output_size=len(df_PeMS.columns))\n",
    "train_loader_TGCN, val_loader_TGCN, test_loader_TGCN, _ = createLoaders(node_1, window_size=_window_size, stride=_stride, prediction_horizon=horizon)\n",
    "model_path = \"./test7_13_mask.pkl\"\n",
    "best_model , train_losses, valid_losses = train_model(model_TGCN, train_loader_TGCN, val_loader_TGCN, model_path=model_path, mask=mask_node_1, num_epochs=num_epochs_TGCN, remove=False)\n",
    "y_true, y_pred = testmodel(best_model=best_model, test_loader=test_loader_TGCN, meanstd_dict=meanstd_dict, sensor_order_list=node_1.columns)\n",
    "rmse = 0\n",
    "for i in range(n_neighbors+1):\n",
    "    rmse = rmse + np.sqrt(mean_squared_error(y_true[:,:,i], y_pred[:,:,i]))\n",
    "print(rmse/n_neighbors+1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
